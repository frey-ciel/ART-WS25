% --- set doc class ---
\documentclass[11pt,oneside]{book}                  % book class with chapters (oneside to avoid mirrored margins)

% --- Language & Encoding ---
\usepackage[ngerman]{babel}                         % German hyphenation, captions, date formats
\usepackage[T1]{fontenc}                            % Proper output encoding (umlauts copy/paste correctly)
\usepackage[utf8]{inputenc}                         % Input encoding: write umlauts directly in UTF-8

% --- Fonts ---
\usepackage{lmodern}                                % Load Latin Modern fonts (includes serif, sans-serif, and mono)
\renewcommand{\familydefault}{\sfdefault}           % Set default font family to Latin Modern Sans Serif
\usepackage{newtxmath}                              % Times-like math that matches well with LM
\usepackage{microtype}                              % Subtle typographic improvements (protrusion, expansion)
\usepackage{siunitx}                                % Consistent numbers & units: \SI{3.0}{m/s}, tables with S
\sisetup{locale=DE}                                 % set punctuation and comma for numbers like this: 1.234,56 
\usepackage{soul}                                   % Text highlighting (e.g., \hl{})

% --- Page Geometry ---
\usepackage[a4paper,left=20mm,right=20mm,top=28mm,bottom=28mm]{geometry}
\setlength{\parindent}{0pt}                         % Suppress paragraph indentation globally (no need to write \noindent)
\setlength{\parskip}{6pt}                           % a bit of space between paragraphs

% --- Graphics & Floats ---
\usepackage{graphicx}                               % \includegraphics[width=.5\textwidth]{...}
\usepackage{float}                                  % force figures to be placed exactly where they are in the latex text with [H]
\usepackage[justification=centering]{caption}       % Centered captions
\usepackage{subcaption}                             % Subfigures via subfigure environment
\graphicspath{{images/}}                            % Look for figures under ./images/
\usepackage{floatflt}                               % show text and figure side by side (also works inside \begin ... \end blocks)
\usepackage{wrapfig}                                % show text and figure side by side
\usepackage{tikz}                                   % tikz drawings
\usetikzlibrary{arrows.meta,positioning,calc,bending}

% --- Tables ---
\usepackage{booktabs}                               % table rules (toprule, midrule, bottomrule)
\usepackage{tabularx}                               % Tables with automatic column width adjustment
\usepackage[table]{xcolor}                          % table color support, including row/column shading in tables

% --- Math & Theorems ---
\usepackage{amsmath}                                % Display math environments, align, cases, etc.
\let\openbox\relax                                  % Avoid \openbox clash with newtxmath
\usepackage{amsthm}                                 % Theorem environments + proof environment
\usepackage{cancel}                                 % cross out terms
\usepackage{mathtools}                              % Extra math tools (e.g. \coloneqq, paired delimiters)
\numberwithin{equation}{chapter}                    % Equation numbers like (1.1), (1.2), ...

% ---------- Hyperlinks & Cleveref ----------
\usepackage[colorlinks=true,allcolors=blue]{hyperref} % Clickable links with colored text
\usepackage[nameinlink,noabbrev]{cleveref}   % Smart cross-refs: \cref{fig:...,thm:...}

% --- Theorem aliases for cleveref ---
\usepackage{aliascnt}

% Theoremstyle
\newtheoremstyle{mystyle}
    {18pt}% Space above
    {12pt}% Space below
    {\normalfont}% Body font
    {}% Indent amount
    {\bfseries}% Theorem head font
    {.}% Punctuation after theorem head
    {0.5em}% Space after theorem head
    {}% Theorem head spec


\theoremstyle{mystyle}

% Hauptzähler
\newtheorem{theorem}{Satz}[chapter]
\crefname{theorem}{Satz}{Sätze}
\Crefname{theorem}{Satz}{Sätze}

% came enumeration accross def, lemma, remark..., but different typename for cleverref
\newaliascnt{lemma}{theorem}
\newtheorem{lemma}[lemma]{Lemma}
\aliascntresetthe{lemma}
\crefname{lemma}{Lemma}{Lemmata}
\Crefname{lemma}{Lemma}{Lemmata}

\newaliascnt{corollary}{theorem}
\newtheorem{corollary}[corollary]{Korollar}
\aliascntresetthe{corollary}
\crefname{corollary}{Korollar}{Korollare}
\Crefname{corollary}{Korollar}{Korollare}

\newaliascnt{proposition}{theorem}
\newtheorem{proposition}[proposition]{Proposition}
\aliascntresetthe{proposition}
\crefname{proposition}{Proposition}{Propositionen}
\Crefname{proposition}{Proposition}{Propositionen}

\newaliascnt{definition}{theorem}
\newtheorem{definition}[definition]{Definition}
\aliascntresetthe{definition}
\crefname{definition}{Definition}{Definitionen}
\Crefname{definition}{Definition}{Definitionen}

\newaliascnt{satzdefinition}{theorem}
\newtheorem{satzdefinition}[satzdefinition]{Satz-Definition}
\aliascntresetthe{satzdefinition}
\crefname{satzdefinition}{Satz-Definition}{Satz-Definitionen}
\Crefname{satzdefinition}{Satz-Definition}{Satz-Definitionen}

\newaliascnt{remark}{theorem}
\newtheorem{remark}[remark]{Bemerkung}
\aliascntresetthe{remark}
\crefname{remark}{Bemerkung}{Bemerkungen}
\Crefname{remark}{Bemerkung}{Bemerkungen}

\newaliascnt{example}{theorem}
\newtheorem{example}[example]{Beispiel}
\aliascntresetthe{example}
\crefname{example}{Beispiel}{Beispiele}
\Crefname{example}{Beispiel}{Beispiele}

\crefname{figure}{Abbildung}{Abbildungen}
\Crefname{figure}{Abbildung}{Abbildungen}

% ---------- Custom command to insert a break after a theorem head ----------
\newcommand{\breakafterhead}{\mbox{}\\[-1.7em]}
\newcommand{\bigbreakafterhead}{\mbox{}\\[-1em]}

% ---------- Lists ----------
\usepackage{enumitem}                        % Customizable lists; set labels for nested enumerate
\setlist[enumerate,1]{label=\arabic*., labelsep=1em}       % 1., 2., 3., ...
\setlist[enumerate,2]{label=\alph*), labelsep=1em}         % a), b), c), ...
\setlist[enumerate,3]{label=\roman*), labelsep=1em}        % i), ii), iii), ...

% Customize all itemize levels
\setlist[itemize,1]{label=\(\bullet\), labelsep=1em}
\setlist[itemize,2]{label=\(\circ\), labelsep=1em}
\setlist[itemize,3]{label=\textbullet, labelsep=1em}

% ---------- vertical line for non VO parts ------------
\usepackage[most]{tcolorbox}
\tcbset{
  noVO/.style={
    enhanced,
    breakable,
    sharp corners,
    frame hidden,
    colback=white,
    colframe=gray!60,
    boxrule=0pt,
    borderline west={2pt}{0pt}{gray!60},            % nur linker Strich
    left=6pt, right=0pt, top=0pt, bottom=0pt,
    before skip=6pt, after skip=6pt,
    parbox=false,
  }
}


% ---------- Headers/Footers ----------
\usepackage{fancyhdr}
\setlength{\headheight}{14pt}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[R]{\nouppercase{\leftmark}}       % Right header: current chapter title
\fancyfoot[C]{\thepage}                      % Center footer: page number
\renewcommand{\headrulewidth}{0.4pt}

% --- Custom heading (not in ToC, bold, underlined, with spacing) ---
\newcommand{\heading}[1]{%
  \vspace{1em}% space before
  \noindent\textbf{\underline{#1}}%
}

\newcommand{\headingnospace}[1]{%
  \noindent\textbf{\underline{#1}}%
}



% --- Title Data ---
\title{Einführung in die Allgemeine Relativitätstheorie}
\author{Ciel Frey}
\date{\today}

% ===================== Document =====================
\begin{document}

% --- Front matter (roman page numbers) ---
\frontmatter
\begin{titlepage}
    \centering
    \vspace*{2cm}
    {\usefont{T1}{ptm}{b}{sc}\Huge Einführung in die Allgemeine Relativitätstheorie\par}
    \vspace{0.5cm}
    \rule{\textwidth}{0.4pt}\par
    \vspace{0.5cm}
    {\LARGE\ VO 136.026, Priv. Doz. Dr. Herbert Balasin, WS 2025/26\par}
    \vspace{0.5cm}
    {\Large\itshape Mitschrift von Ciel Frey\par}
    \vfill
    \includegraphics[width=0.7\textwidth]{spacetime_curve.png}\par
    \vfill
    {\large \today\par}
\end{titlepage}

\tableofcontents

\chapter*{Vorwort}
Diese Mitschrift ergänzt die bereits vorhandenen Mitschriften von Robert Berndl-Forstner (WS20/21) und Florian Lindenbauer (WS18/19), die hier zu finden sind: \newline
\url{https://forum.fstph.at/t/einf-i-d-allgemeine-relativitaetstheorie-mitschrift-ws2018/3734} \newline
\url{https://forum.fstph.at/t/latex-skriptum-allg-relativitaetstheorie/4021}

Zuerst gibt es eine kurze Einführung, wie es zur speziellen und allgemeinen Relativitätstheorie (kurz SRT und ART) kam. Anschließend wird SRT mit Raumzeit-Diagrammen und dem Minkowski-Raum beschrieben. In den darauffolgenden Kapiteln werden die mathematischen Tools vorgestellt, die für das Verständnis von ART benötigt werden. Zum Schluss folgt die Einsteingleichung. Einen tieferen Einblick in ART gewährt die im Sommersemester anschließende VO 136.002 Geometrie und Gravitation I.

An einigen Stellen habe ich zusätzliche Passagen eingefügt, die zwar nicht Teil der Vorlesung waren, mir aber beim Verständnis geholfen haben. Diese sind entsprechend mit \textit{„nicht Teil der VO“} oder durch einen grauen Strich am linken Rand markiert.

Das Bild des Titelblatts stellt die Raumzeitkrümmung in 3D dar. \newline
{\footnotesize Quelle: \url{https://sketchfab.com/3d-models/gravity-warping-space-time-749a8acac04049a89dc8d02054a02ee8}}

% --- Main matter (arabic page numbers) ---
\mainmatter

\chapter{Einführung}\label{chap:Einführung}

\section{Newtonsche Mechanik und Gravitationsphysik}
Die Gravitation zählt seit Jahrhunderten zu den zentralen Themen der Physik. Schon Isaac Newton formulierte nach der bekannten Anekdote mit dem fallenden Apfel das \textit{Gravitationsgesetz}.

Der Betrag der Gravitationskraft/Schwerkraft lautet
\begin{equation*}
F_{\text{grav}} = G \frac{M m}{r^2}
\end{equation*}
wobei $M$ und $m$ die Massen zweier Körper, $r=|\vec x |$ ihr Abstand, $\vec x$ der Ortsvektor von $m$ relativ zu $M$ und $G = 6,6743 \cdot 10^{-11} \frac{m^3}{kg\cdot s^2}$ die Gravitationskonstante. 

In Vektorform lautet die Gravitationskraft
\begin{equation*}
\vec F_{\text{grav}} = -G \frac{M m}{r^3}\, \vec x
\end{equation*}

\begin{figure}[H]
\centering
\includegraphics[width=0.3\textwidth]{images/Gravitation.jpg}
\end{figure}

In Vektorkomponenten $i$ geschrieben
\begin{equation*}
F^i_{\text{grav}} = - G \frac{M m}{r^3}\, x^i
\end{equation*}

Dieses Gesetz beschreibt bei Geschwindigkeiten, die wesentlich kleiner als die Lichtgeschwindigkeit sind und solange man sich nicht in der Nähe großer Massen aufhält, eine sehr gute Näherung der auftretenden Gravitationswechselwirkungen.

Das Gravitationsgesetz kann auch mithilfe des Gravitationspotentials (Skalarfeld) $\Phi(r)=-\frac{GM}{r}$ formuliert werden:
\begin{equation*}
F^i_{\text{grav}} = -m \partial_i \Phi(x^k(t))
\end{equation*}
wobei $\partial_i := \frac{\partial}{\partial x^i}$ die partielle Ableitung nach der $i$-ten Raumkoordinate bezeichnet und somit $\partial_i\Phi = (\nabla \Phi)^i$ die Komponenten des Gradienten des Gravitationspotentials.

Zugleich verband Newton das Konzept der \textit{Kraft} mit dem der \textit{Beschleunigung} und formulierte damit die Grundlage der heute sogenannten \textit{Newton’schen Mechanik}. Nach dem zweiten Newtonschen Axiom gilt
\begin{equation*}
m\,\ddot{x}^i(t) = F^i(x^k(t))
\end{equation*}

Die an der jeweiligen Position des Körpers wirkende Kraft ist also Masse mal Beschleunigung.

Die Lösung dieser Gleichung beschreibt die Bahn $x^i(t,m)$ eines (punktförmig gedachten) Körpers in Abhängigkeit von der Zeit und der Masse des Körpers. Wendet man das zweite Newtonsche Axiom auf die Gravitationskraft an, erhält man:
\begin{equation*}
\cancel{m}\,\ddot{x}^i(t) = F^i_{\text{grav}} = -\cancel{m} \partial_i \Phi(x^k(t))
\end{equation*}

Die Masse $m$ kürzt sich heraus, sodass sich die Bewegungsgleichung auch in der Form
\begin{equation*}
\ddot{x}^i(t) = - \partial_i \Phi(x^k(t))
\end{equation*}

schreiben lässt.  
Die Bahn des Teilchens ist somit unabhängig von seiner Masse, also $x^i=x^i(t)$.

Diese Unabhängigkeit wird als das \href{https://de.wikipedia.org/wiki/Allgemeine_Relativit%C3%A4tstheorie#Verallgemeinerung_des_%C3%84quivalenzprinzips}{\textbf{klassische Äquivalenzprinzip}} oder auch \textbf{schwache Äquivalenzprinzip} bezeichnet, und geht auf Überlegungen von Galileo Galilei und seine \href{https://www.leifiphysik.de/mechanik/freier-fall-senkrechter-wurf/geschichte/galileis-untersuchung-des-freien-falls}{Versuche mit Kugeln auf schiefen Bahnen} zurück
\begin{quote}
Alle Körper fallen im selben Gravitationsfeld im Vakuum gleich schnell, unabhängig von ihrer Masse. Die Beschleunigung hängt allein vom Gravitationsfeld ab.
\end{quote}

Bei der Mondlandung 1971 wurde dies im Vakuum bestätigt: Hammer und Feder trafen gleichzeitig auf der Mondoberfläche auf: \url{https://www.youtube.com/watch?v=Oo8TaPVsn9Y}

Wirken keine Kräfte $(F^i = 0)$, so folgt aus dem zweiten Newtonschen Axiom mit zweimaliger Integration:
\begin{equation*}
m\,\ddot{x}^i(t) = 0
\quad\Rightarrow\quad
x^i(t) = x^i_0 + \dot{x}^i_0\,t
\end{equation*}
Dies entspricht dem \textit{ersten Newtonschen Axiom}:  
Ein kräftefreier Körper bleibt in Ruhe oder bewegt sich geradlinig mit konstanter Geschwindigkeit. Also ist die Bahn eine Gerade mit gleichförmiger Bewegung.

\subsection{Inertialsystem, Galilei-Transformation und Galileisches Relativitätsprinzip (nicht Teil der VO)}
Bewegungen können immer nur relativ zu etwas beschrieben werden: von etwas weg, zu etwas hin, ruhend relativ zu etwas. Ein \textbf{Bezugssystem} (oder Koordinatensystem) hat einen Bezugspunkt (Koordinatenursprung), bezüglich dessen die Bewegungen beobachtet werden. Es gibt ruhende und bewegte Bezugssysteme.

\begin{definition}
Ein \textbf{Inertialsystem} hat die folgenden äquivalenten Definitionen:
\begin{itemize}
    \item ein Bezugssystem, in dem das erste Newtonsche Gesetz gilt: jeder kräftefreie Körper innerhalb dieses Bezugssystems befindet sich entweder in Ruhe oder bewegt sich mit konstanter Geschwindigkeit auf einer Geraden.
    \item ein nicht beschleunigtes Bezugssystem.
\end{itemize}
\end{definition}

In einem Inertialsystem wirken also keine Scheinkräfte/Trägheitskräfte. Systeme, die sich relativ zu einem Inertialsystem beschleunigt oder rotierend bewegen, sind selbst keine Inertialsysteme.

\begin{example}
Ein Zug bewegt sich mit konstanter Geschwindigkeit auf einer Strecke mit geraden Schienen. Ein Bezugssystem, das sich mit dem Zug bewegt, ist ein Inertialsystem. Sobald der Zug um eine Kurve fährt, müssen sich die äußeren Teile des Zugs beschleunigen, um dieselbe Strecke wie die inneren Teile zurückzulegen. Das mitgeführte Bezugssystem ist kein Inertialsystem mehr. Ebenso wenn der Zug auf der geraden Strecke abbremst oder beschleunigt. Durch die krummlinige Bewegung oder die Beschleunigung/das Abbremsen treten Trägheitskräfte auf.
\end{example}

\begin{theorem}[Galileisches Relativitätsprinzip]\label{thm:Relativitätsprinzip}
Alle physikalischen Gesetze sind in allen Inertialsystemen gleich.
Oder auch: Bewegt sich ein Inertialsystem $I'$ mit konstanter Geschwindigkeit relativ zu einem anderen Inertialsystem $I$, so gelten in beiden dieselben physikalischen Gesetze.
\end{theorem}
Dieses Prinzip bedeutet, dass keine mechanischen Experimente durchgeführt werden können, die erkennen lassen, ob man sich in Ruhe oder in gleichförmiger Bewegung befindet.

\begin{definition}
Die \textbf{Galilei-Transformation} beschreibt den Zusammenhang zwischen den Raum- und Zeitkoordinaten zweier Inertialsysteme $I$ und $I'$, die sich mit konstanter Geschwindigkeit $\vec v$ zueinander bewegen, wobei $\vec r$ die Position des Körpers und $t$ die Zeit beschreibt.
Sie lautet:

\begin{equation*}
\begin{aligned}
\vec{r}' &= \vec{r} - \vec{v}t \qquad \text{Translation der Trajektorie des Körpers}\\
\vec{v}' = \dot{\vec{r}}' &= \dot{\vec{r}} - \vec{v} \qquad \, \text{Geschwindigkeiten werden addiert} \\
t' &= t \qquad \qquad \text{Zeit bleibt gleich / ist absolut}
\end{aligned}
\end{equation*}

\end{definition}

\begin{example}
    Ein Schiff bewegt sich geradlinig und gleichförmig mit Geschwindigkeit $\vec v$.  
    Ein Fußballspieler unter Deck kann durch ein Experiment - etwa indem er einen Ball hochwirft - nicht feststellen, ob sich das Schiff bewegt oder ruht.  
    Das Schiff bildet also ein Inertialsystem, und die Flugbahn des Balls kann mit der Galilei-Transformation in das ruhende System an Land überführt werden.
    
\begin{figure}[H]
\centering
\includegraphics[width=0.3\textwidth]{images/Boat.jpg}
\end{figure}

\end{example}


\subsection{spezielle Relativitätstheorie im Überblick (nicht Teil der VO)}
{\footnotesize{gutes Video: 4D Spacetime and Relativity explained simply and visually: \url{https://youtu.be/ZfR1Jc6Zglo}}}

\headingnospace{Ausgangspunkt: Newtonsche Mechanik}

In der newtonschen Mechanik wird von einem \textbf{absoluten Raum} und einer \textbf{absoluten Zeit} ausgegangen, die \textbf{unabhängig} voneinander existieren.  
Alle Beobachter messen dieselbe Zeit und dieselben Längen, und alle Bewegungen werden relativ zu einem Inertialsystem beschrieben.

Transformationen zwischen Inertialsystemen sind Galilei-Transformationen: lineare Transformationen, bei denen die Zeit unverändert bleibt und sich Geschwindigkeiten addieren.

\begin{example}
Ein Zug fährt geradlinig mit konstanter Geschwindigkeit $v_1 = \SI{100}{km/h}$ (Inertialsystem). Eine Person im Zug wirft einen Ball mit $v_2 = \SI{10}{km/h}$ nach vorne. Für einen stillstehenden Beobachter außerhalb des Zuges (Inertialsystem), bewegt sich der Ball dann mit der Geschwindigkeit $v = v_1 + v_2 = \SI{110}{km/h}$.
\end{example}

\heading{Konflikt: Lichtgeschwindigkeit}

Aus den Maxwell-Gleichungen (Gesetze des Elektromagnetismus) folgt, dass die Lichtgeschwindigkeit \\ 
$c = \SI{299792}{km/s}$ konstant ist. Experimente zeigen, dass diese Geschwindigkeit in allen Inertialsystemen denselben Wert hat (invariant ist).  
Damit widerspricht die Lichtgeschwindigkeit jedoch der Galilei-Transformation, da sie sich nicht addieren lässt.

\begin{example}
Ein Zug bewegt sich geradlinig mit konstanter Geschwindigkeit $v_1 = \frac{c}{2}$. Eine Person $A$ im Zug leuchtet mit einer Taschenlampe nach vorne auf eine Wand in der Entfernung $L_A$. Das Licht legt diese Strecke in der Zeit $t_A = L_A / c$ zurück.

\begin{floatingfigure}[r]{0.3\textwidth}
\centering
\includegraphics[width=0.3\textwidth]{Licht-bsp.jpg}
\end{floatingfigure}

Da $c$ = konst. bewegt sich das Licht auch für einen stillstehenden Beobachter $B$ außerhalb des Zuges mit der Geschwindigkeit $c$ und nicht mit $v_2=c+v_1$. Das heißt Beobachter B muss die Zeit $t_A$ und/oder die Länge $L_A$ anders wahrnehmen. Tatsächlich tritt beides ein: Zeitdilatation und Längenkontraktion. 

Für den außenstehenden Beobachter $B$ trifft der Lichtstrahl später auf die Wand, weil sich die Wand wegbewegt. D.h. $t_B>t_A$ (Zeitdilatation). Außerdem nimmt Beobachter $B$ den Zug kürzer wahr. D.h. $L_b <L_A$ (Längenkontraktion). Beides wird im Folgenden noch mehr erklärt.
\end{example}

\heading{Gedankenexperiment: Die Lichtuhr (Zeitdilatation)}

{\footnotesize{gute Videos: \newline
Time Dilation - Einstein's Special Relativity: \url{https://youtu.be/P1MG61R17Ks} \newline
I never understood why you can't go faster than light - until now!: \url{https://youtu.be/Vitf8YaVXhc} \newline
Special Relativity Part 2: Time Dilation and the Twin Paradox: \url{https://youtu.be/iIEeSiT3SI4}}}

\begin{wrapfigure}[4]{r}{0.20\textwidth}
\vspace{-1.8em}
\centering
\includegraphics[width=\linewidth]{Lichtuhr.jpg}
\end{wrapfigure}

Eine Lichtuhr besteht aus zwei Spiegeln, zwischen denen ein Photon hin und her springt. Jedes Mal, wenn das Photon einen Spiegel erreicht, tickt die Uhr um eine Einheit weiter. \textit{Fun fact: Für eine Sekunde müssten dafür die Spiegel }$L = c \cdot \SI{1}{s} = \SI{299792}{km}$ \textit{voneinander entfernt sein.}

\begin{minipage}[t]{0.70\textwidth}
Person $A$ (im mit $v=\frac{c}{2}$ fahrenden Zug; Inertialsystem $I'$) und Person $B$ (außerhalb des Zugs; Inertialsystem $I$) besitzen jeweils eine solche Lichtuhr.  
Im fahrenden Zug bewegt sich das Photon diagonal, da sich die Spiegel weiter bewegen. Also legt es, aus der Sicht von $B$, eine längere Strecke zurück, sodass die Uhr im Zug, aus Sicht von $B$, langsamer läuft.\newline

Es gilt: $L=c \cdot t$ (Weg = Geschwindigkeit $\cdot$ Zeit) \newline
Sei $t$ die Zeit, die aus der Sicht von $B$ im Zug vergeht, und $t'$ die Zeit, die $B$ an seiner Lichtuhr abliest.
\end{minipage}\hfill
\begin{minipage}[t]{0.30\textwidth}
\centering
\vspace{-1em}
\includegraphics[width=\textwidth]{Lichtuhrdiag.jpg}
\includegraphics[width=\textwidth]{Train.jpg} \\
\end{minipage}

\begin{minipage}[t]{0.87\textwidth}
Dann ist mit dem Satz des Pythagoras
\begin{equation*}
\begin{aligned}
 & c^2t^2+v^2t'^2=c^2t'^2 \\
 \Leftrightarrow \quad & t' = \sqrt{\frac{c^2t^2}{c^2-v^2}} = \sqrt{\frac{c^2t^2}{c^2(1-\frac{v^2}{c^2})}} = \frac{t}{\sqrt{1 - \frac{v^2}{c^2}}}
\end{aligned}
\end{equation*}
\end{minipage}\hfill
\begin{minipage}[t]{0.13\textwidth}
\centering
\vspace{-1em}
\includegraphics[width=\textwidth]{Pyth.jpg}
\end{minipage}

Dies ist die \textbf{Zeitdilatationsgleichung}.

Hierbei gilt:
\begin{itemize}
  \item $t$ $\dots$ Eigenzeit / proper time (Zeit von bewegtem System, aus der Sicht des ruhenden Beobachters; ruhender Beobachter sieht wie die Person \textit{im} Zug ihre \textit{eigene} Zeit hat)
  \item $t'$ $\dots$ dilated time (Zeit, die vom ruhenden Beobachter für sich selbst gemessen wird)
  \item $v$ $\dots$ Geschwindigkeit des bewegten Objekts relativ zum ruhenden Beobachter
\end{itemize}


Es gilt $lim_{v \to c} t' = \infty$. Also braucht alles außer Licht unendlich viel Zeit, um Lichtgeschwindigkeit zu erreichen. Daher kann nichts, außer Licht, mit Lichtgeschwindigkeit unterwegs sein.

Der Faktor
\begin{equation*}
\gamma = \frac{1}{\sqrt{1 - \frac{v^2}{c^2}}}
\end{equation*}

heißt \textbf{Lorentz-Faktor}.

Damit gilt
\begin{equation*}
\boxed{
t' = \gamma \cdot t
}
\end{equation*}

Da $\frac{v^2}{c^2}<1$ für alles außer Licht, ist $\sqrt{1 - \frac{v^2}{c^2}} \in (0,1)$ (Bruch) und somit $\gamma > 1$.

Aus Sicht der ruhenden Person $B$ außerhalb des Zugs, die ihre Zeit mit $t'$ misst, vergeht die Zeit der Person $A$ im Zug mit $t=\frac{t'}{\gamma}$. Also ist die Zeit im Zug, aus Sicht von außerhalb, um den Faktor $\gamma$ langsamer. (z.B. $t'=\SI{2}{s}$ vergeht außen, dann vergeht, für $\gamma =2$, die Zeit $t=\SI{1}{s}$ innen) 

\renewcommand{\arraystretch}{1.5}   % Row spacing
\rowcolors{2}{gray!10}{white}       % Alternating row colors
% Table starts here
\begin{table}[h]
\centering
\begin{tabular}{c c}

\rowcolor{cyan!20}
\textbf{$\frac{v}{c}$} & \textbf{$\gamma$} \\
1\% & 1,0005 \\
10\%  & 1,005 \\
50\% & 1,2 \\
87\% & 2 \\
99\% & 7 \\
99,9\%  & 22 \\
99,99\%   & 70 \\
\end{tabular}
\caption{Lorentz-Faktor zu Geschwindigkeit gegeben in Prozent der Lichtgeschwindigkeit. \newline
1\% = \SI{1079252848,8}{km/h}}
\end{table}
\rowcolors{1}{}{}  % beendet die abwechselnde Zeilenfärbung


Für $v \ll c$ ist $\gamma \approx 1$, und die Zeitdilatation ist praktisch nicht bemerkbar - die Newtonsche Mechanik bleibt gültig.



\begin{remark}
Zeitdilatation ist symmetrisch. Aus Sicht der Person im Zug bewegt sich der außenstehende Beobachter mit der Geschwindigkeit $v$ in die entgegengesetzte Richtung. Damit gilt die Zeitdilatation auch hier, wobei nun $t'$ die Zeit \textit{im} Zug und $t=\frac{t'}{\gamma}$ die Zeit \textit{außerhalb} ist. Also ist die Zeit außerhalb, aus der Perspektive von innen, um den Faktor $\gamma$ langsamer.
\end{remark}

Die Zeitdilatation beeinflusst tatsächlich den Alltag. Satelliten und GPS-Systeme müssen aufgrund ihrer hohen Geschwindigkeit und der allgemeinen Relativität (Gravitationseinfluss) regelmäßig korrigiert werden.

\heading{Längenkontraktion}

{\footnotesize gute Videos: \newline
Time Dilation and Length Contraction | Special Relativity: \url{https://youtu.be/bArTzG3Mkmk} \newline
Special Relativity Part 3: Length Contraction: \url{https://youtu.be/FPzGAksFCbs}}

% \begin{floatingfigure}[r]{0.3\textwidth}
% \vspace{-2.5em}
% \centering
% \includegraphics[width=0.3\textwidth]{Zug2.jpg}
% \end{floatingfigure}

\begin{wrapfigure}[4]{r}{0.25\textwidth}
\vspace{-2.8em}
\centering
\includegraphics[width=\linewidth]{Zug2.jpg}
\end{wrapfigure}

In einem ruhenden Zug befindet sich an der Vorderseite ein Spiegel. Ein Photon wird von einer Quelle an der Rückseite auf den Spiegel geschossen und erreicht die Rückseite wieder nach Zeit $t$. Also ist die Länge des Zuges $L'=\frac{ct}{2}$ und die Zeit $t=2\frac{L'}{c}$.

% \begin{floatingfigure}[r]{0.3\textwidth}
% \centering
% \includegraphics[width=0.3\textwidth]{Zug3.jpg}
% \end{floatingfigure}

\begin{wrapfigure}[6]{r}{0.3\textwidth}
\vspace{-2em}
\centering
\includegraphics[width=\linewidth]{Zug3.jpg}
\end{wrapfigure}

Bewegt sich der Zug nun mit Geschwindigkeit $v$, so braucht das Photon länger, um den vorderen Spiegel zu erreichen, da sich dieser wegbewegt, und weniger lang die Rückseite zu erreichen, da sich diese auf das Photon zubewegt. Für einen außenstehenden Beobachter braucht das Photon nun Zeit $t$ für die Distanz $L$, wobei $t'=\gamma \cdot t$.

Setze $t$ in $t=2\frac{L'}{c}$ ein:
\begin{equation}
    \frac{t'}{\gamma}=2\frac{L'}{c}
    \label{eq:time-dilation-length}
\end{equation}


Andererseits ist die Länge von der Rückseite zur Vorderseite $L=t_1(c-v)$ bzw. $t_1=\frac{L}{c-v}$ und von der Vorderseite wieder zur Rückseite $L=t_2(c+v)$ bzw. $t_2=\frac{L}{c+v}$.

Dann ist
\begin{equation*}
t' = t_1 + t_2 = \frac{L}{c - v} + \frac{L}{c + v} = \frac{(c+v)L+(c-v)L}{(c+v)(c-v)} =L\frac{c+v+c-v}{c^2-v^2} = \frac{2Lc}{c^2-v^2}=\frac{2Lc}{c^2(1-\frac{v^2}{c^2})}=\frac{2L}{c}\gamma^2
\end{equation*}

Setze in \eqref{eq:time-dilation-length} ein: 
\begin{align*}
    & \frac{2L}{c}\gamma^2 \cdot \frac{1}{\gamma}=2\frac{L'}{c} \\
    & \Leftrightarrow \ L\gamma=L'
\end{align*}
\begin{equation*}
\boxed{
 \ L=\frac{L'}{\gamma}
}
\end{equation*}

Dies ist die \textbf{Längenkontraktionsgleichung}.

Dabei ist:
\begin{itemize}
  \item $L'$ $\dots$ Eigenlänge / proper length (Länge des ruhenden Objekts; oft auch mit $L_0$ bezeichnet)
  \item $L$ $\dots$ kontrahierte Länge (Länge des bewegten Objekts)
\end{itemize}

Das bedeutet:  
Ein bewegtes Objekt erscheint in Bewegungsrichtung kürzer, und zwar um den Faktor $\gamma$. Genauer: wenn der Zug eine Initiallänge $L'$ im ruhenden Zustand hatte, hat er mit der Geschwindigkeit $v$ die Länge $L=\frac{L'}{\gamma}$, die wegen $\gamma>1$ kürzer ist als $L'$.

\heading{Lorentz-Transformation}

Ist der Beobachter $A$ in Inertialsystem $I'$ mit konstanter Geschwindigkeit $v$ in $x$-Richtung gegenüber einem anderen Beobachter $B$ in Inertialsystem $I$ bewegt, so hängen die Koordinaten $(t', x', y', z')$, die Beobachter $A$ einem Ereignis zuschreibt, durch die \textit{Lorentz-Transformation} mit den Koordinaten $(t, x, y, z)$ des Beobachters $B$ für dasselbe Ereignis zusammen, falls die beiden Bezugssysteme $I$ und $I'$ denselben Ursprung haben, das heißt, zum Zeitpunkt $t = t' = 0$ miteinander übereinstimmen. 

\begin{equation*}
  \begin{aligned}
    t' &= \gamma \left( t - \frac{v}{c^2} x \right) \\
    x' &= \gamma (x - v t) \\
    y' &= y \\
    z' &= z \\
  \end{aligned}
\end{equation*}

wobei $\gamma = \frac{1}{\sqrt{1 - \frac{v^2}{c^2}}}$ der Lorentz-Faktor ist.


\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\textwidth]{Lorentz.jpg}
  \caption*{Darstellung der Lorentz-Transformation: Zwei Beobachter $A$ und $B$ in unterschiedlichen Inertialsystemen $I$ und $I'$. $r$ ist der Ortsvektor des Ereignisses.}
  \vspace{0.1em}
  {\footnotesize Quelle: \url{https://en.wikipedia.org/wiki/Lorentz_transformation#/media/File:Lorentz_boost_x_direction_standard_configuration.svg}}
\end{figure}


\heading{Gleichzeitigkeit}

\begin{floatingfigure}[r]{0.15\textwidth}
\centering
\includegraphics[width=0.15\textwidth]{Gleichzeit.jpg}
\end{floatingfigure}

Auch Gleichzeitigkeit ist relativ. Eine Person im fahrenden Zug befindet sich in der Mitte und sendet zwei Lichtblitze aus - einen nach vorne, einen nach hinten. Für sie treffen beide gleichzeitig an den Enden des Wagens ein.

Ein außenstehender Beobachter sieht jedoch, dass der Lichtstrahl zuerst am hinteren Ende auftrifft und danach am vorderen, da sich die hintere Wand auf den Strahl zubewegt und die vordere sich wegbewegt.

\heading{Zusammenfassung}

Die spezielle Relativitätstheorie beruht auf zwei Grundannahmen:
\begin{enumerate}
  \item Das galileische Relativitätsprinzip: Alle physikalischen Gesetze sind in allen Inertialsystemen gleich.
  \item Die Lichtgeschwindigkeit $c$ ist konstant.
\end{enumerate}

Daraus folgt, dass die Galilei-Transformation mit $t = t'$ nicht mehr gültig sein kann. Raum und Zeit sind nicht unabhängig, sondern bilden gemeinsam die \textit{Raumzeit}. Jedes Objekt besitzt seine eigene Zeit (Eigenzeit) und seine eigene Länge.  

Anstelle der Galilei-Transformation gilt nun die Lorentz-Transformation.


\section{Geraden in gekrümmten Räumen}

Das Gravitationsgesetz von Newton ist in den meisten Situationen eine sehr gute Näherung. Bei genauen Messungen oder in der Nähe großer Massen treten jedoch Abweichungen auf. Beispiele sind die Lichtablenkung oder die Periheldrehung der Planeten. Solche Effekte lassen sich mit der Newtonschen Mechanik nicht erklären.


\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.4\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Periheldrehung.png}
        \caption{Die Periheldrehung der Merkurbahn wird auch durch Raumzeitkrümmung verursacht.}
        {\footnotesize{Quellen: \href{https://www.researchgate.net/figure/Periheldrehung-der-Merkurbahn-eine-Konsequenz-der-Raumkruemmung-nach-Hoffmann_fig21_233427933}{Link} \newline
        I never understood why planets don’t follow perfect orbits… until now!: \url{https://youtu.be/S78h8zQwQe0?si=UaW3JNNzFQhHuBl7&t=1124}}}
    \end{subfigure}
    \hspace{1em}
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Lichtablenkung.png}
        \caption{Lichtablenkung}
        {\footnotesize{Quelle: \url{https://einstein-website.de/albert-einstein-biografie/}}}
    \end{subfigure}
\end{figure}
\vspace{1em}

Die allgemeine Relativitätstheorie (ART) ersetzt das Konzept der Gravitations\textbf{kraft} durch die Vorstellung einer gekrümmten Raumzeit. Massen verändern die \textbf{Geometrie} der Raumzeit. In dieser gekrümmten Raumzeit bewegen sich Körper auf den kürzesten Wegen zwischen zwei Punkten, also auf „Geraden“, die auf gekrümmten Flächen \textit{Geodäten} genannt werden. Für einen Beobachter, der in der Raumzeit lebt, \textit{scheint} es jedoch so, als würde eine Kraft auftreten (Gravitationskraft).

\begin{example}[Gedankenexperiment: Aufzug]
Wir betrachten eine Person in einem geschlossenen Aufzug. Die Person hat keine Möglichkeit hinauszusehen.

Wenn der Aufzug abstürzt, schwebt die Person im Inneren schwerelos. Gegenstände bleiben an ihrem Ort oder bewegen sich geradlinig mit konstanter Geschwindigkeit weiter, solange keine Kraft auf sie wirkt. Der Zustand des freien Falls ist also lokal nicht von einem kräftefreien Raum zu unterscheiden. 

Diese Beobachtung wird als \href{https://joergresag.hier-im-netz.de/mybkhtml/chap81.htm}{\textbf{starkes Äquivalenzprinzip}} oder \textbf{Einsteinsches Äquivalenzprinzip} bezeichnet:
\begin{quote}
Ein frei fallendes Bezugssystem kann lokal (in einer hinreichend kleinen Umgebung) nicht von einem kräftefreien Bezugssystem unterschieden werden. Frei fallende Bezugssysteme sind lokal äquivalent zu Inertialsystemen. In jedem frei fallenden Bezugssystem gelten lokal dieselben physikalischen Gesetze wie in der Speziellen Relativitätstheorie.
\end{quote}

Es bildet das Fundament der Allgemeinen Relativitätstheorie. Weitere \href{https://www.einstein-online.info/spotlight/aequivalenzprinzip/}{Quelle}.

Global gilt diese Äquivalenz jedoch nicht. Zoomt man weiter heraus, bewegen sich Körper im freien Fall nicht mehr auf Geraden, sondern auf geradest-möglichen Strecken (Kurven), die durch die Gravitation verursacht werden. Beispiel: ein Stift bewegt sich geradlinig mit konstanter Geschwindigkeit in der Schwerelosigkeit der ISS. Weiter herausgezoomt, fällt die ISS um die Erde herum. Global betrachtet bewegt sich der Stift nicht auf einer Geraden.
\end{example}

Einstein erkannte, dass diese Beobachtung darauf hindeutet, dass die Bewegung von Körpern im Gravitationsfeld nicht durch eine Kraft, sondern durch die Krümmung der Raumzeit bestimmt wird. Körper bewegen sich also entlang von Geraden auf dieser gekrümmten Raumzeit.

Um eine solche Bewegung zu beschreiben, müssen wir klären, was eine \textit{Gerade in einem gekrümmten Raum} bedeutet. Im euklidischen Raum ist sie die kürzeste Verbindung zwischen zwei Punkten. Allgemein gilt:
\begin{itemize}
    \item Die kürzeste Verbindung zwischen zwei Punkten verläuft entlang einer Geraden.
    \item Der Tangentialvektor einer Geraden bleibt stets zu sich selbst parallel. Diese Eigenschaft nennt man \textit{Autoparallelität}.
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=0.2\textwidth]{Gerade.jpg}
\caption{In blau: eine Gerade im flachen Raum, denn alle Tangentialvektoren sind parallel. In türkis: keine Gerade den mind. zwei Tangentialvektoren sind nicht parallel.}
\label{fig:image}
\end{figure}

In gekrümmten Räumen wird diese Vorstellung schwieriger. Auf einer Kugeloberfläche liegt die kürzeste Verbindung zwischen zwei Punkten auf einem Großkreis (Kreis mit maximalem Umfang). Von \underline{außen} betrachtet sind die Tangentialvektoren solcher Großkreise jedoch nicht autoparallel, siehe \cref{fig:parallel}. Für eine Person, die auf der Kugeloberfläche lebt, erscheint die Umgebung lokal flach. Daher sind die Tangentialvektoren für diese Person autoparallel.

\begin{figure}[H]
\centering
\includegraphics[width=0.2\textwidth]{Parallel.jpg}
\caption{Tangentialvektoren auf Großkreisen sind von \underline{außen} betrachtet nicht autoparallel}
\label{fig:parallel}
\end{figure}

In einem flachen Raum kehrt ein Vektor, der entlang einer geschlossenen Kurve parallelverschoben wird, wieder zu seiner Ausgangsrichtung zurück. In einem gekrümmten Raum ist das nicht der Fall.

Starten wir mit einem Tangentialvektor auf dem Äquator und verschieben ihn entlang eines Großkreises bis zum Nordpol. Dann folgen wir einem anderen Großkreis zurück zum Äquator und dann entlang des Äquators zurück zum Ausgangspunkt. Obwohl der Vektor auf dem gesamten Weg nur parallel verschoben wurde, kehrt er nicht zur ursprünglichen Richtung zurück, sondern ist gegenüber seiner Anfangsrichtung verdreht. Das zeigt anschaulich, was Raumkrümmung bedeutet.

\begin{figure}[H]
\centering
\includegraphics[width=0.4\textwidth]{Parallelversch.jpg}
\caption{Parallelverschiebung}
{\footnotesize{Quelle: ART Mitschrift Berndl-Forstner (WS20/21) \url{https://forum.fstph.at/t/latex-skriptum-allg-relativitaetstheorie/4021}}}
\end{figure}

\section{Newton und Einstein im Vergleich}

Newton und Einstein erklären Gravitation auf unterschiedliche Weise. Newton beschreibt sie als Kraft, die zwischen Massen wirkt. Einstein beschreibt sie als Folge der Raumzeitkrümmung.

Das lässt sich mit der Kugeloberfläche verdeutlichen. Zwei Körper bewegen sich auf Großkreisen, die durch den Nordpol verlaufen. Beide starten von der Äquatorebene in Richtung Pol. Ein Beobachter auf der Oberfläche weiß nicht, dass er sich in einem gekrümmten Raum befindet. Wenn sich die Körper annähern, deutet er das als gegenseitige Anziehung, also als Kraftwirkung.

Im Einstein’schen Bild bewegen sich beide Körper einfach entlang der kürzesten Wege auf der gekrümmten Oberfläche, also entlang der Großkreise. Die Näherung der beiden ist keine Folge einer Kraft, sondern der Geometrie.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{Einstnew.jpg}
  \caption{Vergleich zwischen Newton und Einstein: Während Newton Gravitation als Kraft interpretiert, versteht Einstein sie als Folge der Raumzeitkrümmung. Die Körper bewegen sich entlang der kürzesten Wege auf der gekrümmten Oberfläche.}
  {\footnotesize{Quelle: ART Mitschrift Berndl-Forstner (WS20/21) \url{https://forum.fstph.at/t/latex-skriptum-allg-relativitaetstheorie/4021}}}
\end{figure}

Die allgemeine Relativitätstheorie beschreibt genau diese Krümmung der Raumzeit. Sie erklärt Gravitation ohne das Konzept einer Kraft. In den folgenden Kapiteln wird der dafür notwendige mathematische Rahmen entwickelt. Am Ende steht die zentrale Gleichung der allgemeinen Relativitätstheorie: die \textit{Einsteingleichung}.


\subsection{Übergang zu allgemeiner Relativitätstheorie (nicht Teil der VO)}

{\footnotesize gute Videos: \newline
General Relativity Explained simply \& visually: \url{https://youtu.be/tzQC3uYL67U} \newline
I finally rediscovered why matter curves spacetime! \url{https://youtu.be/qKZvAwcXceE} \newline
General Relativity: The Curvature of Spacetime: \url{https://youtu.be/R7V3koyL7M}}


Die spezielle Relativitätstheorie (SRT) gilt nur für Inertialsysteme, also für Bezugssysteme ohne Beschleunigung und insbesondere ohne Rotation oder Gravitation. Die allgemeine Relativitätstheorie (ART) erweitert diesen Rahmen auf beschleunigte Systeme und formuliert Gravitation im Sinne der Relativitätstheorie.

\heading{Konflikt: wieder die Lichtgeschwindigkeit}

In Newtons Theorie wirkt Gravitation augenblicklich und über beliebig große Entfernungen hinweg. Das widerspricht jedoch der Tatsache, dass nichts (also auch keine Kraftwirkung) schneller als die Lichtgeschwindigkeit sein kann.

\heading{Einsteins Gedankenexperiment}
 
Eine Person in einem geschlossenen Raumschiff kann nicht unterscheiden, ob sich das Raumschiff
\begin{itemize}
    \item auf der Erde befindet und die Person die Gewichtskraft des Gravitationsfeldes $F_G = m g$ erfährt (\textit{schwere Masse}),
    \item oder ob sich das Raumschiff im Weltraum befindet und mit $g$ nach oben beschleunigt wird (\textit{träge Masse}; Widerstand bei Beschleunigung).
\end{itemize}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\textwidth]{equivalence1.jpg}
  \caption{Gleichwertigkeit von Beschleunigung und Gravitation}
  {\footnotesize{Quelle: \url{https://asymptotia.com/wp-images/2008/02/equivalence1.jpg}}}
\end{figure}

Schwere Masse und träge Masse sind also lokal äquivalent/gleichwertig. Dies ist eine weitere Formulierung des \textbf{starken Äquivalenzprinzips}.

Einstein führte diesen Gedanken weiter. Wenn Gravitation und Beschleunigung äquivalent sind, dann kann Gravitation nicht als Kraft verstanden werden. Massen verändern stattdessen die Geometrie der Raumzeit. Körper bewegen sich in dieser gekrümmten Raumzeit entlang des kürzesten Wegs zwischen zwei Punkten (Geodäte).

Gravitation ist somit Ausdruck der Raumzeitkrümmung: Sie beschreibt, wie sich die Geodäten in der Nähe von Massen verändern. Bewegung im Gravitationsfeld folgt also nicht einer Kraft, sondern der Struktur der Raumzeit selbst.

In der speziellen Relativitätstheorie ist die Raumzeit flach, in der allgemeinen Relativitätstheorie dagegen gekrümmt.

Die allgemeine Relativitätstheorie sagte auch schwarze Löcher vorher und dass Uhren in der Nähe von großen Massen langsamer ticken.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{spacetime.png}
    \caption{Krümmung der Raumzeit in 3D}
    {\footnotesize{Quelle (bewegtes Bild): \url{https://1ucasvb.tumblr.com/post/142549026838/figured-id-add-a-mass-in-there-see-also}}}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{2d curve.png}
    \caption{Krümmung der Raumzeit in 2D}
    {\footnotesize {Quelle: \url{https://hiteshsahu.com/lab/relativity/}}}
\end{figure}


\chapter{Minkowski-Geometrie und SRT}\label{chap:Minkowski-Geometrie und SRT}

\section{Der Minkowski-Raum}

{\footnotesize {gute Videos: \newline
4D Spacetime and Relativity explained simply and visually: \url{https://youtu.be/ZfR1Jc6Zgl} \newline
Weltlinien und Minkowski-Diagramm - Spezielle Relativitätstheorie (5) | Peter Kroll: \url{https://youtu.be/ef7X9-1qhkA?si=v6eHwvrXmXKqFNKo&t=1339}}}

Der \textbf{Minkowski-Raum} $\mathbb{M}$ ist die mathematische Grundlage der speziellen Relativitätstheorie. Er beschreibt die Raumzeit als einen vierdimensionalen Raum, in dem drei Raumrichtungen und eine Zeitrichtung gemeinsam betrachtet werden. 

Die drei räumlichen Koordinaten $x^i$ beschreiben den gewohnten euklidischen Raum. Die vierte Koordinate steht für die Zeit $t$. Sie wird mit der Lichtgeschwindigkeit $c$ multipliziert, damit auf allen Achsen mit Längen gearbeitet werden kann: $ct=\frac{km}{s}\cdot s = km$.

Damit ist der Minkowski-Raum
\begin{equation*}
\mathbb{M}:=\{(\underbrace{ct}_{=x^0}, x^1, x^2, x^3) \in \mathbb{R}^4\}
\end{equation*}
Dennoch unterscheidet sich der Minkowski-Raum wesentlich von einem vierdimensionalen euklidischen Raum.

Zur Veranschaulichung verwendet man sogenannte \textit{Minkowski-Diagramme} oder auch \textit{Raumzeit-Diagramme}. Da es unmöglich ist, vier aufeinander normal stehende Achsen graphisch darzustellen, werden für solche Diagramme die Raumdimensionen von drei auf eine oder zwei reduziert, und eine Zeitachse eingeführt, die meistens nach oben zeigt.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\textwidth]{Minkow.jpg}
    \caption{Minkowski-Raum} 
\end{figure}


\subsection{Weltlinien und Gleichzeitigkeit}

Im Minkowski-Diagramm stellt jeder Punkt ein Teilchen oder auch ein Ereignis dar, also einen Ort zu einem bestimmten Zeitpunkt. Die Bahn eines Teilchens durch die Raumzeit wird durch eine sogenannte \textit{Weltlinie} in diesem Diagramm beschrieben.

Ein Körper, der ruht, wird durch eine vertikale Linie dargestellt, da sich sein Ort in der Zeit nicht ändert. Ein Körper, der sich mit konstanter Geschwindigkeit bewegt, hat eine geneigte Weltlinie. Je größer die Geschwindigkeit, desto flacher verläuft die Linie. Parallele Weltlinien beschreiben Körper, die sich in relativer Ruhe befinden. Auf der \textbf{Gleichzeitebene} erfahren alle Teilchen die gleiche Zeit/alle Ereignisse/Events finden zur gleichen Zeit statt.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{Min1.jpg}
  \caption{Weltlinien im Minkowski-Diagramm. $x_1$ \& $x_2$ sind ruhende Körper, $x_3$ \& $x_4$ bewegen sich gleichförmig mit derselben Geschwindigkeit.}
\end{figure}

Wenn sich $x_3$ nun mit der gleichen Geschwindigkeit $v$ in alle Richtungen bewegt, entsteht ein Kegel, der sogenannte \textbf{Gleichgeschwindigkeitskegel}. Der Winkel $\theta$ des Gleichgeschwindigkeitskegels ist also durch die Geschwindigkeit bestimmt. Desto größer die Geschwindigkeit des Teilchens, desto größer der Winkel.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{Kegel.jpg}
  \caption{Gleichgeschwindigkeitskegel von $x_3$. Punkt $x_3''$ liegt auf unbewegten Weltlinie von $x_3$.}
\end{figure}

In der Newton-Mechanik werden Geschwindigkeiten addiert. Also kann eine unendlich große Geschwindigkeit auftreten (deutsche Autobahn). Das bedeutet, dass der Winkel eines Gleichgeschwindigkeitskegels $\theta = 90^\circ$ erreichen kann. Die wesentliche Erkenntnis der speziellen Relativitätstheorie ist, dass es eine maximale Geschwindigkeit gibt: die Lichtgeschwindigkeit $c$. Diese ist in allen Bezugssystemen gleich – also eine Invariantengeschwindigkeit. Also kann ein Gleichgeschwindigkeitskegel die $90^\circ$ nur annähern, aber nicht erreichen.

Skaliere so, dass $v = c$ einem Winkel $\theta = 45^\circ$ entspricht. (Man könnte auch anders wählen; die Idee ist, dass der Winkel zwischen $0^\circ$ und $90^\circ$ liegen muss, wobei $0^\circ$ und $90^\circ$ nicht möglich sind – also nimmt man die Mitte.)


\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{Lichtkegel.jpg}
  \caption{$x_1$ sendet Licht aus und erzeugt somit den Lichtkegel.}
\end{figure}


\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{Lichtkeg2d.jpg}
  \caption{Zukunft und Vergangenheit.}
\end{figure}

\pagebreak

\section{Lorentz-Transformation}
{\footnotesize {gute Videos: \newline
Lorentz Transformations | Special Relativity Ch. 3: \url{https://www.youtube.com/watch?v=Rh0pYtQG5wI} \newline
Length Contraction and Time Dilation | Special Relativity Ch. 5: \url{https://www.youtube.com/watch?v=-NN_m2yKAAk} \newline
Relativistische Längenkontraktion • Garagenparadoxon • Spezielle Relativitätstheorie(6)| Peter Kroll \url{https://youtu.be/fHGFZ2GVDkM?si=hlNgn3ipegW7KE4h&t=770}}}

\subsection{Gleichzeitigkeit, Zeitdilatation und Längenkontraktion}

Bei Newton sind Zeit und Längen absolut und unabhängig. Wenn sich ein Teilchen bewegt, nimmt es bei Newton dieselbe Zeit und Länge war, wie ein ruhendes Teilchen. Die Gleichzeitebenen verändern sich nicht. Die spezielle Relativitätstheorie zeigt, dass Gleichzeitigkeit, Zeit und Länge vom Bezugssystem/Beobachter abhängen.


\begin{minipage}[t]{0.75\textwidth}
\textbf{Gleichzeitigkeit} hängt vom Bezugssystem/Beobachter ab. Sei ein Stab im Koordinatenursprung platziert. Der Stab bewegt sich gleichmäßig mit Geschwindigkeit $v$ nach rechts. Betrachte zwei Bezugssysteme: ein fixes $I$ und ein mitbewegtes $I'$. Im System $I$ treten die Ereignisse $x_1$ und $x_2$ nicht gleichzeitig ein. Das Licht trifft $x_1$ viel früher. Im System $I'$ treten sie gleichzeitig ein: die Linie $x_1 x_2$ ist eine Gleichzeitigkeitsebene in $I'$ (geht durch die Schnittpunkte der Weltlinien von A und E mit dem Lichtkegel). Alle Linien, die parallel dazu sind, sind auch Gleichzeitigkeitsebenen in $I'$.
\end{minipage}\hfill
\begin{minipage}[t]{0.25\textwidth}
\centering
\vspace{1em}
\includegraphics[width=\linewidth]{Stab.jpg}
\end{minipage}


\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\textwidth]{Lorentztrafo.jpg}
  \caption{Die Beziehung zwischen $I$ und $I'$ wird durch eine Lorentz-Transformation beschrieben.}
\end{figure}

Zweite Möglichkeit zur Konstruktion einer Gleichzeitebene in $I'$:\newline
Da wir den Lichtkegel mit 45° einzeichnen, ist der Abstand von $t$ (zeitlicher Abstand) vom Ursprung zu einem Ereignis gleich dem Abstand von $x$ (räumlicher Abstand) von $t$ zum Lichtkegel (siehe \cref{fig:Lorentztrafo2}). Da die Lichtgeschwindigkeit im bewegten System $I'$ ebenso gleich schnell ist, muss auch dort der zeitliche Abstand $t'$ und der räumliche Abstand $x'$ zum Lichtkegel übereinstimmen.

Der Begriff "Abstand" wurde hier noch nicht korrekt eingeführt. Wie wir später sehen werden, können wir genaugenommen nur über ein Abstandsquadrat sprechen.

\begin{enumerate}
    \item Zeichne bewegte Weltlinie ein.
    \item Nehme einen beliebigen Punkt auf der bewegten Weltlinie her und trage den Vektor $t'$ ein.
    \item Trage anschließend, ausgehend von der Spitze vom Vektor $t'$, einen Vektor $x'$ mit derselben Länge von $t'$ so ein, dass dessen Spitze den Lichtkegel trifft.
    \item Die Gleichzeitebenen im bewegten System $I'$ sind nun diejenigen, die parallel zu $x'$ sind.
\end{enumerate}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{images/Mindiag.jpg}
    \caption{Konstruktion einer Gleichzeitebene in $I'$ mit nur einer Weltlinie.}
    \label{fig:Lorentztrafo2}
\end{figure}



\textbf{Zeit} hängt vom Bezugssystem/Beobachter ab.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{Zeitdilatation.jpg}
  \caption{Zeitdilatation: Aus der Sicht von dem ruhenden $x_1$, läuft die Zeit von sich selbst normal, aber die Zeit vom bewegten $x_2$ um 0,3 s langsamer.}
\end{figure}

Dies kann auch mit dem Satz des Pythagoras gezeigt werden. In \cref{fig:zeitdil1} gilt
\begin{equation*}
t^2 < t^2 + x^2 = \bar{t}^2 
\end{equation*}

Also läuft die Zeit $\bar{t}$ im bewegten System langsamer als im ruhenden.
\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{images/Zeitdil.jpg}
\caption{Zeitdilatation}
\label{fig:zeitdil1}
\end{figure}


Auch \textbf{Länge} ist beobachterabhängig. Die Länge eines Objekts ist der Abstand des Anfangs- und Endpunktes gleichzeitig gemessen. Die Länge bewegter Objekte wird aus der Sicht des ruhenden Beobachters kürzer.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{Längenkont.jpg}
  \caption{Längenkontraktion: Ein Tisch bewegt sich mit Geschwindigkeit v. Die ruhende Person nimmt die Länge $l$ für den bewegten Tisch wahr, der im ruhenden Zustand die Länge $L$ hatte. Da $l < L$, nimmt die ruhende Person den Tisch kürzer wahr. (Kontruktion der Gleichzeitebenen von $I'$ gleich wie bei \cref{fig:Lorentztrafo2})}
\end{figure}



\subsection{Einheiten und einsteinsche Summenkonvention}

Um die Notation zu vereinfachen, wird die Lichtgeschwindigkeit $c = 1$ gesetzt. Also messen wir nun die Zeit in Längen. Um die Formeln in SI-Einheiten zu erhalten, kann anschließend mit $c$ multipliziert werden.

\begin{definition}[Einsteinsche Summenkonvention]
Über doppelt vorkommende Indizes wird summiert.  
Das heißt, jedes Mal, wenn ein Index zweimal innerhalb eines Terms vorkommt, ist implizit die Summe über diesen Index gemeint.

\begin{equation*}
  \sum_{i=1}^{3} a^i b^i \;\; \widehat{=}\;\; a^i b^i \;\; \widehat{=}\;\; \delta_{ij} a^i b^j
\end{equation*}

\textbf{Achtung:}  
Die Summenkonvention gilt nur, wenn Indizes genau \textit{zweimal} vorkommen und nicht mehrfach!

\begin{equation*}
  a^i b^i c^i \;\neq\; \sum_{i=1}^{3} a^i b^i c^i
\end{equation*}
\end{definition}

\subsection{Herleitung generelle Lorentz-Transformation}
Die bereits im vorherigen Kapitel verwendete Lorentz-Transformation soll nun in der generellen Form (also mit Bewegung in x-, y- und z-Richtung) aus dem Galileischen Relativitätsprinzip \cref{thm:Relativitätsprinzip} hergeleitet werden, \textbf{ohne} dass die Konstanz der Lichtgeschwindigkeit verwendet wird.

Die \textbf{Lorentz-Transformation} beschreibt eine Koordinatentransformation von einem Inertialsystem $I$ in ein dagegen in $x$-, $y$- und $z$-Richtung mit der Geschwindigkeit $v^i$ (also $\vec v = (v_x, v_y, v_z)$) bewegtes Inertialsystem $\bar{I}$. $I$ hat die Koordinaten $(t, x^i)$, $\bar{I}$ hat die Koordinaten $(\bar{t}, \bar{x}^i)$

\begin{equation*}
\begin{aligned}
\bar{t} &= \gamma \left( t - \frac{v^j x^j}{c^2} \right) \\[0.5em]
\bar{x}^i &= x^i - \gamma v^i t + (\gamma - 1)\frac{v^i v^j x^j}{v^2}
\end{aligned}
\qquad
\text{wobei } \gamma = \frac{1}{\sqrt{1 - \frac{v^2}{c^2}}} > 1
\end{equation*}


Da wir uns in Inertialsystemen befinden, gibt es keine beschleunigten Bewegungen. Teilchen bewegen sich auf geraden Weltlinien. (Eine beschleunigte Bewegung wäre eine gekrümmte Weltlinie). Ein Bezugssystemwechsel muss daher Geraden auf Geraden sowie Ebenen auf Ebenen abbilden. Mathematisch besagt dies, dass die Transformation \textbf{linear} ist.

Nach dem galileischen Relativitätsprinzip gibt es weder ausgezeichnete Orte noch ausgezeichnete Raumrichtungen (Homogenität (an allen Orten gleich) + Isotropie (in alle Richtungen gleich)). Daher darf die einzige ausgezeichnete räumliche Richtung, die in der Transformation auftreten kann, der Relativgeschwindigkeitsvektor $v^i$. D.h. die Transformation muss \emph{drehinvariant} sein. In einem Bezugssystem, das gegenüber dem anderen verdreht ist, ändert sich die Physik nicht.

Gesucht ist also eine lineare Transformation der Form
\begin{equation*}
\left(
\begin{array}{c}
\bar{t} \\
\bar{x}^i
\end{array}
\right)
=
A(v^i) \cdot
\left(
\begin{array}{c}
t \\
x^j
\end{array}
\right)
\end{equation*}

Da $\bar{t}$ ein Skalar ist und $\bar{x}^i$ die Komponenten eines Vektors sind, hat die allgemeinste lineare Form die Gestalt
\begin{align*}
\bar{t} &= \alpha\,t + \beta^j x^j\\
\bar{x}^i &= \gamma^i t + M^{ij}x^j
\end{align*}
wobei $\alpha$ ein Skalar, $\beta^j$ ein Vektor, $\gamma^i$ ein Vektor und $M^{ij}$ eine Matrix sind.

Wegen der Drehinvarianz darf $\beta^j$ nur proportional zum einzigen ausgezeichneten Vektor $v^j$ sein, also $\beta^j=b(v)\,v^j$, wobei die Koeffizienten nur vom Betrag $v=\sqrt{v^iv^i}$ abhängen können. Entsprechend setzen wir $\alpha=a(v)$.

Analog folgt für den Vektorterm $\gamma^i=d(v)\,v^i$. Für den Matrixterm ergibt sich ebenso wegen der Drehinvarianz $M^{ij} = e(v)\, \delta^{ij} + f(v) \, v^i v^j$.

Wir erhalten
\begin{align*}
\bar{t} &= a\,t + b\,v^j x^j \\
\bar{x}^i &= d\,v^i t + e\,x^i + f\,v^i(v^j x^j)
\end{align*}

wobei $a(v),\,b(v),\,d(v),\,e(v),\,f(v)$ zunächst unbekannte Funktionen des Betrags der Relativgeschwindigkeit $v=\sqrt{v^i v^i}$ sind.

Diese Transformation ist dann in Matrixform:

\begin{equation}
\left(
\begin{array}{c}
\bar{t} \\
\bar{x}^i
\end{array}
\right)
=
\underbrace{
\left(
\begin{array}{cc}
a & b\,v^j \\
d\,v^i & e\,\delta^{ij} + f\,v^i v^j
\end{array}
\right)
}_{\displaystyle = A(v^i)}
\left(
\begin{array}{c}
t \\
x^j
\end{array}
\right)
\label{eq:transformation}
\end{equation}


wobei $b\,v^j$ ein Zeilenvektor, $d\,v^i$ ein Spaltenvektor und $e\,\delta^{ij} + f\,v^i v^j$ eine 3x3 Matrix ist.

\underline{\textbf{Ziel:}} Bestimmung Koeffizienten $a(v),\,b(v),\,d(v),\,e(v),\,f(v)$

Wir verwenden dazu folgende Bedingungen:
\begin{enumerate}
    \item Bedingung: Die Transformation soll \textbf{stetig} sein. Weiters soll eine Rücktransformation mit $-v^i$ muss wieder dasselbe Bezugssystem liefern, also die \textbf{Zusammensetzung der beiden Lorentz-Transformationen} muss der \textbf{identischen Abbildung} entsprechen:

\begin{equation*}
    I \xrightarrow{v^i} \bar{I} \xrightarrow{-v^i} I \ = id
\end{equation*}

Das heißt, die Matrix $A(v^i)$ muss mit $A(-v^i)$ die Einheitsmatrix ergeben $A(-v^i) A(v^i) = E$:
\begin{equation*}
\left(
\begin{array}{cc}
a & -b\,v^k \\
-d\,v^i & e\,\delta^{ik} + f\,v^i v^k
\end{array}
\right)
\left(
\begin{array}{cc}
a & b\,v^j \\
d\,v^k & e\,\delta^{kj} + f\,v^k v^j
\end{array}
\right)
\overset{!}{=}
\left(
\begin{array}{cc}
1 & 0^k \\
0^i & \delta^{ij}
\end{array}
\right)
\end{equation*}

Ausmultiplizieren der Matrizen ergibt:
\begin{align*}
\text{(I)} \quad & a^2 - b d\,v^2 = 1 \\[0.4em]
\text{(II)} \quad & a b\,v^j - b f\,v^2 v^j = b\,v^j(a - e - f v^2) = 0 \\[0.4em]
\text{(III)} \quad & a d\,v^i + d f\,v^i v^2 = d\,v^i(a - e - f v^2) = 0 \\[0.4em]
\text{(IV)} \quad & -b d\,v^i v^j + e^2 \delta^{ij} + 2 e f\,v^i v^j + f^2 v^i v^j v^2 = \delta^{ij}
\end{align*}

Für $v=0$ ist $I = \bar{I}$. Sei also $v\ne 0$.

Aus Gleichung (II) folgt, dass Fall 1: $bv^j=0$ (also b=0) oder Fall 2: $a-e-fv^2=0$. Betrachte nur den Fall 2. Man kann zeigen, dass Fall 1 auf einen Spezialfall führt, der von den anderen Transformationen auch abgedeckt wird.

Aus Gleichung (III) folgt, dass Fall 1: $dv^i=0$ (also b=0) oder Fall 2: $-a+e+fv^2=0$. Betrachte nur den Fall 2. Selbes Argument wie vorhin. Beachte, dass $-a+e+fv^2=0$ einfach $a-e-fv^2=0$ mal $-1$ ist.

Aus Gleichung (IV) folgt, dass $e^2=1$ und $-bd+2ef+f^2v^2=0$.

Erhalte also
\begin{align*}
\text{(I')} \quad & a^2 - b d\,v^2 = 1 \\[0.4em]
\text{(II')} \quad & a-e-fv^2=0 \\[0.4em]
\text{(III')} \quad & e^2=1 \\[0.4em]
\text{(IV')} \quad & -bd+2ef+f^2v^2=0
\end{align*}


Aus Gleichung (III') folgt unmittelbar:
\begin{equation*}
e^2 = 1 \quad \Rightarrow \quad e = \pm 1.
\end{equation*}
Für $v = 0$ soll die Transformation in die Identität übergehen:

\begin{equation*}
\underbrace{
\left(
\begin{array}{cc}
a & b\,0^j \\
d\,0^i & e\,\delta^{ij} + f\,0^i 0^j
\end{array}
\right)
}_{\displaystyle = A(0^i)}
=
\left(
\begin{array}{cc}
1 & 0^k \\
0^i & \delta^{ij}
\end{array}
\right)
\end{equation*}

Daher ist $e = 1$ bei $v=0$. Da die Transformation stetig sein soll, soll $e$ es auch sein und kann somit keine Sprungstelle besitzen, bei der zu -1 gewechselt wird. Daher ist $\forall v$:
\begin{equation*}
    \boxed{e=1}
\end{equation*}

Aus (II') folgt nun mit $e=1$:
\begin{equation}
a - 1 = f v^2
\quad \Rightarrow \quad
\boxed{f = \frac{a - 1}{v^2}}
\label{eq:f}
\end{equation}

    \item Bedingung: Ein Körper, der sich gleichförmig mit $v^i$ bewegt, kann durch eine Transformation in Ruhe gebracht werden. 
    
    Wähle eine Transformation in das Bezugssystem, dass sich mit dem bewegten Körper mitbewegt. In diesem Fall ist dann die Geschwindigkeit des Körpers im neuen Bezugssystem gleich 0. Mit $x^j = v^jt$ (Körper bewegt sich mit Geschwindigkeit v) gilt (siehe \eqref{eq:transformation} mit $e=1$):


\begin{equation}
\left(
\begin{array}{cc}
a & b\,v^j \\
d\,v^i & \delta^{ij} + f\,v^i v^j
\end{array}
\right)
\left(
\begin{array}{c}
t \\
v^jt
\end{array}
\right)
=
\left(
\begin{array}{c}
\tau \\
0^i
\end{array}
\right)
\end{equation}

Wobei $\tau$ die Zeit im mitbewegten Bezugssystem ist und $0^i$ die Position des mit $v$ bewegten Objekts. Diese ist im Ursprung, da für $t=0$ dort $0^i$ steht. Da sich die Position des Objekts im mitbewegten Bezugssystem nicht ändert, bleibt es bei der Position $0^i$. $\tau$ ist übrigens die Eigenzeit die, vom ruhenden System im mitbewegten System beobachtet wird.

Ausmultiplizieren ergibt (wobei nur die Ortskomponente relevant ist):
\begin{equation*}
\begin{aligned}
    & dv^it+v^it+fv^2v^it = 0^i \\
    \Leftrightarrow \ & v^it(d+1+\underbrace{fv^2}_{\displaystyle = a-1}) = 0^i \\
    \Leftrightarrow \ & v^it(d+a) = 0^i \\
    \Leftrightarrow \ & d + a = 0 \quad \text{ da i.A. } v\ne 0 \text{ und } t > 0 \\
    \Leftrightarrow \ & \boxed{d = -a}
\end{aligned}
\end{equation*}

Setze $d = -a$ in (I') ein und forme auf $b$ um:

\begin{equation*}
\begin{aligned}
    & a^2 + abv^2 = 1 \\
    \Leftrightarrow \ & \boxed{b = \frac{1-a^2}{av^2}}
\end{aligned}
\end{equation*}

Wir haben nun alle Koeffizienten in Abhängigkeit von $a$ und $v$ bestimmt, womit sich die Lorentz-Transformation anschreiben lässt als:

\begin{equation*}
A(v^i) =
\left(
\begin{array}{cc}
a & \frac{1-a^2}{av^2}v^j \\
- a\,v^i & \delta^{ij} + \frac{a-1}{v^2}v^i v^j
\end{array}
\right)
\end{equation*}

Nun muss noch $a$ bestimmt werden.

\item Bedingung: \textbf{Transitivität}: Zwei hintereinander ausgeführte Lorentz-Transformationen mit unterschiedlichen Geschwindigkeiten, aber in dieselbe Richtung, sollen wieder eine Lorentz-Transformation bilden.

Betrachte dazu einen Koordinatenwechsel zwischen mehreren Inertialsystemen. Das Inertialsystem $\bar{I}$ bewege sich mit der Geschwindigkeit $ve^i$ relativ zu $I$, und ein weiteres System $\bar{\bar{I}}$ bewege sich mit der Geschwindigkeit $u^i$ relativ zu $\bar{I}$. Somit bewegt sich $\bar{\bar{I}}$ relativ zu $I$ mit der zusammengesetzten Geschwindigkeit $we^i$, wobei wir noch nicht wissen wie $we^i$ aussieht.


\begin{center}
\begin{tikzpicture}[scale=1.1,>=stealth]
\node (I) at (0,0) {$I$};
\node (Ip) at (2,0) {$\bar{I}$};
\node (Ipp) at (4,0) {$\bar{\bar{I}}$};
\draw[->] (I) -- (Ip) node[midway,above] {$ve^i$};
\draw[->] (Ip) -- (Ipp) node[midway,above] {$ue^i$};
\draw[->,bend right=25] (I) to node[below] {$we^i$} (Ipp);
\end{tikzpicture}
\end{center}

Die Hintereinanderausführung der beiden Transformationen $A(v^i)$ und $A(u^i)$ muss der direkten Transformation mit der Geschwindigkeit $w^i$ entsprechen. Das heißt:
\begin{equation*}
A(ue^i) \, A(ve^i) = A(we^i)
\end{equation*}

Beziehungsweise

\begin{equation*}
\left(
\begin{array}{cc}
a_u & \dfrac{1 - a_u^2}{a_u u^{\cancel{2}}} \cancel{u}e^i \\
- a_u ue^k & \delta^{ki} + \dfrac{a_u - 1}{\cancel{u^2}} \cancel{u}e^k \cancel{u}e^i
\end{array}
\right)
\left(
\begin{array}{cc}
a_v & \dfrac{1 - a_v^2}{a_v v^{\cancel{2}}} \cancel{v}e^j \\
- a_v ve^i & \delta^{ij} + \dfrac{a_v - 1}{\cancel{v^2}} \cancel{v}e^i \cancel{v}e^j
\end{array}
\right)
\overset{!}{=}
\left(
\begin{array}{cc}
a_w & \dfrac{1 - a_w^2}{a_w w^{\cancel{2}}} \cancel{w}e^j \\
- a_w we^k & \delta^{kj} + \dfrac{a_w - 1}{\cancel{w^2}} \cancel{w}e^k \cancel{w}e^j
\end{array}
\right)
\end{equation*}

wobei $a_v = a(v)$, $a_u = a(u)$ und $a_w = a(w)$

Aus der linken oberen Komponente erhalten wir die Gleichung:
\begin{equation}
a_w = a_v a_u - \frac{1 - a_u^2}{a_u u} a_uu
\label{eq:w1}
\end{equation}

Der rechte untere Block ergibt:
\begin{equation*}
\begin{array}{c}
- a_u ue^k \cdot \dfrac{1 - a_v^2}{a_v v} e^j + (\delta^{ki} + (a_u - 1) e^k e^i ) \cdot (\delta^{ij} + (a_v - 1)e^ie^j) = \delta^{kj} + (a_w - 1)e^ke^j \\
\Bigl(-a_uu\frac{1-a_v^2}{a_v v}+(a_v-1)+(a_u-1)+(a_v-1)(a_u-1)\Bigr) e^k e^j+\underbrace{\delta^{ki}\delta^{ij}}_{\displaystyle = \cancel{\delta^{kj}}} = \cancel{\delta^{kj}} + (a_w-1)e^k e^j 
\end{array}
\end{equation*}

Vergleiche Koeffizienten von $e^k e^j$:
\begin{align*}
a_w - \cancel{1} &= -a_uu\frac{1-a_v^2}{a_v v}+(a_v-1)+(a_u-1)+(a_v-1)(a_u-1)\\
&= -a_uu\frac{1-a_v^2}{a_v v} + a_v a_u - \cancel{1}
\end{align*}

Damit folgt
\begin{equation}
a_w = -a_uu\frac{1-a_v^2}{a_v v} + a_v a_u
\label{eq:w2}
\end{equation}

Setze \eqref{eq:w1} und \eqref{eq:w2} gleich:
\begin{align*}
 & \cancel{a_v a_u} - \frac{1 - a_u^2}{a_u u} a_uu = -a_uu\frac{1-a_v^2}{a_v v} + \cancel{a_v a_u} \\
\Leftrightarrow \quad & \frac{a_u^2 - 1}{(a_u u)^2} = \frac{a_v^2 - 1}{(a_v v)^2} =: K = \text{konstant}\\
\end{align*}

Da die linke Seite nur noch von u und die rechte nur noch von v abhängt und die Gleichheit für alle $u$ und $v$ gelten soll, kann diese nur eintreten, wenn sie konstant sind. ($\forall u,v \in \mathbb{R}: \ g(u)=g(v) \Rightarrow g \equiv const$)

\begin{align*}
& a_v^2-1=K a_v^2 v^2 \\
\Leftrightarrow \quad & a_v^2\left(1-K v^2\right)=1 \\
\Leftrightarrow \quad & a_v= \pm \frac{1}{\sqrt{1-K v^2}}
\end{align*}

Für $v \to 0$, also wenn nur die triviale Transformation vorliegt, muss der Matrixeintrag $a_v$ gegen $1$ gehen. Daher ist für uns nur der positive Zweig der Lösung relevant.

Also gilt:
\begin{equation}
    \boxed{a = \frac{1}{\sqrt{1-K v^2}}}
\label{eq:a}
\end{equation}

\eqref{eq:f} $f=\frac{1 - a^2}{a v^2}$ lässt sich mit Hilfe von $a$ und $K$ ausdrücken:
\begin{equation*}
\frac{1 - a^2}{a v^2} = -Ka
\end{equation*}

Damit schreiben wir die Transformation in Matrixform als
\begin{equation}
\left(
\begin{array}{c}
\bar{t} \\
\bar{x}^i
\end{array}
\right)
=
\left(
\begin{array}{cc}
a & -K a v^j \\
- a v^i & \delta^{ij} + \dfrac{a - 1}{v^2} v^i v^j
\end{array}
\right)
\left(
\begin{array}{c}
t \\
x^j
\end{array}
\right)
\label{eq:lorentztrafo}
\end{equation}


Damit haben wir die \textit{Form} der Lorentz-Transformationen einzig aus dem Galileischen Relativitätsprinzip bereits bestimmt. Bisher ging die Konstanz der Lichtgeschwindigkeit NICHT ein. Wir erhalten eine Familien von Transformationen abhängig von $K$.

Für $K = 0$ ist $a=1$ und wir erhalten die Galilei-Transformation mit einer absoluten Zeit ($t=\bar{t}$). Für $K > 0$ erhalten wir eine endliche Geschwindigkeit, die in allen Inertialsystemen gleich ist. Jetzt müssen wir nur noch K bestimmen.
\end{enumerate}

\underline{Bestimmung von $K$ über den Lichtkegel}\newline
Das \href{https://de.wikipedia.org/wiki/Michelson-Morley-Experiment}{Michelson–Morley-Experiment} zeigte, dass die Lichtgeschwindigkeit konstant ist. Das kann nun verwendet werden, um den Wert von $K$ zu bestimmen. Wir betrachten nun einen Punkt $s=(t, x^1, x^2, x^3) \in \mathbb{M}$ auf dem Lichtkegel. Da wir ohne Beschränkung der Allgemeinheit so skaliert haben, dass der Winkel des Lichtkegels bei 45° liegt, gilt im System $I$:
\begin{equation*}
t^2 = x^i x^i
\end{equation*}
D.h. das Längenquadrat der zeitlichen Komponente stimmt mit dem der räumlichen Komponente überein.

Im System $\bar{I}$, das sich mit der Geschwindigkeit $v$ relativ zu $I$ bewegt, gilt ebenso
\begin{equation*}
\bar{t}^2 = \bar{x}^i \bar{x}^i
\end{equation*}

Licht breitet sich in jedem Inertialsystem gleich schnell aus, daher gilt für einen Punkt auf dem Lichtkegel:

\begin{equation*}
t^2 = x^i x^i \ \Leftrightarrow \ \bar{t}^2 = \bar{x}^i \bar{x}^i
\end{equation*}

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{FindK.jpg}
\caption{Die Längenquadrate von $t$ und $x$ stimmen in $I$ überein genau dann wenn die Längenquadrate von $\bar{t}$ und $\bar{x}$ in $\bar{I}$ übereinstimmen.}
\end{figure}

Mit der Transformation \eqref{eq:lorentztrafo} gilt daher:

\begin{equation}
\begin{aligned}
\bar{t} &= a t - K a v^j x^j \\
\bar{x}^i &= -a v^i t + x^i + \frac{a - 1}{v^2} v^i v^j x^j
\end{aligned}
\label{eq:licht}
\end{equation}

Quadrieren wir diese beiden Gleichungen, so erhalten wir:
\begin{equation}
\begin{aligned}
\bar{t}^{\,2} &= a^2 t^2 - 2 K a^2 v^i x^i t + K^2 a^2 v^2x^2 \\
\bar{x}^2 &= a^2 v^2 t^2+x^2+\frac{(a-1)^2}{v^2}(v x)^2-2 a v x t-2 a(a-1) v x t+2(v x)^2 \frac{a-1}{v^2} \\
\end{aligned}
\label{eq:quadrat}
\end{equation}

Da auf dem Lichtkegel $t^2 = x^2$ und $\bar{t}^2 = \bar{x}^2$ gilt, obiges gleichsetzen und auch das im zweiten Termin vorkommende $x^2$ auf $t^2$ umschreiben.
\begin{equation*}
a^2 t^2+K^2 a^2(v x)^2-2 a^2 K t v x=\left(a^2 v^2+1\right) t^2+\frac{(a-1)^2}{v^2}(v x)^2-2 a^2 v x t+\frac{2(a-1)}{v^2}(v x)^2
\end{equation*}

Koeffizientenvergleich für $t^2$ Terme:

\begin{equation*}
a^2 = a^2 v^2 + 1 \;\Rightarrow\; a^2(1 - v^2) = 1
\end{equation*}

Setze \eqref{eq:a} ein:
\begin{equation*}
\frac{1-v^2}{1-Kv^2}=1 \;\Rightarrow\; \boxed{K=1}
\end{equation*}

\textbf{Es folgt die generelle Lorentz-Transformation:} \newline
Damit kann nun $a$ bestimmt werden:
\begin{equation*}
a= \frac{1}{\sqrt{1 - Kv^2}} = \frac{1}{\sqrt{1 - v^2}} \overset{c^2=1}{=} \frac{1}{\sqrt{1 - \frac{v^2}{c^2}}} =: \gamma\, \dots \text{ Lorentz-Faktor}
\end{equation*}

Setze $K=1$ und $a=\gamma$ in \eqref{eq:licht} ein und erhalte schließlich:
\begin{equation*}
\begin{aligned} 
& \bar{t}=\gamma t-\gamma v^j x^j=\gamma\left(t-v^j x^j\right) \overset{c^2=1}{=} \gamma\left(t-\frac{v^jx^j}{c^2} \right) \\ 
& \bar{x}=-\gamma t v^i+x^i+\frac{\gamma-1}{v^2} v^i x^j v^j
\end{aligned}
\end{equation*}

\section{Minkowski-Metrik und Raumzeitabstand}

Wir haben in der Herleitung der Lorentz-Transformation gesehen, dass für ein Punkt auf dem Lichtkegel in beiden Inertialsystemen $I$ und $\bar{I}$ gilt:
\begin{equation}
-\,t^2 + x^ix^i = 0
\quad\Leftrightarrow\quad
-\,\bar{t}^2 + \bar{x}^i\bar{x}^i = 0
\label{eq:lichtkegel_invarianz}
\end{equation}

Tatsächlich gilt das sogar ganz allgemein, wie wir im Folgenden sehen werden. In \eqref{eq:quadrat} haben wir die Längenquadrate bestimmt:

\begin{align*}
\bar{t}^2 &= a^2 t^2 - 2 K a^2 v x t + K^2 a^2 v^2 x^2 \overset{K=1}{=} a^2 t^2 - 2 a^2 v x t + a^2 v^2 x^2 \\
\bar{x}^2 &= a^2 v^2 t^2 + x^2 + \frac{(a-1)^2}{v^2}(v x)^2 - 2 a v x t - 2 a (a-1) v x t + 2 \frac{a-1}{v^2}(v x)^2 \\
              &= a^2 v^2 t^2 + x^2 + \frac{a^2 - 1}{v^2}(v x)^2 - 2 a^2 v x t
\end{align*}

Damit können wir nun den Ausdruck $-\,\bar{t}^2 + \bar{x}^2$ bilden:
\begin{equation*}
\begin{aligned}
    -\bar{t}^2 + \bar{x}^2 &= -a^2 t^2 + \cancel{2 a^2 v x t} - a^2 v^2 x^2 + a^2 v^2 t^2 + x^2 + \frac{a^2 - 1}{v^2}(v x)^2 - \cancel{2 a^2 v x t} \\
    & = -t^2 a^2 (1 - v^2) + x^2 + (v x)^2 \frac{a^2 - 1}{v^2} \qquad \qquad \qquad \qquad | \ a^2 = \frac{1}{1 - v^2}\\ 
    & =-t^2+x^2+(v x)^2\left(a^2+\frac{\frac{1}{1-v^2}-1}{v^2}\right) \\ 
    & =-t^2+x^2+(v x)^2\left(a^2+\frac{\frac{v^2}{1-v^2}}{v^2}\right) \\ 
    & =-t^2+x^2+(v x)^2\left(a^2-a^2\right) \\ 
    & = -t^2 + x^2
\end{aligned}
\end{equation*}

Damit haben wir gezeigt, dass der Ausdruck
\begin{equation*}
-\bar{t}^2 + \bar{x}^2 = -t^2 + x^2
\label{eq:invarianz}
\end{equation*}
in allen Inertialsystemen denselben Wert besitzt. Dies gilt unabhängig davon, ob es sich um Lichtsignale handelt oder nicht.

\medskip
Das bedeutet: Der 4-dimensionale Abstand $-t^2 + x^2$ ist eine vom Bezugssystem unabhängige Größe – eine \textit{Lorentz-Invariante}. Er ändert sich nicht durch Transformation in ein anderes Inertialsystem.

Diese Invarianz motiviert die Definition des \textbf{Raumzeitabstandes} $\Delta s^2$:
\begin{equation*}
\Delta s^2 := -c^2 \Delta t^2 + \Delta x^2 + \Delta y^2 + \Delta z^2
\label{eq:raumzeitabstand}
\end{equation*}

Mit der sogenannten \textit{Minkowski-Metrik} $\eta_{\mu\nu}$ kann dies kompakt geschreiben werden:
\begin{equation*}
\Delta s^2 = \eta_{\mu\nu}\, \Delta x^{\mu}\, \Delta x^{\nu},
\end{equation*}
wobei
\begin{equation*}
(\eta_{\mu\nu}) :=
\left(
\begin{array}{cccc}
-1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1
\end{array}
\right)
=\text{diag}(-1,1,1,1)
\qquad
\mu,\nu = 0,1,2,3
\end{equation*}

Sie hat die Signatur (-,+,+,+). Der Minkowski-Raum $\mathbb{M}$ ist ein Vektorraum, zusammen mit der Minkowski-Metrik $\eta_{\mu \nu}$.

Die Raumzeit-Koordinaten eines Ereignisses lassen sich also zu einem Vierervektor zusammenfassen, wobei $x^0=ct$ die zeitliche Komponente und $x^i$ die räumlichen Komponenten sind:
\begin{equation*}
v = 
\left(
\begin{array}{c}
ct \\
x^1 \\
x^2 \\
x^3 \\
\end{array}
\right)
\in \mathbb{M}
\end{equation*}

Dann ist das \textbf{Längenquadrat} des Vektors $v$:
\begin{equation}
v^2 = \eta_{\mu\nu} v^{\mu} v^{\nu} = -c^2t^2 + \vec x^2 = -c^2 t^2 + (x^1)^2 + (x^2)^2 + (x^3)^2
\label{eq:eta_form}
\end{equation}

wobei jeweils über $\mu\nu$ summiert wird und die Summe weggelassen wurde (Einsteinsche Summenkonvention).
\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{images/Abstandsrt.jpg}
\caption{$v_t=ct$ zeitliche Komponente des Vektors $v$; $v_x=(x^1, x^2, x^3)= \vec x$ räumliche Komponente vom Vektor $v$}
\end{figure}

Durch das negative Vorzeichen der Zeitkomponente kann der Raumzeitabstand eines Vektors im Minkowski-Raum positiv, negativ oder null werden:
\begin{itemize}
    \item $v^2 < 0$: \textbf{zeitartig} – Ereignisse liegen innerhalb des Lichtkegels und werden auch \textit{kausale} Vektoren genannt, da sie die mögliche Zukunft bedeuten.
    \item $v^2 = 0$: \textbf{lichtartig} – Ereignisse liegen auf dem Lichtkegel.
    \item $v^2 > 0$: \textbf{raumartig} – Ereignisse liegen außerhalb des Lichtkegels.
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{Abstandart.jpg}
\caption{$v$ ist zeitartig, $\tilde{v}$ ist lichtartig, $\tilde{\tilde{v}}$ ist raumartig}
\end{figure}

\subsection{Mathematische Struktur der Minkowski-Metrik (nicht Teil der VO)}
Im Folgenden möchte ich die soeben kennengelernte Minkowski-Metrik mathematisch einordnen. Falls folgende Begriffe nicht bekannt sind, diesen Teil der Mitschrift überspringen, sie zu Ende lesen und am Schluss wieder diesen Abschnitt durchlesen. Alle Begriffe werden im Laufe der Mitschrift eingeführt.

\underline{TLDR:} Die Minkowski-Metrik ist keine Metrik im Sinne der Theorie der \href{https://de.wikipedia.org/wiki/Metrischer_Raum#Formale_Definition}{metrischen Räume}, sondern eine symmetrische, nicht entartete Bilinearform, die nicht positiv definit ist. Auf dem Minkowski-Vektorraum $\mathbb{M}$ ist sie ein Pseudo-Skalarprodukt. Auf der flachen Raumzeit, aufgefasst als Mannigfaltigkeit, ist sie eine pseudo-riemannsche Metrik bzw. ein metrischer Tensor. Da sie negative Werte annehmen kann, induziert sie keine Norm, sondern ein Längen\textit{quadrat} $v^2$, welches ebenso negativ sein kann. 

\begin{definition}[reelles Skalarprodukt] \breakafterhead

{\footnotesize Quelle: \url{https://de.wikipedia.org/wiki/Skalarprodukt#Definition_(Axiomatik)}}\newline
Sei $V$ ein reeller Vektorraum. Eine Abbildung
\begin{equation*}
\langle \cdot , \cdot \rangle : V \times V \to \mathbb{R}
\end{equation*}
heißt \textbf{Skalarprodukt}, falls es eine positiv definite symmetrische Bilinearform ist, d.h. für alle $x,y,z \in V$ und $\lambda \in \mathbb{R}$
die folgenden Eigenschaften gelten:
\begin{enumerate}
\item \textbf{Bilinear:}
linear in beiden Argumenten
\begin{align*}
\langle x+y,z\rangle = \langle x,z\rangle + \langle y,z\rangle
\qquad
\langle \lambda x,y\rangle = \lambda \langle x,y\rangle \\
\langle x,y+z\rangle = \langle x,y\rangle + \langle x,z\rangle
\qquad
\langle x, \lambda y\rangle = \lambda \langle x,y\rangle
\end{align*}

\item \textbf{Symmetrisch:}
\begin{equation*}
\langle x,y\rangle = \langle y,x\rangle
\end{equation*}
\item \textbf{Positiv definit:}
\begin{equation*}
\langle x,x\rangle \ge 0
\quad \text{und} \quad
\langle x,x\rangle = 0 \;\Leftrightarrow\; x = 0
\end{equation*}
\end{enumerate}
\end{definition}

Ein Skalarprodukt liefert Winkelbegriffe/Orthogonalität und induziert durch Wurzelziehen eine Norm $\|x\| := \sqrt{\langle x,x\rangle}$. Die Norm induziert durch Differenzenbildung eine Metrik $d(x,y):= \|x-y\|$.

\begin{remark} \breakafterhead

{\footnotesize Quelle: \url{https://de.wikipedia.org/wiki/Bilinearform#Nicht_ausgeartete_Bilinearform}}\newline
Aus der positiven Definitheit folgt, dass jedes reelle Skalarprodukt \textbf{nicht entartet}/\textbf{nicht ausgeartet} ist, d.\,h.
\begin{align*}
\forall\, x \in V \setminus \{0\}\; \exists\, y \in V \;:\; \langle x,y\rangle \neq 0 \\
\forall\, y \in V \setminus \{0\}\; \exists\, x \in V \;:\; \langle x,y\rangle \neq 0
\end{align*}

Nicht entartet heißt: kein Vektor, außer der Nullvektor, ist zu allen anderen Vektoren orthogonal. Aus nicht entartet, folgt allerdings NICHT die positive Definitheit.
\end{remark}

\begin{definition} \breakafterhead

{\footnotesize Quelle: \url{https://matheplanet.com/default3.html?call=article.php?sid=1516}}
Sei $V$ ein reeller Vektorraum. Eine Abbildung
\begin{equation*}
(\cdot , \cdot ) : V \times V \to \mathbb{R}
\end{equation*}
heißt \textbf{Pseudo-Skalarprodukt}, falls es eine nicht entartete symmetrische Bilinearform ist.
\end{definition}

{\footnotesize Quelle: \url{https://www.thp.uni-koeln.de/~rk/tpI_18.html/vl22.pdf}} \newline
Bei einem Pseudo-Skalarprodukt ist also $(x,x) < 0$ zugelassen. Somit erhalten wir aus einem Pseudo-Skalarprodukt nur ein "Normquadrat" \ $\|x\|^2$ (genauer: \href{https://de.wikipedia.org/wiki/Quadratische_Form}{quadratische Form} $q(x)$) und keine Norm 
\begin{equation*}
q(x) := (x,x)
\end{equation*}
Wobei der Name irreführend ist, da die quadratische Form dennoch negativ sein kann. Entsprechend erhalten wir auch keine Metrik, sondern 
\begin{equation*}
d(x,y)^2 := q(x-y)^2
\end{equation*}

\begin{definition}  \breakafterhead

{\footnotesize Quellen: \url{https://de.wikipedia.org/wiki/Riemannsche_Mannigfaltigkeit#Definition}\newline 
\url{https://de.wikipedia.org/wiki/Pseudo-riemannsche_Mannigfaltigkeit}}\newline
Sei $M$ eine differenzierbare Mannigfaltigkeit. Ein \textbf{riemannsche Metrik} ist eine Abbildung
\begin{equation*}
g : M \to \mathfrak{Bil}(T_pM,\ \mathbb{R}): \ p \mapsto g_p
\end{equation*}
die jedem Punkt $p \in M$ eine positiv definite, symmetrische Bilinearform
\begin{equation*}
g_p : T_p M \times T_p M \to \mathbb{R}
\end{equation*}
zuordnet. Wobei $T_pM$ der Tangentialraum (Vektorraum) an einem Punkt $p \in M$ ist.
\end{definition}

Das heißt die riemannsche Metrik ist ein (0,2)-Tensorfeld $g \in \mathcal{T}_2^0(M)$.

{\footnotesize Quelle: \url{https://de.wikipedia.org/wiki/Riemannsche_Mannigfaltigkeit#Riemannsche_Mannigfaltigkeiten_als_metrische_R%C3%A4ume}}\newline
Die riemannsche Metrik ist keine Metrik im Sinne der Theorie der \href{https://de.wikipedia.org/wiki/Metrischer_Raum#Formale_Definition}{metrischen Räume}, sondern punktweise ein Skalarprodukt
auf den Tangentialräumen. Wie zuvor erwähnt, induziert es eine Metrik. Somit können riemannsche Mannigfaltigkeiten als metrische Räume verstanden werden. Die riemannsche Metrik legt Längen und Winkel fest.

\begin{definition} \breakafterhead

{\footnotesize Quelle: \url{https://de.wikipedia.org/wiki/Pseudo-riemannsche_Mannigfaltigkeit}}\newline
Ist die Bilinearform $g_p$ symmetrisch und nicht entartet, jedoch nicht positiv definit, so heißt $g$ ein \textbf{pseudo-riemannsche Metrik}.
\end{definition}

Die pseudo-riemannsche Metrik ist punktweise ein Pseudo-Skalarprodukt auf den Tangentialräumen.

In der Literatur ist der \textbf{metrische Tensor} nicht eindeutig definiert. Manchmal wird damit die riemannsche Metrik, manchmal die pseudo-riemannsche Metrik bezeichnet. 

\begin{definition} \breakafterhead

{\footnotesize Quellen: \url{http://www.vermessungsseiten.de/erdvermessung/node10.html}\newline
\url{https://matheplanet.com/default3.html?call=article.php?sid=1516}\newline
\url{https://www.thp.uni-koeln.de/~rk/tpI_18.html/vl22.pdf}}\newline
Der \textbf{Minkowski-Raum} $\mathbb{M}:=\{(ct, x^1, x^2, x^3) \in \mathbb{R}^4\} \cong \mathbb{R}^4$ ist ein reeller Vektorraum,
ausgestattet mit dem Pseudo-Skalarprodukt
\begin{equation*}
\eta: \mathbb{M} \times \mathbb{M} \to \mathbb{R}
\end{equation*}
mit
\begin{equation*}
\eta(x,y) := \underbrace{-c^2t^2}_{=-x^0y^0} + x^1y^1 + x^2y^2 + x^3y^3 := \eta_{\mu\nu} x^\mu y^\nu
\end{equation*}
wobei
\begin{equation*}
\eta_{\mu\nu} = \mathrm{diag}(-1,1,1,1)
\end{equation*}
\end{definition}

{\footnotesize Quelle: \url{https://de.wikipedia.org/wiki/Raumzeit#Minkowski-Raum,_Vierervektoren}}\newline
Die Matrix $\eta_{\mu\nu}$ ist also auf dem Minkowski-Vektorraum $\mathbb{M}$ ein \textit{Pseudo-Skalarprodukt}. Auf der flachen Raumzeit, aufgefasst als Mannigfaltigkeit (\href{https://de.wikipedia.org/wiki/Lorentzsche_Mannigfaltigkeit}{Lorentzsche Mannigfaltigkeit}), ist sie eine \textit{pseudo-riemannsche Metrik} bzw. ein \textit{metrischer Tensor}. Somit ist sie keine Metrik im Sinne der Theorie der metrischen Räume. Da sie nicht positiv definit ist (indefinit) ist ebenso zugelassen
\begin{equation*}
\eta(x,x) < 0
\end{equation*}
Somit erhalten ein "Normquadrat" \ $\|x\|^2$ (genauer: \href{https://de.wikipedia.org/wiki/Quadratische_Form}{quadratische Form} $q(x)$) und keine Norm 
\begin{equation*}
q(x) := \eta(x,x) = \eta_{\mu\nu} x^\mu x^\nu
\end{equation*}

Das Normquadrat kann somit negativ sein, womit der Name irreführend ist. Entsprechend erhalten wir auch \textit{keine Metrik}, sondern ein \textit{Längenquadrat/Abstandsquadrat} für $s_1 = (ct, x^1, x^2, x^3),\ s_2=(ct, y^1, y^2, y^3) \in \mathbb{M}$
\begin{equation*}
ds^2:=\Delta s^2 := (s_1-s_2)^2 =q(x-y)
\end{equation*}

Das Längenquadrat kann somit negativ sein. Wir erhalten also negative Längen. Dennoch wird $\eta$ bzw. $\eta_{\mu\nu}$ als \textbf{Minkowski-Metrik} bezeichnet. Sie stellt die Beziehung von Teilchen/Ereignissen in der flachen Raumzeit dar.

Für das Längenquadrat eines Vektors $v \in \mathbb{M}$ schreiben wir
\begin{equation*}
v^2= \eta_{\mu\nu}v^\mu v^\nu
\end{equation*}

Das Längenquadrat $\Delta s^2 := \eta_{\mu\nu}\,\Delta x^\mu \Delta x^\nu$ wird \textbf{Raumzeitabstand} genannt.
\vspace{1em}

{\footnotesize Quellen: \url{https://matheplanet.com/default3.html?call=article.php?sid=1516}\newline
\url{https://en.wikipedia.org/wiki/Orthonormal_basis}\newline
\url{https://en.wikipedia.org/wiki/Orthonormal_basis#Examples}}\newline
Mit einem Pseudo-Skalarprodukt kann eine Pseudo-Orthonormalbasis definiert werden.

\begin{definition}[Pseudo-Orthonormalbasis]
Eine Basis $E^\mu$ von $V$ heißt \textbf{pseudo-orthonormal} bezüglich eines Pseudo-Skalarprodukts $(\cdot,\cdot)$, falls
\begin{equation*}
(E^\mu,E^\nu) = g_{\mu\nu}
\end{equation*}
wobei $g_{\mu\nu}$ eine diagonale Matrix mit Einträgen $\pm 1$ entsprechend der Signatur des Pseudo-Skalarprodukts ist.\newline
Siehe später in \cref{ex:pseudoONB}.
\end{definition}



\chapter{Multilineare Algebra}
Um die allgemeine Relativitätstheorie mathematisch präzise formulieren zu können, benötigen wir Werkzeuge aus der sogenannten multilinearen Algebra. Dieses Kapitel führt die wichtigsten Begriffe ein, auf denen später die Tensorrechnung aufbaut.

\section{Vektorräume}

\begin{definition}
Ein \textbf{Körper} ist ein Tupel $(\Gamma, +, \cdot)$, bestehend aus einer Menge $\Gamma$ und zwei inneren zweistelligen Verknüpfungen $+$ und $\cdot$ (Addition und Multiplikation):

\begin{equation*}
\begin{aligned}
+ &:\Gamma\times\Gamma\to\Gamma,\quad (a,b)\mapsto a+b \\
\cdot &:\Gamma\times\Gamma\to\Gamma,\quad (a,b)\mapsto a\cdot b 
\end{aligned}
\end{equation*}

Für die Verknüpfungen gleiten die folgenden Axiome:

\textbf{1. Additive Eigenschaften}
\vspace{-1em}

\begin{enumerate}
    \item[(1.1)] $\forall a,b,c \in \Gamma: \; a + (b + c) = (a + b) + c$ \hfill (Assoziativgesetz)
    \item[(1.2)] $\forall a,b \in \Gamma: \; a + b = b + a$ \hfill (Kommutativgesetz)
    \item[(1.3)] $\exists 0 \in \Gamma\; \forall a \in \Gamma: \; 0 + a = a$ \hfill (neutrales Element bzgl. +)
    \item[(1.4)] $\forall a \in \Gamma\; \exists -a \in \Gamma: \; (-a) + a = 0$ \hfill (additives Inverses)
\end{enumerate}

Damit ist $(\Gamma, +)$ eine \textit{kommutative/abelsche Gruppe}.
\vspace{1em}

\textbf{2. Multiplikative Eigenschaften}
\vspace{-1em}

\begin{enumerate}
    \item[(2.1)] $\forall a,b,c \in \Gamma: \; a \cdot (b \cdot c) = (a \cdot b) \cdot c$ \hfill (Assoziativgesetz)
    \item[(2.2)] $\forall a,b \in \Gamma: \; a \cdot b = b \cdot a$ \hfill (Kommutativgesetz)
    \item[(2.3)] $\exists 1 \in \Gamma\; \forall a \in \Gamma: \; 1 \cdot a = a$ \hfill (neutrales Element bzgl. $\cdot$)
    \item[(2.4)] $\forall a \in \Gamma\setminus\{0\}\; \exists a^{-1} \in \Gamma: \; a^{-1}\cdot a = 1$ \hfill (multiplikatives Inverses)
\end{enumerate}

Damit ist $(\Gamma\setminus\{0\}, \cdot)$ ebenfalls eine kommutative/abelsche Gruppe.
\vspace{1em}

\textbf{3. Zusammenspiel von Addition und Multiplikation}
\vspace{-1em}

\begin{enumerate}
    \item[(3.1)] $\forall a,b,c \in \Gamma: \; a\cdot(b + c) = a\cdot b + a\cdot c$ \hfill (Linksdistributivgesetz)
    \item[(3.2)] $\forall a,b,c \in \Gamma: \; (b + c)\cdot a = b\cdot a + c\cdot a$ \hfill (Rechtsdistributivgesetz)
\end{enumerate}

Beispiele: $(\mathbb{R}, +, \cdot), \ (\mathbb{C}, +, \cdot)$

\end{definition}

\begin{definition}\label{def:Vektorraum}
Ein \textbf{Vektorraum} über einem Körper $(\Gamma, +, \cdot)$, kurz $\Gamma$, ist ein Tupel $(V, \oplus, \odot)$, bestehend aus einer Menge $V$, einer inneren zweistelligen Verknüpfung $\oplus$ (Vektoraddition) und einer äußeren zweistelligen Verknüpfung $\odot$ (Multiplikation mit Skalaren).

\begin{equation*}
\begin{aligned}
\oplus &: V\times V \to V,\quad (u,w)\mapsto u\oplus w \\
\odot &: \Gamma\times V \to V,\quad (\lambda,u)\mapsto \lambda\odot u
\end{aligned}
\end{equation*}

Für die Verknüpfungen gleiten die folgenden Axiome:

\textbf{1. Vektoraddition}
\vspace{-1em}

\begin{enumerate}
    \item[(1.1)] $\forall u,v,w\in V:\; u\oplus(v\oplus w)=(u\oplus v)\oplus w$ \hfill (Assoziativgesetz)
    \item[(1.2)] $\forall v,w\in V:\; v\oplus w = w\oplus v$ \hfill (Kommutativgesetz)
    \item[(1.3)] $\exists 0\in V\;\forall v\in V:\; 0\oplus v=v$ \hfill (neutrales Element bzgl. + (Nullvektor))
    \item[(1.4)] $\forall v\in V\;\exists (-v)\in V:\; (-v)\oplus v = 0$ \hfill (additives Inverses)
\end{enumerate}
Damit bildet $(V,\oplus)$ eine kommutative/abelsche Gruppe.
\vspace{1em}

\textbf{2. Multiplikation mit Skalaren}
\vspace{-1em}

\begin{enumerate}
    \item[(2.1)] $\forall v\in V,\;\lambda,\mu\in\Gamma:\; (\lambda\mu)\odot v = \lambda\odot(\mu\odot v)$ \hfill (Assoziativgesetz)
    \item[(2.2)] $\exists 1\in\Gamma\;\forall v\in V:\; 1\odot v = v$ \hfill (neutrales Element bzgl. $\cdot$ (Skalar))
    \item[(2.3)] $\forall u,v\in V,\;\lambda\in\Gamma:\; \lambda\odot(u\oplus v)=(\lambda\odot u)\oplus(\lambda\odot v)$ \hfill (Linksdistributivgesetz)
    \item[(2.4)] $\forall v\in V,\;\lambda,\mu\in\Gamma:\; (\lambda+\mu)\odot v = (\lambda\odot v)\oplus(\mu\odot v)$ \hfill (Rechtsdistributivgesetz)
\end{enumerate}

Beispiele: $(\mathbb{R}^n, +, \cdot)\ \text{über dem Körper } \mathbb{R}, \ (\mathbb{C}^n, +, \cdot)\ \text{über dem Körper } \mathbb{C}, \ n\in\mathbb{N}$

\end{definition}


\begin{remark}
    Wir verwenden ab jetzt für Körperaddition \& Vektoraddition $+$ und Körpermultiplikation \& Skalarmultiplikation $\cdot$, um die Notation zu vereinfachen. 
\end{remark}

Es folgt bereits und muss somit nicht extra gefordert werden:
\vspace{-1em}


\begin{corollary}\label{cor:zerovec}
    \begin{equation*}
        \exists \ 0 \in \Gamma, \ \forall v\in V: \ 0\cdot v = 0 \in V 
    \end{equation*}
\end{corollary}

\begin{proof}
\begin{equation*}
\begin{aligned}
    & 0 \cdot v 
    \overset{\substack{\text{neutr. El.} \\ \text{bzgl. }+}}{=} (0+0)\cdot v 
    \overset{\substack{\text{Rechts-} \\ \text{distr.ges.}}}{=} 0 \cdot v + 0 \cdot v \qquad | -0 \cdot v \text{ (additives Inverses)}\\
    & \Rightarrow 0 = 0 \cdot v
\end{aligned}
\end{equation*}
\end{proof}

\subsection{Linearkombination und lineare Unabhängigkeit}
\begin{definition}
Sei $(V,\Gamma)$ ein Vektorraum und $(v_\alpha)_{\alpha\in I}\subseteq V$ ein Teilsystem von Vektoren (auch Familie oder Teilmenge von Vektoren genannt). Die Indexmenge ist beliebig und kann sogar überabzählbar unendlich sein. Ein Vektor $v\in V$ heißt \textbf{Linearkombination} dieses Teilsystems, wenn es Skalare $\lambda^\alpha\in\Gamma$ gibt mit

\begin{equation*}
v = \sum_{\alpha\in I}\lambda^\alpha v_\alpha
\qquad
\lambda^\alpha\ne 0 \text{ für endlich viele } \alpha
\end{equation*}

Auch hier verwendet der Physiker gerne die Einsteinsche Summenkonvention: $\sum_{\alpha\in I}\lambda^\alpha v_\alpha \equiv \lambda^\alpha v_\alpha$

\begin{remark}
    Bei der Forderung, dass nur endlich viele $\lambda^\alpha \ne 0$ sein dürfen, geht \cref{cor:zerovec} ein und sie ist notwendig, da rein algebraisch definierte Vektorräume (\cref{def:Vektorraum}) keinen Abstandsbegriff in Form einer Topologie oder Metrik besitzen. Somit existiert kein Konvergenzbegriff für unendliche Summen und Linearkombinationen müssen endlich sein. Man beachte, dass es für jeden Vektorraum mind. eine Topologie gibt, mit diesem er ein topologischer Vektorraum wird, wir hier jedoch ausschließlich die algebraische Definition betrachten. Siehe \hl{später}
\end{remark}

\end{definition}

\begin{definition}
Sei $(V,\Gamma)$ ein Vektorraum und $(v_\alpha)_{\alpha\in I}\subseteq V$ ein Teilsystem von Vektoren, wobei $I$ beliebige Indexmenge. Seien $\lambda^\alpha \in \Gamma$.
\begin{itemize}
    \item $(v_\alpha)_{\alpha\in I}$ heißt \textbf{linear unabhängig} (kurz l.u.) genau dann wenn
    \begin{equation}\label{eq:lu}
        \lambda^\alpha v_\alpha = 0 \quad \Leftrightarrow \quad \forall\alpha\in I: \ \lambda^\alpha = 0
    \end{equation}

    Das heißt: der Nullvektor lässt sich nur mit einer Linearkombination von Vektoren aus $(v_\alpha)_{\alpha\in I}$ darstellen, in der \textit{alle} Koeffizienten $\lambda^\alpha = 0$. Da alle Vektoren unterschiedlich sind, können sie nicht voneinander abgezogen werden sodass $0$ rauskommt. Die Vektoren zeigen in verschiedene Richtungen.
    
    Bemerkung: die Rückrichtung von \eqref{eq:lu} gilt klarerweise immer.
\end{itemize}

Sind diese Bedingungen nicht erfüllt, heißt das Teilsystem linear abhängig: 

\begin{itemize}
    \item $(v_\alpha)_{\alpha\in I}$ heißt \textbf{linear abhängig} (kurz l.a.) genau dann wenn
    \begin{equation*}
        \exists \ \text{endlich viele } \alpha \in I \text{ mit } \lambda^\alpha \ne 0: \ \lambda^\alpha v_\alpha = 0
    \end{equation*}

    Das heißt: der Nullvektor lässt sich auch mit einer Linearkombination von Vektoren aus $(v_\alpha)_{\alpha\in I}$ darstellen, in der \textit{nicht alle} Koeffizienten $\lambda^\alpha = 0$. 
\end{itemize}


\begin{itemize}
    \item Eine äquivalente Definition für linear abhängig ist:
    \begin{equation*}
        \exists \beta \in I: v_\beta = \sum_{\alpha \in I \backslash \{\beta\}} \lambda^\alpha v_\alpha
    \end{equation*}

Das heißt: mind. ein Vektor des Teilsystems kann durch eine Linearkombination der anderen Vektoren des Teilsystems dargestellt werden. Die Vektoren zeigen in dieselbe oder in die entgegengesetzte Richtung.

\end{itemize}
\end{definition}

\begin{remark}
    Vektorräume $(v_\alpha)_{\alpha\in I}\subseteq V$ sind per Definition linear abhängig: zu jedem Vektor $v_\beta \in V$ ist das additive Inverse $-v_\beta$ ebenso enthalten. Dann kann beispielsweise die Linearkombination gewählt werden $1 \cdot v_\beta + 1 \cdot (-v_\beta ) + \sum_{\alpha \in I \backslash \{\beta\}} 0\cdot v_\alpha =0$.
\end{remark}

\begin{corollary}\label{cor:eindeutig}
Sei $(v_\alpha)_{\alpha\in I}\subseteq V$ linear unabhängig. Wenn ein Vektor $v \in V$ durch diese mit einer Linearkombination ($v= \lambda^\alpha v_\alpha$), dann ist die Darstellung eindeutig (d.h. $(\lambda^\alpha)_{\alpha\in I}$ eindeutig).
\end{corollary}

\begin{proof}
Annahme: es gibt zwei verschiedene Mengen von Koeffizienten $(\lambda^\alpha)_{\alpha\in I}$ und $(\mu^\alpha)_{\alpha\in I}$ durch die $v \in V$ ebenso darstellbar ist: $v= \lambda^\alpha v_\alpha = \mu^\alpha v_\alpha$.

Dann ist
\begin{equation*}
\begin{aligned}
0 & = v - v = \lambda^\alpha v_\alpha - \mu^\alpha v_\alpha \overset{\text{Distr.ges.}}{=} (\lambda^\alpha-\mu^\alpha)v_\alpha \\
& \overset{v_\alpha \text{ l.u.}}{\Rightarrow} \lambda ^\alpha - \mu ^\alpha = 0 \ \Rightarrow \ \lambda^\alpha = \mu^\alpha, \ \forall \alpha \in I
\end{aligned}
\end{equation*}
\end{proof}

\subsection{Basis eines Vektorraums}

Wir können Vektoren also eindeutig durch eine Linearkombination aus linear unabhängigen Vektoren darstellen. Beispielsweise können mit zwei linear unabhängigen Vektoren alle anderen, die in derselben Ebene liegen, aufgespannt werden, aber keine, die aus der Ebene herauszeigen. Also stellt sich die Frage: Welche kleinste Menge von Vektoren reicht aus, um alle anderen im Raum zu erzeugen? Dies führt uns zu dem Begriff der Basis.
\vspace{1em}

\begin{tcolorbox}[noVO]
\begin{definition}[nicht Teil der VO]
Sei $(V,\Gamma)$ ein Vektorraum und $U := (v_\alpha)_{\alpha\in I}\subseteq V$ ein Teilsystem von Vektoren, wobei $I$ beliebige Indexmenge. Seien $\lambda^\alpha \in \Gamma$. \newline

Dann heißt
\begin{equation*}
\mathrm{span}(U) := 
\left\{\lambda^\alpha v_\alpha \ \middle| \  \lambda^\alpha \in \Gamma, \; v_\alpha \in U \right\}
\end{equation*}
die \textbf{lineare Hülle} von $U$. Andere Schreibweisen sind: $[U], \ \langle U \rangle$


$\mathrm{span}(U)$ ist die Menge aller Linearkombinationen der Vektoren $v_\alpha$. Sie ist der kleinste lineare Teilraum, der U enthält (also $U \subseteq \mathrm{span}(U)$). Wenn U ein linearer Teilraum ist, dann gilt: $U = \mathrm{span}(U)$.

$U$ heißt \textbf{Erzeugendensystem} von $V$, wenn gilt:
\begin{equation*}
\mathrm{span}(U) = V
\end{equation*}

Das bedeutet: $U$ \textit{spannt den gesamten Raum $V$ auf}. Jeder Vektor $v \in V$ lässt sich als Linearkombination von Vektoren aus $U$ darstellen.

\end{definition}

\begin{definition}[nicht Teil der VO]
Sei $(V,\Gamma)$ ein Vektorraum und $B \subseteq V$ ein Teilsystem von Vektoren.

$\mathcal{B}$ heißt \textbf{Basis} des Vektorraums $V$ wenn $\mathcal{B}$ ein linear unabhängiges Erzeugendensystem von $V$ ist. \newline
Das bedeutet: Jeder Vektor in $V$ lässt sich als Linearkombination von Vektoren aus $B$ darstellen und die Vektoren von $B$ sind linear unabhängig.
\end{definition}

\begin{remark}
Basen eines Vektorraums sind nicht eindeutig.
\end{remark}

\begin{example}[nicht Teil der VO] \label{ex:basis} \breakafterhead
\begin{itemize}
    \item Im Vektorraum $\mathbb{R}^2$ ist
    \begin{equation*}
    \mathcal{B} = \{ (1,0), (0,1) \}
    \end{equation*}
    eine Basis (es gibt auch andere).  
    Diese Basis wird \textit{Standardbasis} oder \textit{kanonische Basis} genannt.

    \item $\mathbb{C}$ als Vektorraum über $\mathbb{R}$:  
    \begin{equation*}
    \mathcal{B} = \{ 1, i \}
    \end{equation*}

    \item $\mathbb{C}$ als Vektorraum über $\mathbb{C}$:  
    \begin{equation*}
    \mathcal{B} = \{ 1 \} \quad \text{oder auch} \quad \mathcal{B} = \{ i \}
    \end{equation*}

    \item $\mathbb{C}^2$ als Vektorraum über $\mathbb{C}$:
    \begin{equation*}
    \mathcal{B} = \{ (1,0), (0,i) \}
    \quad \text{oder auch} \quad
    \mathcal{B} = \{ (i,0), (0,-i) \}
    \end{equation*}
\end{itemize}
\end{example}
\end{tcolorbox}
\vspace{1em}

Nun stellt sich die Frage: Hat jeder Vektorraum eine Basis? Um das zu beantworten, benötigen wir erst mal weitere Definitionen.

\begin{definition}
Eine Relation $\leq \subseteq M^2$ auf einer Menge $M$ heißt \textbf{Totale Ordnung} oder \textbf{lineare Ordnung} $(M, \leq)$, wenn für alle $x,y,z \in M$ gilt:
\begin{enumerate}
    \item $x \leq x$ \hfill (Reflexivität)
    \item $x \leq y \;\wedge\; y \leq x \;\Rightarrow\; x = y$ \hfill (Antisymmetrie)
    \item $x \leq y \;\wedge\; y \leq z \;\Rightarrow\; x \leq z$ \hfill (Transitivität)
    \item $x \leq y \;\vee\; y \leq x$ \hfill (Totalität / Linearität)
\end{enumerate}
Die Bedingung 4. bedeutet, dass alle Elemente der Menge $M$ paarweise vergleichbar sind.
\end{definition}

\begin{definition}
Eine Relation $\leq \subseteq M^2$ auf einer Menge $M$ heißt \textbf{Halbordnung} oder \textbf{partielle Ordnung} $(M, \leq)$, wenn für alle $x,y,z \in M$ obige Bedingungen 1., 2. und 3. erfüllt sind. Es fehlt also die Vergleichbarkeit aller Elemente aus $M$ bzgl. $\leq$.
\end{definition}

\begin{tcolorbox}[noVO]
\begin{example}[Totalordnung - nicht Teil der VO] \breakafterhead
\begin{itemize} 
    \item Die Mengen $(\mathbb{N}, \leq)$, $(\mathbb{Z}, \leq)$, $(\mathbb{Q}, \leq)$ und $(\mathbb{R}, \leq)$ sind totalgeordnet.  
    Auch die leere Menge $(\varnothing, \prec)$ ist mit der leeren Ordnung totalgeordnet (trivialerweise, da es keine Elemente gibt, die nicht vergleichbar wären).
    \item Ein weiteres Beispiel ist $(\mathbb{C}, \leq_{\mathrm{lex}})$, wenn man $\mathbb{C}$ als $\mathbb{R} \times \mathbb{R}$ auffasst und die \textit{lexikographische Ordnung} verwendet:
    \begin{equation*}
    (a,b) \leq_{\mathrm{lex}} (c,d) \quad :\Leftrightarrow \quad (a < c) \;\text{ oder }\; (a = c \wedge b \leq d).
    \end{equation*}
\end{itemize}
\end{example}
\end{tcolorbox}

\begin{example}[Halbordnung, aber keine Totalordnung] \breakafterhead
\begin{itemize}
    \item Betrachte die Potenzmenge
    \begin{equation*}
    \mathcal{P}(\{1,2,3\}) = \{ \varnothing, \{1\}, \{2\}, \{3\}, \{1,2\}, \{1,3\}, \{2,3\}, \{1,2,3\} \}
    \end{equation*}
    mit der Inklusionsrelation $\subseteq$.  
    Dann ist $(\mathcal{P}(\{1,2,3\}), \subseteq)$ eine Halbordnung, aber keine Totalordnung, da z.\,B.
    \begin{equation*}
    \{1\} \not\subseteq \{2\} \quad \text{und} \quad \{2\} \not\subseteq \{1\}.
    \end{equation*}
    Diese beiden Mengen sind also \textit{nicht vergleichbar}.
    
    \item (nicht Teil der VO) Betrachte $(\mathbb{C}, \leq)$, wobei $\mathbb{C}$ als Menge aufgefasst wird.  
    Dann gilt:
    \begin{equation*}
    1 + 0 \cdot i = 1 \leq 2 = 2 + i \cdot 0, \qquad 0 + 1 \cdot i = i \leq 2 \cdot i = 0 + 2 \cdot i 
    \end{equation*}
    aber
    \begin{equation*}
    1 + 0 \cdot i = 1 \not\leq i = 0 + 1 \cdot i
    \end{equation*}
    Daher sind die komplexen Zahlen im Allgemeinen \textit{nicht vergleichbar} und bilden somit keine Totalordnung.
\end{itemize}        
\end{example}


\begin{definition}
Sei $(M, \leq)$ eine halbgeordnete Menge und $T \subseteq M$ eine Teilmenge.  
Ein Element $s \in M$ heißt \textbf{obere Schranke} von $T$, wenn gilt:
\begin{equation*}
\forall t \in T: \; t \leq s
\end{equation*}
Das bedeutet: $s$ ist größer oder gleich jedem Element aus $T$ (bezüglich der Ordnung $\leq$).
\end{definition}

\begin{definition}
Ein Element $m \in M$ einer halbgeordneten Menge $(M, \leq)$ heißt \textbf{maximal in M}, wenn gilt:
\begin{equation*}
\forall x \in M: \; m \leq x \;\Rightarrow\; x = m
\end{equation*}
Das heißt: es gibt kein Element $x \in M$, das echt größer als $m$ ist. Es kann mehrere Maximale Elemente in M geben. Diese sind nicht vergleichbar, da sonst eines größer wäre als das andere.
\end{definition}


\begin{definition}
Sei $(M, \leq)$ eine halbgeordnete Menge.  
Eine Teilmenge $T \subseteq M$ heißt \textbf{Kette}, wenn sie totalgeordnet ist. \newline
Eine Kette ist also eine totalgeordnete Teilmenge einer halbgeordneten Menge. Die Bezeichnung Kette rührt daher, dass die Elemente davon nacheinander aufgereiht werden können: $t_1 \leq t_2 \leq \dots \leq t_n$ - also eine \textit{Kette} bilden.
\end{definition}

Das Lemma von Zorn ist ein fundamentales Hilfsmittel in der linearen Algebra und der Mengenlehre. Es sichert die Existenz maximaler Elemente in bestimmten halbgeordneten Mengen. 

\begin{theorem}[Lemma von Zorn] \bigbreakafterhead

\underline{Voraussetzungen:} 
\begin{itemize}
    \item $(A, \leq)$ halbgeordnete Menge 
    \item Jede Kette von $A$ besitzt eine obere Schranke, die in $A$ liegt (muss nicht in der Kette liegen).
\end{itemize}

\underline{Behauptung:}
Dann hat $A$ maximale Elemente bzgl. "$\leq$".
\end{theorem}

Wir verwenden nun das Lemma von Zorn zu zeigen, dass jeder Vektorraum maximal linear unabhängige Teilsysteme besitzt und zeigen anschließend mit \cref{thm:erzeugen}, dass ein maximal linear unabhängiges Teilsystem eine Basis ist. Insgesamt also, dass jeder Vektorraum eine Basis besitzt.

\begin{theorem}\label{thm:maximal}
    Jeder Vektorraum besitzt maximal linear unabhängige Teilsysteme.
\end{theorem}

\begin{proof}
Sei $(V,\Gamma)$ ein Vektorraum. Wir betrachten die Menge $\mathcal{L}$ aller Teilmengen von $V$, die linear unabhängig sind:
\begin{equation*}
\mathcal{L} = \{ L \subseteq V \mid L \text{ ist linear unabhängig} \}.
\end{equation*}

Auf $\mathcal{L}$ definieren wir eine Ordnungsrelation durch Inklusion:
\begin{equation*}
\forall L_1,L_2 \in \mathcal{L}: \ L_1 \leq L_2 \;:\Leftrightarrow\; L_1 \subseteq L_2
\end{equation*}

$(\mathcal{L}, \leq)$ ist eine halbgeordnete Menge, da die Inklusion reflexiv, antisymmetrisch und transitiv ist. \newline 

\begin{minipage}[t]{0.72\textwidth}
Um das Lemma von Zorn anwenden zu können, muss also noch gezeigt werden, dass jede Kette von $\mathcal{L}$ eine obere Schranke besitzt, die in $\mathcal{L}$ liegt.

Sei $\mathcal{K} = (L_\alpha)_{\alpha \in I} \subseteq \mathcal{L}$ eine Kette. (Also $L_1 \subseteq L_2 \subseteq L_3 \subseteq \dots$). $\mathcal{K}$ ist eine Menge von Mengen. Dessen obere Schranke ist die Vereinigung:
\begin{equation*}
L^* = \bigcup_{\alpha \in I} L_\alpha
\end{equation*}

\end{minipage}\hfill
\begin{minipage}[t]{0.25\textwidth}
\centering
\vspace{-3em}
\includegraphics[width=\textwidth]{Kette.jpg}\\[0.3em]
\small Die Kette $\mathcal{K}$
\end{minipage}

Also muss noch gezeigt werden, dass $L^* \in \mathcal{L}$. Das heißt, es muss gezeigt werden, dass $L^*$ linear unabhängig ist.

Sei $L^* = (v_\beta)_{\beta \in J }$. Für lineare Unabhängigkeit muss gezeigt werden: \newline
$\lambda^\beta v_\beta = 0 \ \Rightarrow \ \forall\beta\in J: \ \lambda^\beta = 0 $

Sei also $\lambda^\beta v_\beta = 0$, wobei nach der Definition der Linearkombination nur endlich viele $\lambda^\beta \neq 0$ sein können. Seien diese die $\lambda^{\beta_1}, \ \lambda^{\beta_2}, \dots, \ \lambda^{\beta_n}$. Betrachte die zugehörigen $v_{\beta_i}$. 

\textbf{Fall 1:} Alle $v_{\beta_i}, \ i= 1, ..., n$ liegen in einem $L_{\alpha}$. Da $L_\alpha \in \mathcal{L}$ und $\mathcal{L}$ aus linear unabhängigen Mengen besteht, ist also $v_{\beta_i}, \ i= 1, ..., n$ linear unabhängig und somit $\lambda^{\beta_i} = 0 \ \text{für } i=1, ..., n$. Also $L^*=(v_\beta)_{\beta \in J }$ linear unabhängig.

\textbf{Fall 2:} Alle $v_{\beta_i}, \ i= 1, ..., n$ liegen in unterschiedlichen $L_{\alpha_i}$. \newline
Da $(L_\alpha)_{\alpha\in I}$ eine Kette ist, sind die $L_{\alpha_i}$ paarweise durch Inklusion vergleichbar. Für $v_{\beta_1} \in L_{\alpha_1}$ und $v_{\beta_2} \in L_{\alpha_2}$ gilt also: \newline
entweder $L_{\alpha_1} \subseteq L_{\alpha_2}$ und somit $v_{\beta_1}, v_{\beta_2} \in L_{\alpha_2}$ \newline
oder $L_{\alpha_2} \subseteq L_{\alpha_1}$ und somit $v_{\beta_1}, v_{\beta_2} \in L_{\alpha_1}$.

Also existiert ein Index, ab dem alle drin liegen:
\begin{equation*}
\exists \ \alpha_N\in I \ \text{mit} \ v_{\beta_1},\dots,v_{\beta_n}\in L_{\alpha_N}
\end{equation*}

Wie in Fall 1 folgt nun, dass $L^*=(v_\beta)_{\beta \in J }$ linear unabhängig ist.

Nun kann das Lemma von Zorn angewandt werden: $\mathcal{L}$ hat maximale Elemente bzgl. "$\leq$". 

Da $\mathcal{L}$ aus linear unabhängigen Teilsystemen besteht, gibt es also maximal linear unabhängige Teilsysteme $M\subseteq V$.
\end{proof}

\begin{remark}
Neben der bereits genannten Definition für \textit{Maximalität}, ist die folgende etwas intuitiver: eine l.u. Menge von Vektoren heißt \textit{maximal}, wenn durch Hinzufügen eines weiteren Vektors die Menge linear abhängig wird.
\end{remark}

\begin{theorem}\label{thm:erzeugen}
Sei $M=(v_\alpha)_{\alpha \in I}$ eine maximale linear unabhängige Teilmenge des Vektorraums $V$.  
Dann lässt sich jeder Vektor $v \in V$ eindeutig als Linearkombination der Basisvektoren darstellen:
\begin{equation*}
v = \lambda^\alpha v_\alpha
\end{equation*}

Das heißt: $M$ ist ein linear unabhängiges Erzeugendensystem von $V$ und somit eine Basis.
\end{theorem}

\begin{proof}
Dazu unterscheiden wir zwei Fälle.

\textbf{Fall 1:} $v \in M$.  
Da $M$ per Definition linear unabhängig ist, ist $v$ nur über nur einen einzigen Vektor aus dem System $(v_\alpha)_{\alpha \in I}$ darstellbar: sich selbst.  
\begin{equation*}
v = v_{\alpha_0} = \lambda^{\alpha_0} v_{\alpha_0} + \lambda^{\alpha_i} v_{\alpha_i},
\end{equation*}
wobei $\lambda^{\alpha_0} = 1$ und alle übrigen $\lambda^{\alpha_i} = 0$ sind.  
Damit ist $v$ also eindeutig als Linearkombination der Basisvektoren dargestellt.

\textbf{Fall 2:} $v \notin M$.  
Dann ist die Menge $\{v\} \cup M$ größer als $M$.  
Da sich $M$ aber ein maximal linear unabhängiges Teilsystem von $V$ ist, ist jede Vereinigung von $M$ mit weiteren Elementen aus $V$ nicht mehr linear unabhängig.  
Das bedeutet, dass eine nichttriviale Darstellung der Null existiert, die sowohl $v$ als auch Elemente aus $v_\alpha \in M$ enthält.  
Also gilt:
\begin{equation}\label{eq:linabh}
\exists \ (\lambda^{\alpha})_{\alpha \in I} \neq (0)_{\alpha \in I} : \ \lambda v + \lambda^{\alpha} v_\alpha = 0
\end{equation}

Nun unterscheiden wir wieder zwei Möglichkeiten:

\begin{itemize}
    \item Wenn $\lambda = 0$, folgt mit \eqref{eq:linabh}, dass $\lambda^{\alpha} v_\alpha = 0$. Da $(v_\alpha)_{\alpha \in I}$ l.u. muss also $\lambda^{\alpha} = 0$ für alle $\alpha \in I$. Das steht jedoch im Widerspruch zur Annahme, dass $(\lambda^{\alpha})_{\alpha \in I} \neq (0)_{\alpha \in I}$. Dieser Fall kann nicht eintreten.
    \item Wenn $\lambda \neq 0$, dann gilt:
    \begin{equation*}
    \lambda v = - \lambda^{\alpha} v_\alpha
    \quad \Rightarrow \quad
    v = -\frac{\lambda^{\alpha}}{\lambda} v_\alpha =: \mu^{\alpha} v_\alpha
    \end{equation*}
    Damit ist $v$ als Linearkombination der Vektoren $v_\alpha \in M$ darstellbar.  
\end{itemize}

Mit \cref{cor:eindeutig} folgt die Eindeutigkeit.
\end{proof}

\begin{remark}
Häufig wird die Basis mit $(E_\alpha)_{\alpha \in I}$ bezeichnet, und die Koeffizienten des Vektors $v$ als dessen Komponenten:
\begin{equation*}
v = v^{\alpha} E_{\alpha}
\end{equation*}

Diese Koeffizienten bezeichnet man als \textit{Koordinaten} des Vektors bezüglich der Basis $(E_\alpha)$.
\end{remark}

\begin{tcolorbox}[noVO]
\begin{definition}[nicht Teil der VO]
Die Mengen $M$ und $N$ heißen \textbf{gleichmächtig}, falls es eine Bijektion
\begin{equation*}
f : M \to N
\end{equation*}
gibt.  

Die \textbf{Mächtigkeit} oder \textbf{Kardinalität} einer Menge $M$ ist die Äquivalenzklasse aller Mengen, die zu $M$ gleichmächtig sind.  \newline
Wenn $M$ \textit{endlich} ist, dann ist existiert eine Bijektion zur Menge $\{1,2,\dots,n\}$, wobei $n$ die Anzahl der Elemente von $M$ ist, und es ist $|M| = n$. Die Mächtigkeit entspricht also der Anzahl der Elemente der Menge.

Ist $M$ \textit{nicht endlich}, dann ist $|M| = \infty$.

Die Definition über die Bijektion ist notwendig, da für überabzählbar unendliche Mengen, die Elemente nicht gezählt werden können und man nicht von einer \glqq Anzahl der Elemente der Menge\grqq \ sprechen kann.
\end{definition}

\begin{definition}[nicht Teil der VO]
Sei $(V,\Gamma)$ ein Vektorraum und $\mathcal{B}$ eine Basis des Vektorraums. Dann heißt 
\begin{equation*}
\dim_{\Gamma} V := |\mathcal{B}|
\end{equation*}
die \textbf{Dimension} des Vektorraums $V$ über $\Gamma$.

Besitzt $V$ keine endliche Basis, wird $V$ \textbf{unendlich-dimensional} genannt.
\end{definition}

\begin{remark}[nicht Teil der VO]
Ein Vektorraum mit Dimension $0$ besteht nur aus dem Nullpunkt, da jeder Vektorraum das Element $0$ und sich selbst enthält.
Also gilt insbesondere
\begin{equation*}
\dim\{0\}=0
\end{equation*}

\begin{itemize}
    \item $\dim V = 1$  →  Gerade durch den Ursprung
    \item $\dim V = 2$  →  Ebene durch den Ursprung
    \item $\dim V = 3$  →  unendlicher Würfel
\end{itemize}

In \cref{ex:basis} lernten wir Basen von $\mathbb{C}$ und $\mathbb{R}$ kennen. Daher kennen wir nun ihre Dimension:
\begin{itemize}
    \item $\dim_\mathbb{R} \mathbb{R}^2 = 2$
    \item $\dim_\mathbb{R} \mathbb{C} = 2$
    \item $\dim_\mathbb{C} \mathbb{C} = 1$
    \item $\dim_\mathbb{C} \mathbb{C}^2 = 2$
\end{itemize}
\end{remark}
\end{tcolorbox}


\section{Dualraum}
\begin{definition}
    Seien $V$ und $W$ Vektorräume über dem Körper $\Gamma$. Eine Abbildung $\varphi: V \rightarrow W$ heißt \textbf{lineare Abbildung}, wenn für alle $v, w \in V$ und $\lambda \in \Gamma$ die folgenden Bedingungen erfüllt sind:
    \begin{align}
    \varphi(v + w) &= \varphi(v) + \varphi(w) \qquad \text{(Additivität)} \label{eq:Additivität} \\
    \varphi(\lambda \cdot v) &= \lambda \cdot \varphi(v) \qquad \qquad \text{(Homogenität)} \label{eq:Homogenität}
    \end{align}

    Der \textbf{Raum der linearen Abbildungen} ist definiert als
    \begin{equation*}
        L(V, W) = \{\, \varphi : V \to W \mid \varphi \text{ ist linear} \,\}
    \end{equation*}
\end{definition}

Für zwei lineare Abbildungen $\varphi, \psi \in L(V, W)$ definieren wir außerdem Addition und Skalarmultiplikation durch:
\begin{equation*}
\begin{aligned}
+ &: L(V, W) \times L(V, W) \to L(V, W): \ (\varphi,\psi) \mapsto \varphi + \psi \\
\cdot &: \Gamma\times L(V, W) \to L(V, W): \ (\lambda,\varphi) \mapsto \lambda\cdot \varphi
\end{aligned}
\end{equation*}

Wir kennen eine Funktion, wenn wir ihre Wertetabelle kennen. Daher definieren wir, was die Abbildungen für $v \in V$, $\lambda \in \Gamma$ liefern:
\begin{align}
(\varphi + \psi)(v) &:= \varphi(v) + \psi(v) \in W \label{eq:Addition} \\
(\lambda \cdot \varphi)(v) &:= \lambda \cdot \varphi(v) \in W \label{eq:Skalarmultiplikation}
\end{align}

\begin{definition}
Der \textbf{Dualraum} $V^\sim$ eines Vektorraums $(V, \Gamma)$ ist definiert als die Menge aller linearen Abbildungen von $V$ nach $\Gamma$:
\begin{equation*}
V^\sim := L(V, \Gamma)
\end{equation*}
\end{definition}

Jeder Vektorraum hat genau einen Dualraum.

\begin{lemma}\label{lem:DualerVR}
$V^\sim$ ist ein Vektorraum über dem Körper $\Gamma$.
\end{lemma}

\begin{proof}
Wir müssen nachweisen, dass die Rechengesetze eines Vektorraums erfüllt sind.  

\begin{itemize}
    \item Zunächst zeigen wir, dass $(\varphi + \psi)$ und $(\lambda \cdot \psi)$ linear sind, also $\varphi + \psi \in V^\sim$ und $\lambda \cdot \psi \in V^\sim$. Man sagt $V^\sim$ ist \textit{abgeschlossen bzgl. $+$ und $\cdot$}.
    \begin{itemize}
        \item Additivität für $(\varphi + \psi)$: \newline
            Für $v,w \in V$ gilt:
            \begin{align*}
            (\varphi + \psi)(v + w)
            &\overset{\eqref{eq:Addition}}{=} \varphi(v + w) + \psi(v + w) \\
            &\overset{\eqref{eq:Additivität}}{=} \varphi(v) + \varphi(w) + \psi(v) + \psi(w) \\
            &= (\varphi(v) + \psi(v)) + (\varphi(w) + \psi(w)) \\
            &\overset{\eqref{eq:Addition}}{=} (\varphi + \psi)(v) + (\varphi + \psi)(w)
            \end{align*}
            
        \item Homogenität für $(\varphi + \psi)$: \newline
            Ebenso gilt für $\lambda \in \Gamma$:
            \begin{align*}
            (\varphi + \psi)(\lambda v)
            &\overset{\eqref{eq:Skalarmultiplikation}}{=} \varphi(\lambda v) + \psi(\lambda v) \\
            &\overset{\eqref{eq:Homogenität}}{=} \lambda \varphi(v) + \lambda \psi(v) \\
            &\overset{\substack{\text{Linksdistr.} \\ \text{ges. in }\Gamma}}{=} \lambda (\varphi(v) + \psi(v)) \\
            &\overset{\eqref{eq:Skalarmultiplikation}}{=} \lambda (\varphi + \psi)(v)
            \end{align*}
    
        \item Additivität und Homogenität für $(\lambda \cdot \psi)$ sind analog.
    \end{itemize}

    \item Nun müssen die Vektorraumaxiome überprüft werden.
    \begin{itemize}
        \item + bzgl. :
            \begin{align*}
            (\lambda \cdot (\varphi + \psi))(v)
            &\overset{\eqref{eq:Skalarmultiplikation}}{=} \lambda \cdot ((\varphi + \psi)(v))
            \overset{\eqref{eq:Addition}}{=} \lambda (\varphi(v) + \psi(v)) \\
            &\overset{\substack{\text{Linksdistr.} \\ \text{ges. in }\Gamma}}{=} \lambda \varphi(v) + \lambda \psi(v)
            \overset{\eqref{eq:Skalarmultiplikation}}{=} (\lambda \cdot \varphi)(v) + (\lambda \cdot \psi)(v)
            \overset{\eqref{eq:Addition}}{=} ((\lambda \cdot \varphi) + (\lambda \cdot \psi))(v)
            \end{align*}
        \item Die restlichen Vektorraumaxiome für $(\varphi + \psi)$ und $(\lambda \cdot \psi)$ sind dem Leser überlassen :)
    \end{itemize}
\end{itemize}

\end{proof}

\subsection{Basis des Dualraums}
Sei $(V,\Gamma)$ ein Vektorraum mit einer Basis $(E_\alpha)_{\alpha\in I}$. Für ein beliebiges $\varphi \in V^\sim$  definieren wir 
\begin{equation*}
  \varphi_\alpha \coloneqq \varphi(E_\alpha) \in \Gamma, \quad \alpha\in I
\end{equation*}

\begin{theorem}\label{thm:lineareAbbEindeutig}
Sei $(V,\Gamma)$ ein Vektorraum mit einer Basis $(E_\alpha)_{\alpha \in I}$.  
Für jede Familie $(\varphi_\alpha)_{\alpha \in I}$ von Skalaren aus $\Gamma$ existiert genau eine lineare Abbildung
\begin{equation*}
\varphi : V \to \Gamma
\end{equation*}
mit
\begin{equation}\label{eq:phi}
\varphi(E_\alpha) = \varphi_\alpha \qquad \forall\, \alpha \in I.
\end{equation}

Das heißt $\varphi \in V^\sim$ wird eindeutig durch $(\varphi_\alpha)_{\alpha\in I}$ bestimmt.
\end{theorem}

\begin{proof} \breakafterhead

\begin{itemize}
    \item zz: $\varphi$ wird durch $\varphi_\alpha$ bestimmt: \newline
    Für $v=v^\alpha E_\alpha$, mit $v^\alpha \in \Gamma$ gilt:
    \begin{equation*}
      \varphi(v) = \varphi(v^\alpha E_\alpha)
      \overset{\varphi \text{ linear}}{=} v^\alpha \varphi(E_\alpha)
      \overset{\eqref{eq:phi}}{=} v^\alpha \varphi_\alpha.
    \end{equation*}
    \item zz: $\varphi$ ist eindeutig \newline
    Seien $\varphi,\psi\in V^\sim$ mit $\varphi(E_\alpha)=\psi(E_\alpha)$ für alle $\alpha$, so folgt für jedes $v=v^\alpha E_\alpha \in V$
    \begin{equation*}
      \varphi(v)=\varphi(v^\alpha E_\alpha) 
      \overset{\varphi \text{ linear}}{=} v^\alpha\varphi(E_\alpha)=v^\alpha\psi(E_\alpha)
      \overset{\psi \text{ linear}}{=} \psi(v^\alpha E_\alpha)=\psi(v)
    \end{equation*}
    Also $\varphi=\psi$. D.h. Funktionen aus $V^\sim$, die auf einer Basis übereinstimmen, stimmen überall überein.
\end{itemize}
\end{proof}

Wir können nun also zu vorgegebenen Zahlen $(\varphi_\alpha)_{\alpha \in I}$ auf eindeutige Weise eine zugehörige Funktion finden. Für die neutralen Elemente $0$ und $1$ des Vektorraums können wir damit eine Basis für den Dualraum konstruieren.

\begin{satzdefinition}
Sei $(V,\Gamma)$ ein Vektorraum mit dim$V < \infty$, einer Basis $(E_\alpha)_{\alpha \in I}$ und $V^\sim$ sein Dualraum. Dann ist $(e^\alpha)_{\alpha\in I}\subseteq V^\sim$ mit
\begin{equation}\label{eq:dualeBasis}
  e^\alpha(E_\beta) \;=\; \delta^\alpha_\beta,
  \quad \alpha,\beta\in I
\end{equation}

eine Basis von $V^\sim$, genannt \textbf{duale Basis}, wobei $\delta^\alpha_\beta$ das Kronecker-Delta bezeichnet und eine Matrix ist:
\begin{equation*}
  \delta^\alpha_\beta \;=\; \begin{cases}
    1, & \alpha=\beta\\
    0, & \alpha\neq \beta
  \end{cases}
\end{equation*}
\end{satzdefinition}

\begin{proof} \breakafterhead
\begin{itemize}
    \item zz: Die Familie $(e^\alpha)_{\alpha\in I}\subseteq V^\sim$ ist linear unabhängig.
    
        Also zz: $(\lambda_\alpha e^\alpha)(v)=0 \ \Rightarrow \ \lambda_\alpha=0 \ \forall \alpha \in I$ \newline        
        Nach \cref{thm:lineareAbbEindeutig} ist jede lineare Abbildung durch die Bilder der Vektoren einer Basis eindeutig bestimmt. Daher genügt es, die Auswertung von $e^\alpha \in V^\sim$ auf Basiselementen zu betrachten: \newline
        \begin{equation*}
          0 = (\lambda_\alpha e^\alpha)(E_\beta) 
          \overset{\eqref{eq:Skalarmultiplikation}}{=} \lambda_\alpha\, e^\alpha(E_\beta)
          \overset{\eqref{eq:dualeBasis}}{=} \lambda_\alpha\,\delta^\alpha_\beta = \lambda_\beta \quad \forall \beta \in I
        \end{equation*}

    \item zz: Jede Abbildung $\varphi \in V^\sim$ kann als Linearkombination von $(e^\alpha)_{\alpha\in I}$ dargestellt werden. 
    \newline Also zz: $\forall v\in V: \varphi(v) = (\varphi_\alpha e^\alpha)(v) $

    Wieder genügt es, die Auswertung von $\varphi \in V^\sim$ auf Basiselementen zu betrachten. Für $v=v^\alpha E_\alpha$ erhält man
    \begin{equation*}
      \varphi(E_\alpha) = \varphi_\alpha = \varphi_\beta \delta_\alpha^\beta \overset{\eqref{eq:dualeBasis}}{=} \varphi_\beta e^\beta(E_\alpha) \overset{\eqref{eq:Skalarmultiplikation}}{=} (\varphi_\beta e^\beta)(E_\alpha)
    \end{equation*}
    Das letzte Gleichheitszeichen gilt nur, wenn es endlich viele $\varphi_\beta$ gibt, da sonst die Definition der Linearkombination $\varphi_\beta e^\beta$ verletzt ist. Da $\varphi(E_\beta)=\varphi_\beta$ folgt somit, dass es dies nur der Fall ist, wenn es endlich viele $E_\alpha$ gibt, also dim$V < \infty$ bzw. $|I| < \infty$.
\end{itemize}
\end{proof}

Das heißt, sobald wir eine Basis in $V$ wählen, erhalten wir direkt eine Basis in $V^\sim$ dazu, wenn dim$V<\infty$.
\vspace{1em}

\begin{tcolorbox}[noVO]
\begin{remark}[Nicht Teil der VO]\label{rem:dualisom}
Für jeden endlichdimensionalen Vektorraum $V$ existieren zwar viele Vektorraum-Isomorphismen $V \cong V^\sim$ (linear+bijektive Abbildung), jedoch ist keiner davon \emph{kanonisch}. Jeder davon ist basisabhängig oder benötigt eine zusätzliche Struktur (z.B. Skalarprodukt oder Bilinearform).

\underline{Beispiel eines basisabhängigen Isomorphismus:} \newline
Sei $(E_\alpha)_{\alpha \in I}$ eine Basis von $V$ und $(e^\alpha)_{\alpha \in I}$ die dazu duale Basis, also $e^\alpha(E_\beta) = \delta^\alpha_\beta$, $\alpha, \beta \in I$.

Dann kann gezeigt werden, dass die Abbildung
\begin{equation*}
    \Phi : V \to V^\sim, \qquad \Phi(v^\alpha E_\alpha) = v^\alpha e^\alpha
\end{equation*}

ein Isomorphismus ist. Wird eine andere Basis gewählt, so erhält man einen \emph{anderen} Isomorphismus $V \to V^*$. Daher ist diese Abbildung nicht kanonisch.

Da $V \cong V^\sim$ wenn dim$V<\infty$ ist somit dim$V$ = dim$V^\sim$. Wenn dim$V=\infty$ existieren keine bijektiv und linearen Abbildungen $V \to V^\sim$, nur injektive und lineare Abbildungen mit $\Phi(V) \subsetneq V^\sim$. Man sagt $V$ wird in $V^\sim$ eingebettet. Dann ist dim$V<$ dim$V^\sim$.
\end{remark}
\end{tcolorbox}

\section{Bilineare Abbildungen}

Wir betrachten wieder einen Vektorraum $(V,\Gamma)$.  
Der \textbf{Raum der bilinearen Abbildungen} von $V$ nach $\Gamma$ ist definiert als

\begin{equation*}
\mathfrak{Bil}(V,\Gamma) := \{ f : V \times V \to \Gamma \mid f \text{ ist linear in beiden Argumenten} \}.
\end{equation*}

Das heißt, eine Abbildung $f \in \mathfrak{Bil}(V,\Gamma)$ erfüllt für alle $v,v',w,w'\in V$ und $\lambda \in \Gamma$ die Bedingungen:
\begin{align*}
f(v+v',w) &= f(v,w) + f(v',w) \\
f(v,w+w') &= f(v,w) + f(v,w') \\
f(\lambda v, w) &= \lambda f(v,w) \\
f(v, \lambda w) &= \lambda f(v,w)
\end{align*}

Für zwei bilineare Abbildungen $f, g \in \mathfrak{Bil}(V,\Gamma)$ definieren wir ebenso Addition und Skalarmultiplikation durch:
\begin{equation*}
\begin{aligned}
+ &: \mathfrak{Bil}(V,\Gamma) \times \mathfrak{Bil}(V,\Gamma) \to \mathfrak{Bil}(V,\Gamma): \ (f,g) \mapsto f + g \\
\cdot &: \Gamma\times \mathfrak{Bil}(V,\Gamma) \to \mathfrak{Bil}(V,\Gamma): \ (\lambda,f) \mapsto \lambda\cdot f
\end{aligned}
\end{equation*}

Für $v, v', \in V$, $\lambda \in \Gamma$:
\begin{align}
(f + g)(v,v') &:= f(v,v') + g(v,v')  \in \Gamma \label{eq:AdditionBi} \\
(\lambda \cdot f)(v,v') &:= \lambda \cdot f(v,v') \in \Gamma \label{eq:SkalarmultiplikationBi}
\end{align}

\begin{lemma}
Der Raum $\mathfrak{Bil}(V,\Gamma)$ ist ein Vektorraum über $\Gamma$.
\end{lemma}

\begin{proof}
Da wir in \cref{lem:DualerVR} bereits die Linearität der Addition und Skalarmultiplikation in einem Argument gezeigt haben, kann durch Festhalten je eines der beiden Argumente auf die Linearität in beiden Argumenten geschlossen werden.

Ebenso folgen anschließend wie zuvor die Vektorraumaxiome.
\end{proof}

\begin{definition}\label{def:tensorproduktVO}
Seien $V$ ein Vektorraum über $\Gamma$ und $V^\sim$ sein Dualraum. Für zwei Funktionen $\varphi,\psi \in V^\sim$ ist das \textbf{Tensorprodukt} definiert als
\begin{equation}\label{eq:tensorprod}
\begin{aligned}
\otimes : V^\sim \times V^\sim \to \mathfrak{Bil}(V, \Gamma): \ (\varphi,\psi) \mapsto \varphi \otimes \psi \\
(\varphi \otimes \psi)(v,w) := \varphi(v) \cdot \psi(w) \in \Gamma, \qquad v,w\in V
\end{aligned}
\end{equation}

Damit wird der \textbf{Tensorproduktraum zweiter Ordnung} in der VO definiert als
\begin{align}\label{al:tensorprodraum}
\bigotimes\nolimits^{\!2}\! V^\sim := V^\sim\otimes V^\sim
:= \Bigl\{f \in \mathfrak{Bil}(V, \Gamma) \; \big| \; 
f= \sum_{(\varphi, \psi) \in V^{\sim} \times V^{\sim}}\lambda^{(\varphi, \psi)} (\varphi \otimes  \psi), \quad \lambda \in \Gamma, \ \text{nur endlich viele } \lambda^{(\varphi, \psi)} \ne 0\Bigr\}
\end{align}
wobei $f \in \mathfrak{Bil}(V, \Gamma)$ wegen \eqref{eq:tensorprod} und da die Verknüpfung von bilinearen Funktionen wieder bilinear ist.
\end{definition}

$V^\sim \otimes V^\sim$ wird durch Addition und Skalarmultiplikation zum Vektorraum:
\begin{tcolorbox}[noVO]
\begin{align}
\label{eq:additiontensor}
\left(\sum_{i,j} \lambda^{(i,j)} \, (\varphi_i\otimes\psi_j)\right)
+ \left(\sum_{l,k} \mu^{(k,l)}\,(\tilde{\varphi}_k\otimes\tilde{\psi}_l)\right)
&:= \sum_{i,j} \lambda^{(i,j)}\,(\varphi_i\otimes\psi_j)
+ \sum_{l,k} \mu^{(k,l)}\,(\tilde{\varphi}_k\otimes\tilde{\psi}_l) \qquad \text{(Addition)} \\
\mu \cdot\left(\sum_{i,j} \lambda^{(i,j)}\,(\varphi_i\otimes\psi_j)\right)
\label{eq:skalarmulttensor}
&:= \sum_{i,j} (\mu \lambda^{(i,j)})\,(\varphi_i\otimes\psi_j), \quad \mu \in \Gamma \quad \text{(Skalarmultiplikation)}
\end{align}
\end{tcolorbox}
\vspace{1em}

Mit der Definition der Vorlesung \eqref{al:tensorprodraum} folgt eine \textit{mengentheoretische} Teilmengenbeziehung
\begin{equation}\label{eq:teilmenge}
V^\sim\otimes V^\sim \subseteq \mathfrak{Bil}(V,\Gamma)
\end{equation}

Da $V^\sim \otimes V^\sim$ ein Vektorraum ist, kann auch gezeigt werden, dass er ein Untervektorraum von $\mathfrak{Bil}(V,\Gamma)$ ist.
\vspace{2em}

In der Vorlesung wird das Tensorprodukt $\varphi \otimes \psi$ direkt als bilineare Funktion definiert. In der Literatur findet man auch die folgende Definition.
\vspace{1em}

\begin{tcolorbox}[noVO]
{\footnotesize Quellen: \href{https://duncan.math.sc.edu/s23/math742/notes/lin_alg.pdf}{Multilinear Algebra by Alexander Duncan, 3. Kapitel Tensor Products, Definition 3.1.} \\
\url{https://en.wikipedia.org/wiki/Tensor_product#Linearly_disjoint}}

\begin{definition}[Nicht Teil der Vorlesung]\label{def:tensorproduktFormal}
Seien $V$ ein Vektorraum über $\Gamma$ und $V^\sim$ sein Dualraum. Für zwei Funktionen $\varphi,\psi \in V^\sim$ ist das \textbf{Tensorprodukt} definiert als
\begin{equation*}
\begin{aligned}
\otimes : V^\sim \times V^\sim \to V^\sim \otimes V^\sim: \ (\varphi,\psi) \mapsto \varphi \otimes \psi \\
(\varphi \otimes \psi)(v,w) := \varphi(v) \cdot \psi(w) \in \Gamma, \qquad v,w\in V
\end{aligned}
\end{equation*}

Damit wird der \textbf{Tensorproduktraum zweiter Ordnung} in der Literatur definiert als
\begin{align*}
\bigotimes\nolimits^{\!2}\! V^\sim := V^\sim\otimes V^\sim
&:= \operatorname{span}\bigl\{\,\varphi\otimes\psi \;\big|\; \varphi,\psi\in V^\sim\,\bigr\}
= \Bigl\{\,\sum_{i=1}^N \lambda^i\,(\varphi_i\otimes\psi_i)\ \Big|\ 
N\in\mathbb{N},\ \lambda^i\in\Gamma,\ \varphi_i,\psi_i\in V^\sim \Bigr\} \\
\end{align*}
Hier ist $\sum_{i=1}^N \lambda^i\,(\varphi_i\otimes\psi_i)$ eine \textit{formale Summe}: die Ausdrücke $\varphi_i\otimes\psi_i$ werden zunächst \emph{nicht} als Funktionen verstanden, sondern nur als Symbole. Man betrachtet sie also wie Bausteine eines neu konstruierten Vektorraums, auf denen Addition und Skalarmultiplikation wie gewohnt definiert sind. Erst durch die Abbildung 
\begin{equation}\label{eq:bigphi}
\Phi : V^\sim \otimes V^\sim \to \mathfrak{Bil}(V,\Gamma): \ \varphi\otimes\psi \mapsto \varphi \cdot\psi
\end{equation}
erhalten diese formalen Summen eine Bedeutung als \textit{konkrete bilineare Abbildungen}.

Dann ist
\begin{align*}
\Phi(V^\sim\otimes V^\sim) = \Bigl\{f \in \mathfrak{Bil}(V, \Gamma) \; \big| \; 
f= \sum_{(\varphi, \psi) \in V^{\sim} \times V^{\sim}}\lambda^{(\varphi, \psi)} \Phi(\varphi \otimes  \psi), \quad \lambda \in \Gamma, \ \text{nur endlich viele } \lambda^{(\varphi, \psi)} \ne 0\Bigr\}
\end{align*}

Mit dieser Definition ist $\Phi(V^\sim\otimes V^\sim) \subseteq \mathfrak{Bil}(V,\Gamma)$.

Es kann gezeigt werden, dass $\Phi$ ein \textit{kanonischer oder natürlicher Vektorraum-Isomorphismus} ist. Kanonisch heißt, der Isomorphismus ergibt sich allein aus der Struktur, die die beteiligten Räume schon von Natur aus haben. Er hängt nicht von der Wahl der Basis ab. Hier wird also die Definition \eqref{eq:tensorprod} bzgl. dieses kanonischen Isomorphismus verstanden.

Ein \textit{Vektorraum-Isomorphismus} ist definiert als lineare und bijektive Abbildung.

Das heißt: 
\begin{itemize}
    \item $(V^\sim \otimes V^\sim, +, \cdot)$ und $(\mathfrak{Bil}(V,\Gamma),+,\cdot)$ haben dieselbe \textbf{algebraische Struktur}: Addition und Skalarmultiplikation erfüllen dieselben Rechenregeln (Vektorraumaxiome) und werden, da $\Phi$ linear ist, strukturerhaltend abgebildet:
    
    $\forall \varphi_1,\varphi_2,\psi_1,\psi_2 \in V^\sim$ und $\lambda \in \Gamma$ gilt:
    \begin{equation}\label{eq:linearität}
    \Phi((\varphi_1 \otimes\psi_1) + (\varphi_2 \otimes\psi_2))
      = \Phi(\varphi_1 \otimes\psi_1) + \Phi(\varphi_2 \otimes\psi_2),
    \qquad
    \Phi(\lambda \cdot (\varphi \otimes\psi))
      = \lambda \cdot \Phi(\varphi \otimes\psi)
    \end{equation}
    
    \item $V^\sim \otimes V^\sim$ und $\mathfrak{Bil}(V,\Gamma)$ sind \textbf{gleichmächtig}, da $\Phi$ bijektiv ist: zu jedem $g \in \mathfrak{Bil}(V,\Gamma)$ kann genau ein $f \in V^\sim \otimes V^\sim$ gefunden werden, mit dem es durch $\Phi(f)=g$ identifiziert wird.
    \item Die beiden Räume sind also \textbf{nicht identisch}, verhalten sich jedoch gleich. Wenn dim$V< \infty$ gilt $\Phi(V^\sim \otimes V^\sim) = \mathfrak{Bil}(V,\Gamma)$, siehe \cref{thm:biliso}.
\end{itemize}
\end{definition}
\end{tcolorbox}

\begin{theorem}[mit \cref{def:tensorproduktVO}]\label{thm:biltensor}
Wenn der Vektorraum $V$ endlich dimensional ist, ist der Tensorproduktraum bereits \textit{mengentheoretisch} gleich dem Raum der bilinearen Abbildungen:
\begin{equation*}
V^\sim \otimes V^\sim = \mathfrak{Bil}(V,\Gamma)
\end{equation*}
\end{theorem}


\begin{proof} 
$\subseteq$ wurde bereits in \eqref{eq:teilmenge} argumentiert. Daher zeigen wir nun $\supseteq$.

zz: $\forall f \in \mathfrak{Bil}(V,\Gamma) \Rightarrow f \in V^\sim\otimes V^\sim$

Sei $(E_\alpha)_{\alpha\in I}$ eine Basis des Vektorraums $V$ und $f\in\mathfrak{Bil}(V,\Gamma)$. 
Wir definieren
\begin{equation}\label{eq:ftensor}
f(E_\alpha, E_\beta) =: f_{\alpha\beta} \in \Gamma
\end{equation}

Zeige: $f$ wird durch $f_{\alpha\beta}$ eindeutig bestimmt.

Für beliebige Vektoren $v = v^\alpha E_\alpha$ und $w = w^\beta E_\beta$ gilt:
\begin{equation*}
f(v,w) = f(v^\alpha E_\alpha,\, w^\beta E_\beta)
\overset{f \text{ bilinear}}{=} v^\alpha w^\beta f(E_\alpha, E_\beta)
\overset{\eqref{eq:ftensor}}{=} v^\alpha w^\beta f_{\alpha\beta}
\end{equation*}
Die Abbildung wird also in der Tat durch $f_{\alpha\beta}$ bestimmt. Die Beweisführung der Eindeutigkeit ist gleich wie die im Beweis von \eqref{thm:lineareAbbEindeutig}: zwei Funktionen, die auf einer Basis übereinstimmen, stimmen überall überein.

Kann \eqref{eq:ftensor} nun verwenden, um die Aussage zu zeigen: \newline
Sei $(e^\alpha)_{\alpha\in I}$ die duale Basis. Dann gilt
\begin{align*}
f(E_\alpha, E_\beta)
&\overset{\eqref{eq:f}}{=} f_{\alpha \beta}
= f_{\gamma \delta} \delta_\alpha^\gamma \delta_\beta^\delta
\overset{\substack{\text{duale} \\ \text{Basis}}}{=} f_{\gamma\delta}\,e^\gamma(E_\alpha)\,e^\delta(E_\beta) \\
&\overset{\substack{\eqref{eq:tensorprod}}}{=} f_{\gamma \delta}(e^\gamma \otimes e^\delta)(E_\alpha, E_\beta) 
\overset{\eqref{eq:skalarmulttensor}}{=} (f_{\gamma \delta}e^\gamma \otimes e^\delta)(E_\alpha, E_\beta)
\end{align*}

Damit das letzte Gleichheitszeichen gilt, also die Definition der Linearkombination nicht verletzt wird, muss wieder $(E_\alpha)_{\alpha \in I}$ endlich sein, also dim$V< \infty$.

Damit stimmen $f$ und $f_{\gamma \delta}e^\gamma \otimes e^\delta$ auf einer Basis überein und stimmen somit überall überein. Also ist gezeigt, dass jede bilineare Abbildung $f$ als endliche Summe
\begin{equation*}
f = f_{\gamma\delta}\, e^\gamma \otimes e^\delta \in V^\sim\otimes V^\sim.
\end{equation*}
geschrieben werden kann. Daraus folgt
\begin{equation*}
\mathfrak{Bil}(V,\Gamma) \subseteq V^\sim\otimes V^\sim.
\end{equation*}

\end{proof}

\begin{tcolorbox}[noVO]
\begin{theorem}[nicht Teil der Vorlesung mit \cref{def:tensorproduktFormal}]\label{thm:biliso}
Wenn der Vektorraum $V$ endlich dimensional ist, ist der Tensorproduktraum isomorph zum Raum der bilinearen Abbildungen:
\begin{equation*}
V^\sim \otimes V^\sim \;\cong\; \mathfrak{Bil}(V,\Gamma)
\end{equation*}
Das heißt die Abbildung \eqref{eq:bigphi} ist ein Vektorraum-Isomorphismus.

Wenn der Vektorraum $V$ unendlich dimensional ist, ist $\Phi$ nicht surjektiv. Es gilt also nur $\Phi(V^\sim \otimes V^\sim) \subsetneq \mathfrak{Bil}(V,\Gamma)$. Man sagt, $V^\sim \otimes V^\sim$ wird in $\mathfrak{Bil}(V,\Gamma)$ eingebettet.
\end{theorem}

{\footnotesize Quelle: {\href{https://duncan.math.sc.edu/s23/math742/notes/lin_alg.pdf}{Multilinear Algebra by Alexander Duncan, 3. Kapitel Tensor Products, Corollary 3.8.}}}

\end{tcolorbox}


\begin{lemma}
    Sei $(E_\alpha)_{\alpha \in I}$ eine Basis von $V$ und $(e^\alpha)_{\alpha \in I}$ die zugehörige duale Basis. Dann bildet die Menge $\{ e^\alpha \otimes e^\beta \}$ eine Basis von $\mathfrak{Bil}(V,\Gamma)$.
\end{lemma}

\begin{proof} \breakafterhead
\begin{itemize}
    \item Wir haben im Beweis von \cref{thm:biltensor} bereits gezeigt, dass jede bilineare Abbildung $f$ als Linearkombination
    \begin{equation*}
    f = f_{\alpha\beta}\, e^\alpha \otimes e^\beta
    \end{equation*}
    dargestellt werden kann.  
    Daher ist $\{ e^\alpha \otimes e^\beta \}$ ein Erzeugendensystem.

    \item zz: Die Familie $\{ e^\alpha \otimes e^\beta \} \subseteq \mathfrak{Bil}(V,\Gamma)$ ist linear unabhängig.
    
        Also zz: $\forall v,w \in V: \ (\lambda_{\alpha \beta} e^\alpha \otimes e^\beta)(v,w)=0 \ \Rightarrow \ \lambda_{\alpha \beta}=0 \ \forall \alpha, \beta \in I$ \newline        
        Nach \cref{thm:lineareAbbEindeutig} ist jede lineare Abbildung durch die Bilder der Vektoren einer Basis eindeutig bestimmt. Daher genügt es, die Auswertung von $e^\alpha \otimes e^\beta \in \mathfrak{Bil}(V,\Gamma)$ auf Basiselementen zu betrachten: \newline
        \begin{equation*}
          0 = (\lambda_{\alpha \beta} e^\alpha \otimes e^\beta)(E_\gamma, E_\delta) 
          \overset{\eqref{def:tensorproduktVO}}{=} \lambda_{\alpha \beta} e^\alpha (E_\gamma) \cdot e^\beta (E_\delta)
          \overset{\eqref{eq:dualeBasis}}{=} \lambda_{\alpha \beta} \delta^\alpha_\gamma \delta^\beta_\delta = \lambda_{\gamma \delta} \quad \forall \gamma, \delta \in I
        \end{equation*}

\end{itemize}
\end{proof}

\section{Multilineare Abbildungen}

Analog zum bilinearen Fall betrachten wir Abbildungen, die in mehreren Argumenten linear sind.

\begin{definition}
Der \textbf{Raum der p-multilinearen Abbildungen} ist gegeben durch
\begin{equation*}
\mathfrak{Mult_p}(V,\Gamma) := \{ f : \underbrace{V \times \cdots \times V}_{\displaystyle \text{\small p mal}} \to \Gamma \mid f \text{ ist linear in jedem Argument} \}
\end{equation*}
\end{definition}

Wir definieren wir Addition und Skalarmultiplikation durch
\begin{equation*}
\begin{aligned}
+ &: \mathfrak{Mult_p}(V,\Gamma) \times \mathfrak{Mult_p}(V,\Gamma) \to \mathfrak{Mult_p}(V,\Gamma): \ (f,g) \mapsto f + g \\
\cdot &: \Gamma\times \mathfrak{Mult_p}(V,\Gamma) \to \mathfrak{Mult_p}(V,\Gamma): \ (\lambda,f) \mapsto \lambda\cdot f
\end{aligned}
\end{equation*}

Für $v_i \in V$, $\lambda \in \Gamma$:
\begin{align}
(f+g)(v_1,\dots,v_p) &:= f(v_1,\dots,v_p) + g(v_1,\dots,v_p) \in \Gamma\\
(\lambda \cdot f)(v_1,\dots,v_p) &:= \lambda \cdot f(v_1,\dots,v_p) \in \Gamma
\end{align}
womit durch Festhalten aller Argumente, bis auf eines und der Linearität in einem Argument gezeigt werden kann, dass $\mathfrak{Mult_p}(V,\Gamma)$ wieder ein Vektorraum über $\Gamma$ ist.

\begin{definition}\label{def:tensorprodMult_p}
Für $\varphi_1,\dots,\varphi_p \in V^\sim$ definieren wir das \textbf{p-fache Tensorprodukt}
\begin{equation}
\otimes : \underbrace{V^\sim \times \cdots \times V^\sim}_{\displaystyle \text{\small p mal}} \to \mathfrak{Mult_p}(V,\Gamma): \ (\varphi_1, ..., \varphi_p) \mapsto \varphi_1 \otimes \cdots \otimes \varphi_p \\
\end{equation}

Für $v_1, ..., v_p \in V$ ist
\begin{equation}\label{eq:multtensorprod}
(\varphi_1 \otimes \cdots \otimes \varphi_p)(v_1,\dots,v_p)
:= \varphi_1(v_1) \cdot ... \cdot \varphi_p(v_p) \in \Gamma
\end{equation}

Damit wird der \textbf{Tensorproduktraum p-ter Ordnung} in der VO definiert als
\begin{align*}
&\bigotimes\nolimits^{\!p}\! V^\sim 
:= \underbrace{V^\sim \otimes \cdots \otimes V^\sim}_{\displaystyle \text{\small p mal}} \\
&:= \Bigl\{f \in \mathfrak{Mult_p}(V, \Gamma) \; \big| \; 
f= \sum_{(\varphi_1, ..., \varphi_p) \in \underbrace{\scriptstyle V^\sim \times \cdots \times V^\sim}_{\displaystyle \text{\tiny p mal}}}
\lambda^{(\varphi_1, ..., \varphi_p)} (\varphi_1 \otimes \cdots \otimes  \varphi_p), \; \lambda \in \Gamma, \; \text{\small nur endl. viele } \lambda^{(\varphi_1, ..., \varphi_p)} \ne 0\Bigr\}
\end{align*}
wobei $f \in \mathfrak{Mult_p}(V, \Gamma)$ da per \cref{def:tensorprodMult_p} $\varphi_1 \otimes \cdots \otimes \varphi_p \in \mathfrak{Mult_p}(V, \Gamma)$ und da die Verknüpfung von multilinearen Funktionen wieder multilinear ist.

Damit gilt wieder insbesondere (mengentheoretisch)
\begin{equation}\label{eq:teilmengeMult_p}
\bigotimes\nolimits^{\!p}\! V^\sim  \subseteq \mathfrak{Mult_p}(V,\Gamma)
\end{equation}
\end{definition}

$\mathfrak{Mult_p}(V,\Gamma)$ wird mit der Addition und Skalarmultiplikation zum Vektorraum. Es kann wieder gezeigt werden, dass $\bigotimes\nolimits^{\!p}\! V^\sim$ dann ein Untervektorraum von $\mathfrak{Mult_p}(V,\Gamma)$ ist.
\vspace{1em}

\begin{tcolorbox}[noVO]
\begin{remark}[Nicht Teil der VO]\label{rem:Mult_pisom}
Wieder kann obige Definition in der Literatur mit
\begin{equation}
\otimes : \underbrace{V^\sim \times \cdots \times V^\sim}_{\displaystyle \text{\small p mal}} \to \underbrace{V^\sim \otimes \cdots \otimes V^\sim}_{\displaystyle \text{\small p mal}}: \ (\varphi_1, ..., \varphi_p) \mapsto \varphi_1 \otimes \cdots \otimes \varphi_p
\end{equation}

und bzgl. des kanonischen Isomorphismus
\begin{equation}
\begin{aligned} \label{eq:Mult_piso}
&\hspace{2.5em}
\Phi : \bigotimes\nolimits^{\!p}\! V^\sim \to \mathfrak{Mult_p}(V,\Gamma): \varphi_1 \otimes \cdots \otimes \varphi_p \mapsto \varphi_1 \cdot ... \cdot \varphi_p \\
& \text{mit } v_1, ..., v_p \in V: \ \Phi(\varphi_1 \otimes \cdots \otimes \varphi_p)(v_1, ..., v_p) := \varphi_1(v_1) \cdot ... \cdot \varphi_p(v_p)
\end{aligned}
\end{equation}

gefunden werden.
\end{remark}
\end{tcolorbox}

\begin{theorem}\label{thm:tensorprodraumgleichmulp}
Wenn der Vektorraum $V$ endlich dimensional ist, ist der Tensorproduktraum p-ter Ordnung mengentheoretisch gleich dem Raum der p-multilinearen Abbildungen:
\begin{equation*}
\bigotimes\nolimits^{\!p}\! V^\sim = \mathfrak{Mult_p}(V,\Gamma)
\end{equation*}
\end{theorem}

\begin{proof}
$\subseteq$ wurde bereits in \eqref{eq:teilmengeMult_p} argumentiert. Daher zeigen wir nun $\supseteq$.

zz: $\forall f \in \mathfrak{Mult_p}(V,\Gamma) \Rightarrow f \in \bigotimes\nolimits^{\!p}\! V^\sim$

Sei $(E_\alpha)_{\alpha\in I}$ eine Basis von $V$ und $(e^\alpha)_{\alpha\in I}$ die duale Basis. 
Dann ist für $f\in\mathfrak{Mult_p}(V,\Gamma)$
\begin{equation}\label{eq:fmult}
f(E_{\alpha_1},\dots,E_{\alpha_p}) =: f_{\alpha_1\dots\alpha_p} \in \Gamma.
\end{equation}

Der Beweis, dass $f$ durch diese Darstellung eindeutig bestimmt ist, ist analog zu dem Abschnitt im Beweis von \cref{thm:biliso}.

Sei nun $f \in \mathfrak{Mult_p}(V,\Gamma)$ und 
$(E_\alpha)_{\alpha\in I}$ eine Basis von $V$ mit dualer Basis $(e^\alpha)_{\alpha\in I}$.
Dann gilt analog zum bilinearen Fall:
\begin{equation*}
\begin{aligned}
f(E_{\alpha_1},\ldots,E_{\alpha_p})
&\overset{\eqref{eq:fmult}}{=} f_{\alpha_1\ldots\alpha_p}
=f_{\beta_1\ldots\beta_p}\,\delta_{\alpha_1}^{\beta_1}\ldots\delta_{\alpha_p}^{\beta_p} \\
&\overset{\substack{\text{duale} \\ \text{Basis}}}{=} f_{\beta_1\ldots\beta_p}\, e^{\beta_1}(E_{\alpha_1})\ldots e^{\beta_p}(E_{\alpha_p}) \\
&\overset{\eqref{eq:multtensorprod}}{=} f_{\beta_1\ldots\beta_p}\,
   (e^{\beta_1}\otimes\cdots\otimes e^{\beta_p})(E_{\alpha_1},\ldots,E_{\alpha_p}) \\
&\overset{\substack{\text{Skalar-} \\ \text{mult.}}}{=} (\,f_{\beta_1\ldots\beta_p}\, e^{\beta_1}\otimes\cdots\otimes e^{\beta_p}\,)
   (E_{\alpha_1},\ldots,E_{\alpha_p})
\end{aligned}
\end{equation*}

Damit das letzte Gleichheitszeichen gilt, also die Definition der Linearkombination nicht verletzt wird, muss wieder $(E_\alpha)_{\alpha \in I}$ endlich sein, also dim$V< \infty$.

Damit stimmen $f$ und $f_{\beta_1\ldots\beta_p}\, e^{\beta_1}\otimes\cdots\otimes e^{\beta_p}$ auf einer Basis überein und stimmen somit überall überein. Also ist gezeigt, dass jede multilineare Abbildung $f$ als endliche Summe
\begin{equation*}
f = f_{\beta_1\ldots\beta_p}\, e^{\beta_1}\otimes\cdots\otimes e^{\beta_p} \in \bigotimes\nolimits^{\!p}\! V^\sim
\end{equation*}
dargestellt werden kann.
\end{proof}

\begin{tcolorbox}[noVO]
\begin{theorem}[Nicht Teil der VO mit \cref{rem:Mult_pisom}]
Wenn der Vektorraum $V$ endlich dimensional ist, ist der Tensorproduktraum p-ter Ordnung isomorph zum Raum der p-multilinearen Abbildungen:
\begin{equation*}
\bigotimes\nolimits^{\!p}\! V^\sim \cong \mathfrak{Mult_p}(V,\Gamma)
\end{equation*}

Das heißt die Abbildung $\Phi$ von \eqref{eq:Mult_piso} ist ein Vektorraum-Isomorphismus.

Wenn der Vektorraum $V$ unendlich dimensional ist, ist $\Phi$ nicht surjektiv. Es gilt also nur $\Phi(\bigotimes\nolimits^{\!p}\! V^\sim) \subsetneq \mathfrak{Mult_p}(V,\Gamma)$. Man sagt, $\bigotimes\nolimits^{\!p}\! V^\sim$ wird in $\mathfrak{Mult_p}(V,\Gamma)$ eingebettet.
\end{theorem}
\end{tcolorbox}

\begin{lemma}
    Sei $(E_\alpha)_{\alpha \in I}$ eine Basis von $V$ und $(e^\alpha)_{\alpha \in I}$ die zugehörige duale Basis. Dann bildet die Menge $\{ e^{\alpha_1} \otimes \cdots \otimes e^{\alpha_p} \}$ eine Basis von $\mathfrak{Mult_p}(V,\Gamma)$.
\end{lemma}

\begin{proof} \breakafterhead
\begin{itemize}
    \item Wir haben im Beweis von \cref{thm:tensorprodraumgleichmulp} bereits gezeigt, dass jede multilineare Abbildung $f$ als Linearkombination
    \begin{equation*}
    f = f_{\alpha_1\ldots\alpha_p}\, e^{\alpha_1}\otimes\cdots\otimes e^{\alpha_p}
    \end{equation*}
    dargestellt werden kann.  
    Daher ist $\{ e^{\alpha_1} \otimes \cdots \otimes e^{\alpha_p} \}$ ein Erzeugendensystem.

    \item zz: Die Familie $\{ e^{\alpha_1} \otimes \cdots \otimes e^{\alpha_p} \} \subseteq \mathfrak{Mult_p}(V,\Gamma)$ ist linear unabhängig.
    
        Also zz: $\forall v_1,...,v_p \in V: \ (\lambda_{\alpha_1...\alpha_p} e^{\alpha_1} \otimes \cdots \otimes e^{\alpha_p})(v_1,...,v_p)=0 \ \Rightarrow \ \lambda_{\alpha_1...\alpha_p}=0 \ \forall \alpha_1,...,\alpha_p \in I$ \newline        
        Nach \cref{thm:lineareAbbEindeutig} ist jede lineare Abbildung durch die Bilder der Vektoren einer Basis eindeutig bestimmt. Daher genügt es, die Auswertung von $e^{\alpha_1} \otimes \cdots \otimes e^{\alpha_p} \in \mathfrak{Mult_p}(V,\Gamma)$ auf Basiselementen zu betrachten: \newline
        \begin{align*}
          0 &= (\lambda_{\alpha_1...\alpha_p} e^{\alpha_1} \otimes \cdots \otimes e^{\alpha_p})(E_{\beta_1}, ..., E_{\beta_p}) 
          \overset{\eqref{def:tensorproduktVO}}{=} \lambda_{\alpha_1...\alpha_p} e^{\alpha_1}(E_{\beta_1})\cdot ... \cdot e^{\alpha_p}(E_{\beta_p}) \\
          &\overset{\eqref{eq:dualeBasis}}{=} \lambda_{\alpha_1...\alpha_p} \delta^{\alpha_1}_{\beta_1}\cdot ... \cdot \delta^{\alpha_p}_{\beta_p} = \lambda_{\beta_1 ...\beta_p} \quad \forall \beta_1, ..., \beta_p \in I
        \end{align*}

\end{itemize}
\end{proof}

\section{Doppeldualraum}
Jeder Vektorraum hat einen Dualraum. Da der Dualraum $V^\sim$ ein Vektorraum ist, hat auch dieser einen Dualraum.

\begin{definition}
Der \textbf{Doppeldualraum} (auch \textbf{Bidualraum}) eines Vektorraums $(V,\Gamma)$ ist der Dualraum des Dualraums:
\begin{equation*}
V^{\sim\sim} := (V^\sim)^\sim = \{\psi: V^\sim \to \Gamma : \Psi \text{ ist linear}\}
\end{equation*}

\begin{figure}[H]
\centering
\includegraphics[width=0.3\textwidth]{images/Bidualraum1.jpg}
\end{figure}
\end{definition}

Wie hängen nun $V$ und $V^{\sim\sim}$ zusammen?

Es gibt eine kanonische Einbettung
\begin{equation*}
\Phi : V \to V^{\sim\sim}: v \mapsto \Phi_v
\end{equation*}

wobei das Punktauswertungsfunktional $\Phi_v$ folgendermaßen agiert
\begin{equation}\label{eq:Pktauswertungsfunkt}
\Phi_v: V^\sim \to \Gamma: \varphi \mapsto \Phi_v(\varphi) := \varphi(v)
\end{equation}

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{images/Bidualraum.jpg}
\end{figure}

\begin{lemma}
$\Phi$ ist linear.
\end{lemma}

\begin{proof} \breakafterhead
\begin{itemize}
    \item zz: $\Phi$ ist wohldefiniert. D.h. $\Phi_v \in V^{\sim\sim}$.
    
    Wir müssen zeigen, dass $\Phi_v$ linear ist.

Für $\varphi_1,\varphi_2 \in V^\sim$ und $\lambda \in \Gamma$ gilt:
\begin{align*}
    \Phi_v(\varphi_1 + \varphi_2)
    &\overset{\eqref{eq:Pktauswertungsfunkt}}{=} (\varphi_1 + \varphi_2)(v)
    \overset{\substack{\varphi_1, \varphi_2 \\ \text{linear}}}{=} \varphi_1(v) + \varphi_2(v)
    \overset{\eqref{eq:Pktauswertungsfunkt}}{=} \Phi_v(\varphi_1) + \Phi_v(\varphi_2) \\
    \Phi_v(\lambda \cdot \varphi)
    &\overset{\eqref{eq:Pktauswertungsfunkt}}{=} (\lambda \cdot \varphi)(v)
    \overset{\varphi \text{ linear}}{=} \lambda \cdot \varphi(v)
    \overset{\eqref{eq:Pktauswertungsfunkt}}{=} \lambda \cdot \Phi_v(\varphi)
\end{align*}

\item zz: $\Phi$ ist linear

Für $v, v_1,v_2 \in V$ und $\lambda \in \Gamma$ gilt:
\begin{align*}
    \Phi_{v_1 + v_2}(\varphi)
    &\overset{\eqref{eq:Pktauswertungsfunkt}}{=} \varphi(v_1 + v_2)
    \overset{\varphi \text{ linear}}{=} \varphi(v_1) + \varphi(v_2)
    \overset{\eqref{eq:Pktauswertungsfunkt}}{=} \Phi_{v_1}(\varphi) + \Phi_{v_2}(\varphi)
    \overset{\substack{\text{Addition} \\ \text{in $V^{\sim\sim}$}}}{=} (\Phi_{v_1} + \Phi_{v_2})(\varphi) \\
    \Phi_{\lambda \cdot v}(\varphi)
    &\overset{\eqref{eq:Pktauswertungsfunkt}}{=} \varphi(\lambda \cdot v)
    \overset{\varphi \text{ linear}}{=} \lambda \cdot \varphi(v)
    \overset{\eqref{eq:Pktauswertungsfunkt}}{=} \lambda \cdot \Phi_v(\varphi)
    \overset{\substack{\text{Skalarmult} \\ \text{in $V^{\sim\sim}$}}}{=} (\lambda \cdot \Phi_v)(\varphi)
\end{align*}

Da diese Gleichungen für alle $\varphi \in V^\sim$ gelten, folgt:
\begin{equation}\label{eq:phivlinear}
    \Phi_{v_1 + v_2} = \Phi_{v_1} + \Phi_{v_2}, 
    \qquad 
    \Phi_{\lambda \cdot v} = \lambda \cdot \Phi_v
\end{equation}
\end{itemize}
\end{proof}

\begin{remark}
Da $\Phi$ \textbf{linear} (strukturerhaltend) ist, macht es also keinen Unterschied, ob ich zuerst in $V$ addiere mit $v_1+v_2$ und dann $\Phi_{v_1+v_2}$ auf $\varphi$ anwende oder ob ich zuerst in $V^{\sim\sim}$ addiere mit $\Phi_{v_1}+\Phi_{v_2}$ und dann $(\Phi_{v_1}+\Phi_{v_2})$ auf $\varphi$ anwende. Selbiges gilt für das Skalarprodukt. Daher müssen wir nicht mehr die Addition bzw. die Skalarmultiplikation in $V$ von der Addition bzw. die Skalarmultiplikation in $V^{\sim\sim}$ unterscheiden. Somit agieren Elemente aus $V$ gleich wie Elemente aus $V^{\sim\sim}$. Also können wir $v \in V$ mit $\Phi_v \in V^{\sim\sim}$ identifizieren. Wir definieren daher
\begin{equation}\label{eq:vid}
    v(\varphi)
    := \Phi_v(\varphi)
    = \varphi(v)
    \in \Gamma
\end{equation}
Mit dieser Definition folgt dann die mengentheoretische Teilmengenbeziehung
\begin{equation*}
    V \subseteq V^{\sim\sim}
\end{equation*}
Da $V^{\sim\sim}$ ein Vektorraum ist, kann auch gezeigt werden, dass $V$ mit dieser Definition ein Untervektorraum von $V^{\sim\sim}$ ist.
\end{remark}

\begin{tcolorbox}[noVO]
\begin{remark}[Nicht Teil der VO]
Wenn obige Definition nicht verwendet wird, kann gezeigt werden, dass $\Phi$ ein kanonischer Isomorphismus ist und somit $V \cong V^{\sim\sim}$. Damit gibt es für jedes $\Phi_v \in V^{\sim\sim}$ genau ein $v \in V$, sodass $\Phi(v) = \Phi_v$ und für $\varphi \in V^\sim: \ \Phi(v)(\varphi)=\Phi_v(\varphi)$. Da die Struktur durch die Linearität von $\Phi$ auch erhalten bleibt und für $\Phi$ keine zusätzliche Struktur erfordert bzw. von der Wahl der Basis unabhängig ist, wird in der Vorlesung direkt obige Definition gewählt. \newline
Da $V \cong V^{\sim\sim}$ wenn dim$V<\infty$ ist somit dim$V=$ dim$V^{\sim\sim}$. Mit \cref{rem:dualisom} folgt dann dim$V^\sim$= dim$V^{\sim\sim}$. Wenn dim$V=\infty$ ist dim$V<$ dim$V^{\sim\sim}$.
\end{remark}
\end{tcolorbox}

\begin{theorem}
Wenn der Vektorraum $V$ endlich dimensional ist, ist er bereits \textit{mengentheoretisch} gleich seinem Bidualraum:
\begin{equation*}
V = V^{\sim\sim}
\end{equation*}
\end{theorem}

\begin{proof}
    $\supseteq$ haben wir zuvor schon argumentiert. Wir zeigen nun $\subseteq$.
    
    zz: $\Psi \in V^{\sim\sim} \Rightarrow \Psi \in V$

    Sei also $\Psi \in V^{\sim\sim}$, $(E_\alpha)_{\alpha \in I}$ eine Basis von $(V,\Gamma)$ und $(e^\alpha)_{\alpha \in I}$ die dazu duale Basis des Dualraums $(V^\sim,\Gamma)$.
    
    Dann ist
    \begin{equation}
        \Psi(e^\alpha) =: \underbrace{\Psi^\alpha }_{\displaystyle \in \Gamma}
        = \Psi^\beta \delta^\alpha_{\beta}
        \overset{\substack{\text{kanon.} \\ \text{Basis}}}{=} \Psi^\beta\, e^\alpha(E_\beta)
        \overset{\eqref{eq:vid}}{=} \Psi^\beta\, E_\beta(e^\alpha)
        \overset{\substack{\text{Skalar.} \\ \text{mult.}}}{=} (\Psi^\beta E_\beta)(e^\alpha)
    \end{equation}
    
    Wieder ist das letzte Gleichheitszeiten nur für endlich viele $(E_\alpha)_{\alpha \in I}$ zulässig. Zwei lineare Funktionale, die auf allen Basisvektoren übereinstimmen, stimmen auch auf allen Vektoren überein. Daher ist:
    \begin{equation}
        \Psi = \Psi^\beta\, E_\beta \in V
    \end{equation}
\end{proof}

\begin{definition}
Der \textbf{Raum der p-multilinearen Abbildungen auf dem Dualraum} ist definiert als
\begin{equation*}
    \mathfrak{Mult_p}(V^\sim, \Gamma):= \{f : \underbrace{V^\sim \times \cdots \times V^\sim}_{p\text{-mal}}\to \Gamma \ | \  f \text{ ist linear in jedem Argument}\}
\end{equation*}
\end{definition}

In vollkommener Analogie zu $\mathfrak{Mult_p}(V,\Gamma)$ gilt im endlichdimensionalen Fall:
\begin{equation*}
    \mathfrak{Mult_p}(V^\sim,\Gamma)
    = \bigotimes\nolimits^p V^{\sim\sim}
    = \bigotimes\nolimits^p V
\end{equation*}

und
\begin{equation*}
(v_1 \otimes \cdots \otimes v_p)(\varphi_1, ..., \varphi_p)=v_1(\varphi_1) \cdot ... \cdot v_p(\varphi_p)=\varphi_1(v_1)\cdot ... \cdot \varphi_p(v_p) \in \Gamma
\end{equation*}

\section{Gemischte multilineare Abbildungen}
In den bisherigen Abschnitten haben wir multilineare Abbildungen betrachtet, deren Argumente entweder nur aus dem Vektorraum $V$ oder nur aus dem Dualraum $V^\sim$ stammen. Für die Tensorrechnung in der Relativitätstheorie benötigen wir jedoch allgemeinere Abbildungen, die \emph{sowohl} Vektoren aus $V$ als auch Linearformen aus $V^\sim$ als Argumente aufnehmen und in jedem Argument linear sind.

\begin{definition}
Sei $(V,\Gamma)$ ein Vektorraum über dem Körper $\Gamma$. Der \textbf{Raum der gemischten p,q-multilinearen Abbildungen} ist definiert als
\begin{equation}
\mathfrak{Mult}_{p,q}(V,V^\sim,\Gamma) := \{ t : \underbrace{V \times \cdots \times V}_{p\text{-mal}} \times \underbrace{V^\sim \times \cdots \times V^\sim}_{q\text{-mal}} \to \Gamma \;\big|\; t \text{ linear in jedem Argument}\}
\end{equation}

Ein Element $t \in \mathfrak{Mult}_{p,q}(V,\Gamma)$ bildet somit $v_1, ..., v_p \in V, \ \varphi_1, ..., \varphi_q \in V^\sim$ folgendermaßen ab:
\begin{equation*}
(v_1,\dots,v_p,\varphi_1,\dots,\varphi_q) \mapsto t(v_1,\dots,v_p,\varphi_1,\dots,\varphi_q) \in \Gamma
\end{equation*}
\end{definition}

Es kann wieder gezeigt werden, dass $\mathfrak{Mult}_{p,q}(V,V^\sim,\Gamma)$ mit komponentenweiser Addition und Skalarmultiplikation ein Vektorraum über $\Gamma$ ist.

Sei $(V,\Gamma)$ ein endlichdimensionaler Vektorraum mit Dualraum $V^\sim$. Wir haben zuvor gezeigt, dass dann 
\begin{equation*}
\mathfrak{Mult_p}(V,\Gamma) = \bigotimes\nolimits^p V^{\sim} \ \text{ und } \ \mathfrak{Mult_q}(V^\sim,\Gamma) = \bigotimes\nolimits^q V^{\sim\sim} \overset{V=V^{\sim\sim}}{=} \bigotimes\nolimits^q V
\end{equation*}

Damit folgt 
\begin{equation*}
\mathfrak{Mult_{p,q}}(V,V^\sim,\Gamma) = \bigotimes\nolimits^p V^{\sim} \otimes \bigotimes\nolimits^q V =: \bigotimes\nolimits^q_p V
\end{equation*}

Elemente $t \in \bigotimes\nolimits^q_p V$ heißen \textbf{(q,p)-Tensoren} oder \textbf{Tensoren vom Typ (q,p)}. Also sind Tensoren nach dieser Definition (q,p)-multilineare Funktionen.

Für
\begin{equation}
\mathfrak{Mult}_{p,q}(V^\sim,V,\Gamma) := \{ t : \underbrace{V^\sim \times \cdots \times V^\sim}_{p\text{-mal}} \times \underbrace{V \times \cdots \times V}_{q\text{-mal}}   \to \Gamma \;\big|\; t \text{ linear in jedem Argument}\}
\end{equation}

gilt auch
\begin{equation}\label{eq:tensordef}
\mathfrak{Mult_{p,q}}(V^\sim,V,\Gamma) = \bigotimes\nolimits^p V \otimes \bigotimes\nolimits^q V^\sim = \bigotimes\nolimits^p_q V
\end{equation}
Entsprechend heißen Elemente $t \in \bigotimes\nolimits^p_q V$ \textbf{(p,q)-Tensoren} oder \textbf{Tensoren vom Typ (p,q)}. \textit{Diese} Definition werden wir ab jetzt verwenden.
\vspace{1em}

\begin{tcolorbox}[noVO]
\begin{remark}[Nicht Teil der VO]
In der Literatur kann auch die folgende Definition für \textbf{(p,q)-Tensoren} gefunden werden.

{\footnotesize Quelle: \url{https://en.wikipedia.org/wiki/Tensor_(intrinsic_definition)#Definition_via_tensor_products_of_vector_spaces}}

\begin{equation}
T^p_q(V) := \underbrace{V \otimes \cdots \otimes V}_{p\text{-mal}} \otimes \underbrace{V^\sim \otimes \cdots \otimes V^\sim}_{q\text{-mal}} = \bigotimes\nolimits^p V \otimes \bigotimes\nolimits^q V^{\sim} = \mathfrak{Mult_{p,q}}(V^\sim,V,\Gamma)
\end{equation}

Da für endlichdimensionale Vektorräume $V, W$ gilt: $V\otimes W \cong_\text{kanonisch} W \otimes V$, stimmen dann die beiden Sichtweisen überein:
\begin{equation*}
\mathfrak{Mult_{p,q}}(V^\sim,V,\Gamma) = \bigotimes\nolimits^p V \otimes \bigotimes\nolimits^q V^{\sim} \cong_\text{kanonisch} \bigotimes\nolimits^q V^{\sim} \otimes \bigotimes\nolimits^p V = \mathfrak{Mult_{q,p}}(V,V^\sim,\Gamma)
\end{equation*}
\end{remark}
\end{tcolorbox}

\begin{remark}
Für $t \in {\color{blue}\mathfrak{Mult_{p,q}}(V^\sim,V,\Gamma) } = {\color[rgb]{0.08,0.71,0.47}\bigotimes\nolimits^p V \otimes \bigotimes\nolimits^q V^\sim = \bigotimes\nolimits^p_q V}$ gilt: 
\begin{align*}
& {\color{blue}t(\underbrace{\varphi_1, \dots, \varphi_p}_{\displaystyle \in V^\sim}, \underbrace{v_1, \dots, v_q}_{\displaystyle \in V}) } \in \Gamma \\
&t = {\color[rgb]{0.08,0.71,0.47}t^{a_1 \dots a_p}_{b_1 \dots b_q} \underbrace{E_{a_1} \otimes \cdots \otimes E_{a_p}}_{\displaystyle \in V} \otimes \underbrace{e^{b_1}\otimes \cdots \otimes e^{b_q}}_{\displaystyle \in V^\sim} }
\end{align*}

\begin{itemize}
    \item In der Darstellung $t(\varphi_1, \dots, \varphi_p, v_1, \dots, v_q)$ nimmt $t$ also p \textit{Argumente} aus dem Dualraum und q \textit{Argumente} aus dem Vektorraum.
    \item In der Darstellung $t= t^{a_1 \dots a_p}_{b_1 \dots b_q} E_{a_1} \otimes \cdots \otimes E_{a_p} \otimes e^{b_1}\otimes \cdots \otimes e^{b_q}$ besteht $t$ aus p \textit{Faktoren} aus dem Vektorraum und q \textit{Faktoren} aus dem Dualraum.
    \item $p$ wird \textbf{kontravarianter Grad} genannt (Anzahl der Faktoren aus dem Vektorraum $V$). Vektoren $\in V$ tragen obere Indizes: $v = v^\alpha E_\alpha$.
    \item $q$ wird \textbf{kovarianter Grad} genannt (Anzahl der Faktoren aus dem Dualraum $V^\sim$). Kovektoren $\in V^\sim$ tragen untere Indizes: $\varphi = \varphi_\alpha e^\alpha$. 
\end{itemize}
\end{remark}


\begin{tcolorbox}[noVO]
\begin{example}[Nicht Teil der VO] \breakafterhead

{\footnotesize Quelle: \url{https://en.wikipedia.org/wiki/Tensor#Examples}}

Je nach Definition sind untenstehende Gleichheitszeichen mengentheoretisch oder bzgl. eines kanonischen Isomorphismus zu verstehen.
\begin{itemize}
    \item $\bigotimes\nolimits^0_0 V = \Gamma$ ... Tensor 0. Stufe (Skalar)
    \item $\bigotimes\nolimits^1_0 V = V$ ... Tensor 1. Stufe (Vektor)
    \item $\bigotimes\nolimits^0_1 V = V^\sim$ ... Tensor 1. Stufe (Kovektor/lineares Funktional/Linearform) 
    \item $\bigotimes\nolimits^1_1 V = V \otimes V^\sim \cong V^\sim \otimes V = L(V,V)$ ... Tensor 2. Stufe (Matrix)
    \item $\bigotimes\nolimits^2_0 V = V \otimes V = \mathfrak{Bil}(V^\sim,\Gamma) = \mathfrak{Mult}_{2,0}(V^\sim,V,\Gamma)$ ... Tensor 2. Stufe (Matrix)
    \item $\bigotimes\nolimits^0_2 V = V^\sim \otimes V^\sim = \mathfrak{Bil}(V,\Gamma) = \mathfrak{Mult}_{0,2}(V^\sim,V,\Gamma)$ ... Tensor 2. Stufe (Matrix)
    \item $\bigotimes\nolimits^2_1 V = V \otimes V \otimes V^\sim \cong V^\sim \otimes V \otimes V = \mathfrak{Mult}_{2,1}(V^\sim,V,\Gamma)$ ... Tensor 3. Stufe ("3D Matrix")
    \item $\bigotimes\nolimits^1_2 V = V \otimes V^\sim \otimes V^\sim \cong V^\sim \otimes V^\sim \otimes V = \mathfrak{Mult}_{1,2}(V^\sim,V,\Gamma)$ ... Tensor 3. Stufe ("3D Matrix")
    \item $\bigotimes\nolimits^2_2 V = V \otimes V \otimes V^\sim \otimes V^\sim \cong V^\sim \otimes V^\sim \otimes V \otimes V = \mathfrak{Mult}_{2,2}(V^\sim,V,\Gamma)$ ... Tensor 4. Stufe ("4D Matrix")
\end{itemize}
\end{example}
\end{tcolorbox}
\vspace{1em}


Sei $V$ ein endlichdimensionaler Vektorraum mit Basis $(E_\beta)_{\beta\in I}$ und dualer Basis $(e^\alpha)_{\alpha\in I}$. Sei ein Tensor $t \in \mathfrak{Mult_{p,q}}(V^\sim,V,\Gamma) = \bigotimes\nolimits^p V \otimes \bigotimes\nolimits^q V^\sim = \bigotimes\nolimits^p_q V$.

Jeder Vektor $v \in V$ und jeder Kovektor $\varphi \in V^\sim$ lässt sich bezüglich dieser Basen schreiben als
\begin{equation*}
v = v^\beta E_\beta, \ \varphi = \varphi_\alpha e^\alpha,
\ \text{ wobei } \ \delta^\alpha_\beta = e^\alpha(E_\beta)
\end{equation*}

Die Komponenten von $t$ werden dann durch die Basisvektoren bestimmt
\begin{equation}\label{eq:komponenten}
t(e^{\alpha_1},\ldots,e^{\alpha_p}, E_{\beta_1},\ldots,E_{\beta_q}) = t^{\alpha_1\ldots\alpha_p}_{\beta_1\ldots\beta_q}\in\Gamma
\end{equation}

Für beliebige Vektoren $v_1,\ldots,v_q\in V$ und Kovektoren $\varphi_1,\ldots,\varphi_p\in V^\sim$ gilt dann 
\begin{equation*}
\begin{aligned}
&t(\varphi_1,\ldots,\varphi_p, v_1,\ldots,v_q) = t(\varphi_{1,\alpha_1}e^{\alpha_1}, \ldots, \varphi_{p,\alpha_p}e^{\alpha_p}, v_1^{\beta_1}E_{\beta_1}, \ldots, v_q^{\beta_q}E_{\beta_q}) \\
&\overset{\substack{\text{Skalar} \\ \text{mult.}}}{=} t(e^{\alpha_1},\ldots,e^{\alpha_p}, E_{\beta_1},\ldots,E_{\beta_q})\,\varphi_{1,\alpha_1}\cdots \varphi_{p,\alpha_p}v_1^{\beta_1}\cdots v_q^{\beta_q} \\
&\overset{\eqref{eq:komponenten}}{=} \underbrace{t^{\alpha_1\ldots\alpha_p}_{\beta_1\ldots\beta_q}}_{\text{Koeffizienten}}\,\varphi_{1,\alpha_1}\cdots \varphi_{p,\alpha_p}v_1^{\beta_1}\cdots v_q^{\beta_q}
\end{aligned}
\end{equation*}

Diese Gleichung beschreibt die Auswertung eines $(p,q)$-Tensors in Komponentenform.

\section{Basis- und Koordinatentransformationen}\label{sec:basistrafo}
Wir haben bisher mit einer festen Basis des Vektorraums bzw. Dualraums gearbeitet. Da Vektorräume mehrere Basen besitzen ist es oft notwendig, von einer Basis/Koordinatendarstellung in eine andere zu transformieren. 

Seien dazu $(E_\alpha)_{\alpha\in I}$ und $(\tilde{E}_\alpha)_{\alpha\in I}$ zwei Basen desselben Vektorraums $V$. Um von einer Basis zur anderen überzugehen, verwenden wir eine Basistransformation. Es soll gelten:
\begin{align}
\tilde{E}_\alpha = \Lambda^\beta_\alpha \, E_\beta  \label{eq:BasisTransfVorwaerts} \\
E_\beta = \Sigma^\gamma_\beta\, \tilde{E}_\gamma \label{eq:BasisTransfRueckwaerts}
\end{align}
wobei $\Lambda^\beta_\alpha$ und $\Sigma^\beta_\alpha$ invertierbare Transformationsmatrizen sind und $\alpha$ angegeben werden muss, um zu wissen welches Basiselement zerlegt wurde.

Setzen wir \eqref{eq:BasisTransfRueckwaerts} in \eqref{eq:BasisTransfVorwaerts} ein, so folgt
\begin{equation}\label{eq:basistrafo}
\tilde{E}_\alpha = \Lambda^\beta_\alpha\Sigma^\gamma_\beta\tilde{E}_\gamma
\end{equation}

Andererseits kann $\tilde{E}_\alpha$ geschrieben werden als 
\begin{equation}\label{eq:basistrafo1}
\tilde{E}_\alpha = \delta^\gamma_\alpha \tilde{E}_\gamma
\end{equation}

Da die Basisvektoren linear unabhängig sind folgt mit \cref{cor:eindeutig}, dass die Darstellung von $\tilde{E}_\alpha$ eindeutig ist und somit müssen die Matrizen aus \eqref{eq:basistrafo} und \eqref{eq:basistrafo1} übereinstimmen:
\begin{equation}\label{eq:invers}
\Lambda^\beta_\alpha \Sigma^\gamma_\beta = \delta^\gamma_\alpha
\end{equation}

Analog erhält man durch Einsetzen in umgekehrter Reihenfolge:
\begin{equation*}
\Sigma^\beta_\alpha \Lambda^\gamma_\beta = \delta^\gamma_\alpha
\end{equation*}
Die Matrizen $\Lambda^\beta_\alpha$ und $\Sigma^\beta_\alpha$ sind daher zueinander invers.

\begin{figure}[H]
\centering
\begin{tikzpicture}[>=Stealth, node distance=4cm]
  % Basen
  \node (E)  at (0,0)   {$(E_\alpha)_{\alpha\in I}$};
  \node (Et) at (4,0)   {$(\tilde{E}_\alpha)_{\alpha\in I}$};

  % Pfeile für Basistransformation
  \draw[->] (E)  to[bend left=20] node[above] {$\Lambda^\beta_\alpha$} (Et);
  \draw[->] (Et) to[bend left=20] node[below] {$\Sigma^\beta_\alpha$} (E);
\end{tikzpicture}
\caption{Basiswechsel im Vektorraum $V$}
\end{figure}

Betrachten wir nun die zugehörigen dualen Basen $(e^\alpha)_{\alpha\in I}$ und $(\tilde{e}^\alpha)_{\alpha\in I}$. Diese sind definiert durch
\begin{equation}\label{eq:dual}
e^\alpha(E_\beta)=\delta^\alpha_\beta,
\qquad
\tilde{e}^\alpha(\tilde{E}_\beta)=\delta^\alpha_\beta
\end{equation}

Für die Transformation zwischen den beiden dualen Basen gelte:
\begin{equation}\label{eq:Xi}
\tilde{e}^\beta = \Xi^\beta_\gamma e^{\gamma}
\end{equation}

Dann folgt:
\begin{equation*}
    \Lambda^\gamma_\alpha \Sigma^\beta_\gamma = \delta^\beta_\alpha 
    \overset{\substack{\text{duale} \\ \text{Basis}}}{=} \tilde{e}^\beta(\tilde{E}_\alpha)
    \overset{\eqref{eq:Xi}}{=} \Xi^\beta_\gamma e^{\gamma}(\tilde{E}_\alpha)
    \overset{\eqref{eq:BasisTransfVorwaerts}}{=} \Xi^\beta_\gamma e^\gamma(\underbrace{\Lambda^\delta_\alpha}_{\displaystyle \in \Gamma} E_\delta)
    \overset{\substack{\text{$e^\gamma$} \\ \text{linear}}}{=} \Xi^\beta_\gamma \Lambda^\delta_\alpha e^\gamma(E_\delta)
    \overset{\substack{\text{duale} \\ \text{Basis}}}{=} \Xi^\beta_\gamma \Lambda^\delta_\alpha \delta_\delta^\gamma
    = \Xi^\beta_\gamma \Lambda^\gamma_\alpha
\end{equation*}

und somit:
\begin{align*}
    & \Lambda^\gamma_\alpha \Sigma^\beta_\gamma = \Xi^\beta_\gamma \Lambda^\gamma_\alpha \\
    \Leftrightarrow \ &\Sigma^\alpha_\beta = \Xi^\alpha_\beta \ \text{ bzw. } \ \Xi^\alpha_\beta \cdot (\Sigma^\gamma_\alpha)^{-1} = \delta^\gamma_\alpha
\end{align*}

Die Erkenntnis ergibt wegen \eqref{eq:dual} durchaus Sinn.

Die duale Basis transformiert somit \emph{invers} zur Vektorbasis. Die Transformation, die im Vektorraum von der alten Basis 
in die neue führt, bringt uns im Dualraum von der neuen Basis wieder zurück in die alte. 

\textbf{Kovariant} (Kovektoren $\in V^\sim$): Die Komponenten transformieren gleich (ko = mit) der Basis (untere Indizes: $\varphi=\varphi_\alpha e^\alpha$). \newline
\textbf{Kontravariant} (Vektoren $\in V$): Die Komponenten transformieren entgegengesetzt zur Basis (obere Indizes: $v=v^\alpha E_\alpha$).

\begin{figure}[H]
\centering
\begin{tikzpicture}[>=Stealth, node distance=4cm]
  % Dualbasen
  \node (e)  at (0,0)   {$(e^\alpha)_{\alpha\in I}$};
  \node (et) at (4,0)   {$(\tilde{e}^{\,\alpha})_{\alpha\in I}$};

  % Pfeile für Dualbasistransformation
  \draw[->] (e)  to[bend left=20] node[above]
    {$\Xi^\beta_\alpha = \Sigma^\beta_\alpha$} (et);
  \draw[->] (et) to[bend left=20] node[below]
    {$(\Xi^\beta_\alpha)^{-1} = \Lambda^\beta_\alpha$} (e);
\end{tikzpicture}
\caption{Basiswechsel im Dualraum $V^\sim$}
\end{figure}

Zusammengefasst:\newline
Für 
\begin{equation*}
    \tilde{E}_\alpha = \Lambda^\beta_\alpha E_\beta
    \ \text{ und } \
    E_\alpha = \Sigma^\beta_\alpha \tilde{E}_\beta
    \ \text{ mit } \
    e^\alpha(E_\beta)=\delta^\alpha_\beta
    \ \text{ und } \
    \tilde{e}^\alpha(\tilde{E}_\beta)=\delta^\alpha_\beta
\end{equation*}
folgt
\begin{equation*}
\Lambda^\alpha_\gamma\Sigma^\gamma_\beta = \delta^\alpha_\beta, 
\quad
\tilde{e}^\alpha = \Sigma^\alpha_\beta e^\beta
\ \text{ und } \
e^\alpha = \Lambda^\alpha_\beta \tilde{e}^\beta
\end{equation*}


\subsection*{Transformation eines Tensors}
Wir betrachten nun einen Tensor
\begin{equation*}
t \in \bigotimes\nolimits^p_q V = \bigotimes\nolimits^p V \otimes \bigotimes\nolimits^q V^\sim
\end{equation*}

Bezüglich der Basis $(E_\alpha)_{a\in I}$ von $V$ und der dualen Basis $(e^\alpha)_{a\in I}$ lässt er sich schreiben als
\begin{equation}\label{eq:tensortrafostart}
t = \underbrace{t^{\alpha_1\ldots\alpha_p}_{\beta_1\ldots\beta_q}}_{\text{Koeffizienten}}
   E_{\alpha_1}\otimes\cdots\otimes E_{\alpha_p}
   \otimes 
   e^{\beta_1}\otimes\cdots\otimes e^{\beta_q}
\end{equation}

Da unsere Definition eines Tensors basisunabhängig ist, können wir denselben Tensor ebenso bezüglich einer beliebigen anderen Basis $(\tilde{E}_\alpha)_{a\in I}$ von $V$ und der zugehörige dualen Basis $(\tilde{e}^\alpha)_{a\in I}$ darstellen:
\begin{equation}\label{eq:tensortrafo}
t = \tilde{t}^{\mu_1\ldots\mu_p}_{\nu_1\ldots\nu_q}\;
   \tilde{E}_{\mu_1}\otimes\cdots\otimes \tilde{E}_{\mu_p}
   \otimes
   \tilde{e}^{\nu_1}\otimes\cdots\otimes \tilde{e}^{\nu_q}
\end{equation}

Transformiere nun von der neuen Basis zurück in die alte mit den zuvor gefundenen Transformationsgleichungen 
\begin{equation*}
\tilde{E}_\mu = \Lambda^\alpha_{\mu} E_\alpha,
\qquad
\tilde{e}^{\nu} = \Sigma^\nu_{\beta} e^\beta
\end{equation*}
Setze diese in \eqref{eq:tensortrafo} ein:
\begin{equation} \label{eq:tensortrafoende}
\begin{aligned}
t &= \tilde{t}^{\mu_1\ldots\mu_p}_{\nu_1\ldots\nu_q}\; \Lambda^{\alpha_1}_{\mu_1}E_{\alpha_1}\otimes\cdots \otimes \Lambda^{\alpha_q}_{\mu_q}E_{\alpha_q} \otimes \Sigma^{\nu_1}_{\beta_1}e^{\beta_1}\otimes\cdots\otimes\Sigma^{\nu_p}_{\beta_p}e^{\beta_p} \\
&\overset{\substack{\text{Tensorprod.} \\ \text{linear}}}{=} \tilde{t}^{\mu_1\ldots\mu_p}_{\nu_1\ldots\nu_q}\;
   \Lambda^{\alpha_1}_{\mu_1}\cdots\Lambda^{\alpha_p}_{\mu_p}
   \Sigma^{\nu_1}_{\beta_1}\cdots\Sigma^{\nu_q}_{\beta_q}
   \left(
   E_{\alpha_1}\otimes\cdots\otimes E_{\alpha_p}
   \otimes
   e^{\beta_1}\otimes\cdots\otimes e^{\beta_q}
   \right)
\end{aligned}
\end{equation}

Wir haben also mit \eqref{eq:tensortrafostart} und \eqref{eq:tensortrafoende} zwei Darstellungen vom Tensor $t$ in derselben Basis gefunden. Nach da die Vektoren der Basis linear unabhängig sind ist die Darstellung nach \cref{cor:eindeutig} eindeutig. Daher müssen die Koeffizienten übereinstimmen:
\begin{equation}\label{eq:TensorTransf}
t^{\alpha_1\ldots\alpha_p}_{\beta_1\ldots\beta_q}
=
\tilde{t}^{\mu_1\ldots\mu_p}_{\nu_1\ldots\nu_q}\;
\Lambda^{\alpha_1}_{\mu_1}\cdots\Lambda^{\alpha_p}_{\mu_p}
\Sigma^{\nu_1}_{\beta_1}\cdots\Sigma^{\nu_q}_{\beta_q}
\end{equation}

\eqref{eq:TensorTransf} wird \textbf{Tensor-Transformationsgesetz} genannt. Es beschreibt, wie sich die Komponenten eines $(p,q)$-Tensors beim Wechsel der Basis verändern. 

Es ist der Ursprung der \textquotedbl Münchhausen Definition\textquotedbl\ eines Tensors: Ein Objekt heißt genau dann Tensor, wenn es wie ein Tensor transformiert.

\section{Abstrakte Indexschreibweise}
Wie wir gesehen haben, hängen die Komponenten eines Tensors von der Wahl der Basis ab, der Tensor selbst jedoch nicht. Die Komponenten tragen dabei obere und untere Indizes, entsprechend ihrem kontravarianten bzw. kovarianten Grad. Um den Typ eines Tensors unabhängig von einer konkreten Basis sichtbar zu machen, verwenden wir die sogenannte \textbf{abstrakte Indexschreibweise} (Penrose-Notation).

Dabei werden Tensoren mit \emph{Symbolen} versehen, sogenannten abstrakten Indizes, die nicht für Zahlen stehen, sondern anzeigen, welchen ko- bzw. kontravarianten Rang der Tensor besitzt. Beispielsweise bedeutet
\begin{equation*}
t \in \bigotimes\nolimits_1^2 V = \bigotimes\nolimits^2 V \otimes \bigotimes\nolimits^1 V^\sim = \mathfrak{Mult}_{2,1}(V^\sim,V,\Gamma)
\end{equation*}
dass $t$ zwei kontravariante und einen kovarianten Index besitzt, also ein Tensor vom Typ $(2,1)$ ist. Dies lässt sich 
abstrakt schreiben als
\begin{equation*}
t = t^{ab}_c
\qquad a,b,c \in I
\end{equation*}
wobei $a$, $b$ und $c$ \textit{keine} Komponentenindizes sind, sondern lediglich den Typ/Stufe/Grad des Tensors angeben. 

Auch die Objekte, auf die ein Tensor wirkt, tragen abstrakte Indizes. Um zu kennzeichnen, an welchem Index des Tensors ein Argument wirkt, verwendet man für das Argument denselben abstrakten Index. 

\begin{example}[Vektor und Kovektor]
    Sei $v \in V = \bigotimes\nolimits^1_0 V$ und $\varphi \in V^\sim = \bigotimes\nolimits^0_1 V$. Dann ordnen wir zu
    \begin{align*}
    & v \rightarrow v^a \ \ \text{(kontravarianter abstrakten Index $a$)}\\
    & \varphi \rightarrow \varphi_a \ \ \text{(kovarianter abstrakten Index $a$)}
    \end{align*}
    
    und es gilt
    \begin{equation*}
    \varphi(v) = \varphi_a v^a = v^a \varphi_a = v(\varphi) \in \Gamma = \bigotimes\nolimits^0_0 V
    \end{equation*}
    Abbildung und Argument tragen \textit{denselben} abstrakten Index $a$, sodass klar ist, dass $\varphi$ auf $v$ wirkt. Dabei ist nur wichtig, dass sie denselben Index tragen. Da diese nur ein Symbol ist, kann dafür ein beliebiger gewählt werden: $\varphi_a v^a = \varphi_b v^b = \varphi_c v^c$.
\end{example}


\begin{example}[Tensorprodukt]
    Seien $\varphi, \psi \in V^\sim$. Dann ordnen wir $(\varphi \otimes \psi) \in \mathfrak{Bil}(V, \Gamma) = \bigotimes\nolimits^0_2 V$ zu
    \begin{equation*}
    (\varphi \otimes \psi) \rightarrow (\varphi \otimes \psi)_{ab} = \varphi_a \psi_b
    \end{equation*}
    Für die Auswertung von $(\varphi \otimes \psi)$ auf $v,w \in V$ gilt: $(\varphi \otimes \psi)(v,w)= \varphi(v) \cdot \psi(w) =\varphi_a v^a\psi_b w^b = \varphi_a\psi_b v^a w^b = \psi_b \varphi_a v^a w^b = \psi(w) \cdot \varphi(v)  $
    Also gilt $\varphi_a \psi_a = \psi_b \varphi_a$. Da aber $\varphi(v) \cdot \psi(w) \neq \psi(v) \cdot \varphi(w)$ ist somit $\varphi_a \psi_b \neq \psi_a \varphi_b$.
\end{example}


\begin{example}[Basis]
Die Basis $(E_\alpha)_{\alpha\in I}$ des Vektorraums $V$ kann in abstrakter Indexschreibweise als $E^a_\alpha$ notiert werden, wobei $a$ der abstrakte Index und $\alpha$ der Zählindex ist, über den summiert wird. Die duale Basis wird entsprechend als $e^\alpha{}_a$ notiert. Es gilt somit:
\begin{equation*}
e^\alpha(E_\beta) =e^\alpha_a E^a_\beta = E^a_\alpha e^\beta_a = \delta^\alpha_\beta
\end{equation*}

Die Zählindizes $\alpha, \beta$, über die summiert wird, sind verschieden, aber der abstrakte Index $a$ ist derselbe.
\end{example}

\begin{example}[\href{https://en.wikipedia.org/wiki/Metric\_tensor\#Definition}{Metrischer Tensor}] \breakafterhead

{\footnotesize Quelle: \url{https://en.wikipedia.org/wiki/Metric_tensor_(general_relativity)#Definition}}
\vspace{1em}

Ein \textbf{metrischer Tensor} (z.B. die Minkowski-Metrik) auf einem Vektorraum $V$ ist ein Tensor vom Typ $(0,2)$ (kovarianter Tensor zweiter Stufe), der symmetrisch und nicht-entartet ist. In unserer Notation schreiben wir
\begin{equation*}
g_{ab} \in \bigotimes\nolimits^0_2 V =\mathfrak{Bil}(V, \Gamma)
\end{equation*}

Die Darstellung eines metrischen Tensors ist bezüglich einer Basis $(E_\alpha)_{\alpha\in I}$ wie üblich eindeutig
\begin{equation*}
g_{ab} \, E^a_\alpha \, E^b_\beta = g_{\alpha\beta}
\end{equation*}

Für die Minkowski-Metrik gilt beispielsweise $\eta_{\alpha\beta} := \mathrm{diag}(-1,1,1,1)$.

Ein metrischer Tensor induziert die lineare Abbildung

\begin{equation*}
g_{ab}: V \to V^\sim: v^a \mapsto v_b := g_{ab}v^a
\end{equation*}

und dessen Inverse
\begin{equation*}
    g^{ab}: V^\sim \to V: v_b \mapsto v^a := g^{ab}v_b
\end{equation*}

Der metrischer Tensor identifiziert also ein Element aus dem Vektorraum $V$ mit einem aus dessen Dualraum $V^\sim$. Allerdings sind $v^a$ und $v_a$ nicht dasselbe Objekt! Obige Operationen werden \textit{Index-Transport} genannt.
\vspace{1em}

\begin{tcolorbox}[noVO]
\begin{remark}[Nicht Teil der VO] \breakafterhead

{\footnotesize Quelle: \url{https://en.wikipedia.org/wiki/Musical_isomorphism#Discussion}}

Da $g \in \mathfrak{Bil}(V, \Gamma)$ ist durch das Festhalten eines Vektors $v \in V$ die Abbildung 
\begin{equation*}
    g(v, \cdot): V \to \mathbb{R}
\end{equation*}

linear im zweiten Argument und somit $g(v, \cdot) \in V^\sim$. Der metrische Tensor induziert daher die lineare Abbildung \textit{flat}
\begin{equation*}
    \flat: V \to V^\sim: v^a \mapsto v^\flat := v_b := g(v, \cdot) = g_{ab}v^a
\end{equation*}

wobei für $v \in V^\sim, w \in V$ gilt $v^\flat(w) := v_b w^b := g(v, w) = g_{ab}v^a w^b$.

Wenn $V$ endlich dimensional ist, ist $\flat$ bijektiv und somit ein Isomorphismus. Dieser ist für $V$ zusammen mit der Bilinearform $g$ sogar ein \textit{kanonischer} Isomorphismus. Seine Inverse ist der \textit{sharp} Operator
\begin{equation*}
    \sharp: V^\sim \to V: v_b \mapsto v^\sharp := v^a := g^{-1}(v^\flat, \cdot) = g^{ab}v_b
\end{equation*}
wobei für $v \in V, \varphi \in V^\sim$ gilt $v^\sharp(\varphi) := v^a \varphi_a := g^{-1}(v^\flat, \varphi) = g^{ab}v_a \varphi_b$ und $g(v^\sharp, w)=v(w)$.

Da die beiden Operatoren $\flat$ und $\sharp$ aus der Musiknotation stammen, werden sie auch \emph{musikalische Isomorphismen} genannt. Das Symbol $\flat$ ("flat") bezeichnet das \emph{Absenken} eines Tons, während $\sharp$ (\textquotedbl sharp") ein \emph{Anheben} eines Tons bedeutet. Analog dazu senkt der Operator $\flat$ in der Tensorrechnung einen Index ab, und $\sharp$ hebt einen Index an.

\end{remark}
\end{tcolorbox}
\vspace{1em}

Die inverse Metrik $g^{ab}$ ist entsprechend ein Tensor vom Typ $(2,0)$,
\begin{equation*}
g^{ab} \in \bigotimes\nolimits^2_0 V =\mathfrak{Bil}(V^\sim, \Gamma)
\end{equation*}
und erfüllt 
\begin{equation*}
g^{ab} e_a{}^\alpha e_b{}^\beta = g^{\alpha\beta}
\end{equation*}
wobei $e^\beta{}_a$ die duale Basis bezeichnet. Metrik und inverse Metrik stehen in der Beziehung
\begin{equation*}
g^{ab}g_{bc} = g^{\alpha \beta} E^a_\alpha E^b_\beta g_{\gamma\delta}e^\gamma_be^\delta_c = g^{\alpha \beta} g_{\gamma\delta} E^a_\alpha \underbrace{E^b_\beta e^\gamma_b}_{= \delta^\gamma_\beta} e^\delta_c = g^{\alpha \beta} g_{\beta \delta} E^a _\alpha e^\delta_c = \delta^\alpha_\delta \delta^\alpha_c = \delta^a_c.
\end{equation*}

Die inverse Minkowski-Metrik ist ebenso $\eta^{\alpha \beta}=\mathrm{diag}(-1,1,1,1)$.

Hier wurde die Metrik noch nicht als Längenbegriff eingeführt.
\end{example}


\begin{tcolorbox}[noVO]
\begin{definition}[Nicht Teil der VO]
Sei $(V,\langle \cdot,\cdot\rangle)$ ein Skalarproduktraum, wobei das Skalarprodukt eine positiv definite symmetrische Bilinearform ist. Ein System von Vektoren $(E_\alpha)_{\alpha \in I} \subset V$ heißt \textbf{Orthonormalsystem} (ONS), wenn gilt
\begin{equation*}
\langle E_\alpha, E_\beta \rangle
=
\begin{cases}
1, & \text{falls } \alpha = \beta \\
0, & \text{falls } \alpha \neq \beta
\end{cases}
\quad = \delta^\alpha_\beta
\end{equation*}

Das heißt: alle Vektoren in $(E_\alpha)_{\alpha \in I}$ sind normiert und paarweise orthogonal.

Ein Orthonormalsystem $M = (E_\alpha)_{\alpha \in I}$ heißt \textbf{Orthonormalbasis} (ONB) $\Leftrightarrow \ \overline{\operatorname{span} M} = V$.
\end{definition}

\begin{definition}[Nicht Teil der VO]
Sei $(V,( \cdot,\cdot))$ ein Pseudo-Skalarproduktraum, wobei das Pseudo-Skalarprodukt eine indefinite symmetrische nicht-entartete Bilinearform ist. Ein System von Vektoren $(E_\alpha)_{\alpha \in I} \subset V$ heißt \textbf{Pseudo-Orthonormalsystem} (ONS), wenn gilt
\begin{equation*}
(E_\alpha, E_\beta )
=
\begin{cases}
\pm1, & \text{falls } \alpha = \beta \\
0, & \text{falls } \alpha \neq \beta
\end{cases}
\end{equation*}
\end{definition}

Sei $(V,g)$ ein endlichdimensionaler Vektorraum mit einem metrischen Tensor $g$ und $(E_\alpha)_{\alpha \in I} \subset V$ eine Pseudo-ONB. Dann kann $g$ damit dargestellt werden:
\begin{equation*}
    g(E_\alpha, E_\beta) = g_{ab}E^a_\alpha E^b_\beta = g_{\alpha\beta}
\end{equation*}
wobei $g_{\alpha\beta}$ die Komponenten der Metrik in dieser Pseudo-ONB bezeichnet. Obiges beschreibt die Transformation des metrischen Tensors durch eine Pseudo-ONB in eine Diagonalmatrix, da 

\begin{equation*}
g_{ab}E^a_\alpha E^b_\beta
=
\begin{cases}
\pm1, & \text{falls } \alpha = \beta \\
0, & \text{falls } \alpha \neq \beta
\end{cases}
\end{equation*}

Die Tatsache, dass jede nicht-ausgeartete symmetrische Bilinearform durch Wahl einer geeigneten Basis auf eine Diagonalform mit Einträgen $\pm 1$ gebracht werden kann, folgt aus dem \emph{Trägheitssatz von Sylvester}.
\end{tcolorbox}

\begin{example}[Orthonormalbasis bzgl. der Minkowski-Metrik]\label{ex:pseudoONB}
Betrachte den Minkowski-Raum $(\mathbb{M},\eta)$ mit $\eta_{ab} = \operatorname{diag}(-1,1,1,1)$ und eine Pseudo-ONB $(E_\alpha)_{\alpha=0}^3$. 
Dann gilt
\begin{equation*}
    \eta(E_\alpha, E_\beta)
    = \eta_{ab} E^a_\alpha E^b_\beta
    = \eta_{\alpha\beta}
\end{equation*}

Insbesondere:
\begin{equation*}
    \eta_{ab} E^a_0 E^b_0 = \eta_{00} = -1
\end{equation*}

Für die räumlichen Basisvektoren ($i=1,2,3$) gilt:
\begin{equation*}
\eta_{ab} E^a_1 E^b_1 = \eta_{11} = +1,
\qquad
\eta_{ab} E^a_2 E^b_2 = \eta_{22} = +1,
\qquad
\eta_{ab} E^a_3 E^b_3 = \eta_{33} = +1
\end{equation*}

\underline{\textit{Dualbasis}}

Aus
\begin{equation*}
\eta_{ab} E^a_\beta = E_{\beta b} \in V^\sim
\end{equation*}
folgt, dass es Koeffizienten $\lambda_\alpha$ gibt mit
\begin{equation*}
\eta_{ab} E^a_\beta
= E_{\beta b}
= \lambda_\alpha\, e^\alpha_b,
\qquad \beta = 0,1,2,3
\end{equation*}

Für den Zeitanteil ($\beta = 0$) gilt:
\begin{equation*}
-1 
= \eta_{ab} E^a_0 E^b_0
= \lambda_\alpha\, \underbrace{e^\alpha_a E^a_0}_{= \delta^\alpha_a}
= \lambda_0
\end{equation*}
somit
\begin{equation*}
e^0_a = - E_{0a}
\end{equation*}

Ganz analog für die räumlichen Komponenten:
\begin{equation*}
e^i_a = + E_{ia}, \qquad i = 1,2,3
\end{equation*}

\underline{\textit{Indextransport}}

Ein Vektor $v \in V$ mit
\begin{equation*}
    v = v^\alpha E_\alpha
\end{equation*}
wird mittels der Metrik zu einem Kovektor

\begin{equation*}
v_b = \eta_{ba} v^a = v_\beta e_b^\beta = \eta_{ab} v^\alpha E^\alpha_\beta
= v^\alpha \eta_{ab} E^a_\alpha = v^\alpha E_{\alpha b} = -v^0e^0_b + v^ie^i_b
\end{equation*}

Dies liefert den Zusammenhang zwischen Kovariant- und Kontravariantkomponenten:
\begin{align*}
\text{zeitartig:} \qquad & v_0 = -v^0 \\
\text{räumlich:}  \qquad & v_i = v^i \quad (i=1,2,3)
\end{align*}

Da Komponenten reine Zahlen sind, können sie gleichgesetzt werden, obwohl die Vektoren selbst in unterschiedlichen Räumen liegen.
\vspace{1em}

\underline{\textit{Kompatibilität der Basis- und Dualbasis-Komponenten}}

Aus
\begin{equation*}
    E_{\alpha b}
    = \eta_{ba} E^a_\alpha
    = \lambda_{\alpha\beta} e^\beta_b
\end{equation*}
folgt durch Multiplikation mit $E^b_\gamma$:
\begin{equation*}
E^b_\gamma E^a_\alpha \eta_{ba}
= \eta_{\gamma \alpha}
= \lambda_{\alpha\beta} \underbrace{e^\beta_b E^b_\gamma}_{= \delta^\beta_\gamma} = \lambda_{\alpha \gamma}
\end{equation*}

Damit folgt schließlich
\begin{equation*}
    E_{\alpha b} = \eta_{\alpha\beta} e^\beta_b
\end{equation*}

Die in den Dualraum transportierten Basisvektoren $E_{\alpha b}$ stimmen also mit den dualen Basisvektoren $e^\alpha_b$ überein, bis auf das charakteristische Minuszeichen im zeitartigen Anteil.
\end{example}

\subsubsection{Kontraktion/Tensorverjüngung}
Die Kontraktion eines Tensors reduziert dessen Stufe, indem jeweils ein kontravarianter und ein kovarianter Index "zusammengezogen" werden.
Für einen Tensor
\begin{equation*}
t \in \bigotimes\nolimits^{\,q}_{p} V
\end{equation*}
erhält man durch Kontraktion einen Tensor
\begin{equation*}
s \in \bigotimes\nolimits^{\,q-1}_{p-1} V
\end{equation*}

Sei also
\begin{equation*}
t(\varphi_1,\ldots,\varphi_p,\; v_1,\ldots,v_q) \in \Gamma ,
\end{equation*}
wobei $\varphi_i \in V^\sim$ und $v_j \in V$.
Die Kontraktion an der $k$-ten kovarianten und $l$-ten kontravarianten Stelle definiert man durch
\begin{equation*}
s(\varphi_1,\ldots,\varphi_{p-1},\, v_1,\ldots,v_{q-1}) := t(\varphi_1,\dots,\underbrace{e^\alpha}_{\text{k-te Stelle}, \ 1 \leq k \leq p},\dots,\varphi_p, \, v_1,\dots, \underbrace{E_\alpha}_{\text{l-te Stelle}, \ 1 \leq l \leq q},\dots,v_q)
\end{equation*}
Wir zeigen nun, dass dies unabhängig von der Wahl der Basis und somit wohldefiniert ist.

Betrachte einen Basiswechsel
\begin{equation*}
\tilde{E}_\alpha = \Lambda^\beta_{\alpha} E_\beta,
\qquad
\tilde{e}^{\,\alpha} = \Sigma^{\alpha}_{\beta} e^{\,\beta},
\end{equation*}
mit inversen Transformationsmatrizen
\begin{equation*}
\Sigma^{\alpha}_{\beta}\, \Lambda^{\beta}_{\gamma} = \delta^\alpha_{\gamma}.
\end{equation*}

Dann gilt:
\begin{align*}
t(\varphi_1,\dots,\tilde{e}^\alpha,\dots,\varphi_{p-1},\tilde{E}_\alpha,\dots,v_{q-1})
&= t(\varphi_1,\dots,\Sigma^\alpha_\beta e^\beta,\dots,\varphi_{p-1}, v_1, \dots, \Lambda^\beta_\alpha E_\beta,\dots,v_{q-1} ) \\
&= \Sigma^\alpha_\beta\Lambda^\gamma_\alpha \, t(\varphi_1,\dots,e^\beta,\dots,\varphi_{p-1}, v_1, \dots, E_\gamma,\dots,v_{q-1}) \\
&= \delta^\gamma_\beta\, t(\varphi_1,\dots,e^\beta,\dots,\varphi_{p-1}, v_1, \dots, E_\gamma,\dots,v_{q-1})
\end{align*}

Damit ist die Kontraktion basisunabhängig.

\subsubsection*{Kontraktion in abstrakter Indexschreibweise}

In abstrakten Indizes lässt sich Kontraktion besonders einfach formulieren.
Für einen Tensor
\begin{equation*}
t^{a_1 \dots a_p}_{b_1 \dots b_q} \in \bigotimes\nolimits^{p}_{q} V
\end{equation*}
ergibt sich folgende Rechenvorschrift für die Kontraktion des $k$-ten kontravarianten und des $l$-ten kovarianten Index:
\begin{equation*}
t^{a_1 \dots c \dots a_p}_{b_1 \dots c \dots b_q}
\;:=\;
t^{a_1 \dots a_k \dots a_p}_{b_1 \dots b_l \dots b_q} \, \underbrace{e^\alpha_{\ a_k}\, E^{b_l}_{\ \alpha}}_{= \delta^{b_l}_{a_k}}
= S^{a_1 \dots a_{k-1},\, a_{k+1} \dots a_p}_{b_1 \dots b_{l-1},\, b_{l+1} \dots b_q}
\;\in\;
\bigotimes\nolimits^{\,p-1}_{\,q-1} V 
\end{equation*}

Die Basisvektoren bzw.\ Dualbasisvektoren schreiben wir als
\begin{equation*}
E^a_{\alpha} \in \bigotimes\nolimits^1_0 V,
\qquad
e^{\alpha}_{a} \in \bigotimes\nolimits^0_1 V,
\end{equation*}
mit
\begin{equation*}
E^a_{\alpha} \, e^{\beta}_{a} = \delta^\beta_{\alpha} \in \bigotimes\nolimits^1_1 V
\end{equation*}

Für einen Vektor $v \in V$ gilt
\begin{equation*}
v^a = v^\alpha E^a_{\alpha}.
\end{equation*}

Ebenso für die Wirkung eines Kovektors $e^\beta$ auf einen Vektor $v$:
\begin{equation*}
e^{\beta}(v)
= e^{\beta}_{a} v^a
= e^{\beta}_{a} E^a_{\alpha} v^\alpha
= \delta^\beta_{\alpha} v^\alpha
= v^\beta
\end{equation*}

Schließlich folgt für die kombinierte Wirkung eines Basisvektors und eines Dualvektors:
\begin{equation*}
(E^b_{\alpha} e^{a}_{\beta})\, v^b
= E^b_{\alpha} (e^{a}_{\beta} v_b)
= v^\gamma \delta^b_{\gamma} \delta^{a}_{\beta}
= v^a,
\end{equation*}
also insbesondere
\begin{equation*}
E^b_{\alpha} \, e^{a}_{b} = \delta^a_{\alpha}.
\end{equation*}

Damit ist die abstrakte Indexnotation vollständig konsistent mit der ursprünglichen Definition der Kontraktion.
\vspace{1em}

\begin{tcolorbox}[noVO]
\begin{remark}[Nicht Teil der VO] \breakafterhead

Ein Beispiel für Kontraktion ist die gewöhnliche Matrix–Vektor–Multiplikation.
Eine Matrix $A$ lässt sich als Tensor vom Typ $(1,1),\ (0,2)$ oder $(2,0)$ auffassen. Wähle für dieses Beispiel $(1,1)$:
\begin{equation*}
A^a_b \in \bigotimes\nolimits^1_1 V
\end{equation*}
Ein Vektor $v$ besitzt die Darstellung
\begin{equation*}
v^b \in \bigotimes\nolimits^1_0 V
\end{equation*}

Die Matrix-Vektor Multiplikation ist die Wirkung der Matrix auf den Vektor durch Kontraktion über den Index $b$:
\begin{equation*}
A^a_b \, v^b = w^a\in \bigotimes\nolimits^1_0 V .
\end{equation*} 
Der kontrahierte Index verschwindet, sodass das Ergebnis wieder ein Vektor ist. Die Kontraktion reduziert den Tensorgrad von $(1,1)$ und $(1,0)$ zu $(1,0)$.
\end{remark}
\end{tcolorbox}


\chapter{Topologie}\label{chap:Topologie}
\section{Der Begriff der Topologie}

Die allgemeine Relativitätstheorie findet in gekrümmten Räumen, sogenannten Mannigfaltigkeiten, statt. Daher ist eine Verallgemeinerung des Begriffs der Geraden erforderlich. Dabei lösen wir uns von der Struktur des $\mathbb{R}^n$, insbesondere von seiner linearen (Vektorraum-) und seiner Metrikstruktur. Begriffe wie Konvergenz und Stetigkeit müssen deshalb unabhängig von einem Abstandsbegriff neu formuliert werden. Dies führt uns zum Begriff der Topologie.

Die Topologie bildet die Grundlage aller weiteren Strukturen: Differenzierbarkeit, Metrik, Krümmung und schließlich die Einsteinschen Feldgleichungen.
\vspace{1em}

\begin{tcolorbox}[noVO]
\begin{remark}[Mögliche Prüfungsfrage] \breakafterhead

Wozu brauchen wir Topologie? \newline
Antwort: siehe oben
\end{remark}
\end{tcolorbox}

\begin{definition}
Die \textbf{Potenzmenge} $\mathcal{P}(X)$ einer Menge $X$ ist die Menge aller Teilmengen von $X$:
\begin{equation*}
    \mathcal{P}(X) := \{ U \mid U \subseteq X \}.
\end{equation*}
\end{definition}

\begin{definition}
Sei $X \neq \emptyset$ eine Menge (genannt Grundmenge). Eine Teilmenge $\mathfrak{X} \subseteq \mathcal{P}(X)$ heißt \textbf{Topologie} auf $X$,
wenn folgende Eigenschaften gelten:
\begin{align*}
\text{(O1)} &\quad \emptyset, X \in \mathfrak{X} \\[0.4em]
\text{(O2)} &\quad \mathcal{O}_1, \mathcal{O}_2 \in \mathfrak{X}:\ \mathcal{O}_1 \cap \mathcal{O}_2 \in \mathfrak{X}
\qquad \text{(stabil unter endlichen Schnitten)} \\[0.4em]
\text{(O3)} &\quad (\mathcal{O}_\alpha)_{\alpha \in I} \subseteq \mathfrak{X}:\ 
\bigcup_{\alpha \in I} \mathcal{O}_\alpha \in \mathfrak{X}
\qquad \text{(stabil unter beliebigen Vereinigungen)}
\end{align*}

Die Elemente von $\mathfrak{X}$ heißen \textbf{offene Mengen}. Daher wird $\mathfrak{X}$ auch ein \textit{System offener Mengen} genannt. Das Paar $(X,\mathfrak{X})$ heißt \textbf{topologischer Raum}. Das Zeichen $\mathfrak{X}$ wird "Kurrent X"\ ausgesprochen. 
\end{definition}

\begin{tcolorbox}[noVO]
\begin{remark}[Nicht Teil der VO] (O2) ist äquivalent zu:
    \begin{equation*}
        \text{(O2)'} \quad \mathcal{O}_1, \dots, \mathcal{O}_n \in \mathfrak{X}:\ \bigcap_{i=1}^n\mathcal{O}_i \in \mathfrak{X}
    \end{equation*}
\end{remark}
\end{tcolorbox}
\vspace{1em}

\begin{tcolorbox}[noVO]
\begin{remark}[Prüfungsfrage] \breakafterhead

Wie stelle ich fest, ob eine Menge offen ist? \newline
Antwort: Ich muss mich zuerst fragen, bezüglich welcher Topologie ich die Offenheit prüfen muss. Anschließend prüfe ich, ob die gegebene Menge in der Topologie enthalten ist.
\end{remark}
\end{tcolorbox}

\begin{example}
Sei $X$ eine beliebige Grundmenge und $\mathfrak{Y} = \{ \emptyset, X \}$ die sogenannte \textbf{indiskrete Topologie/Klumpentopologie}, wobei $\mathfrak{Y}$ "Kurrent Y"\ ausgesprochen wird. Dann ist $(X, \mathfrak{Y})$ der indiskrete topologische Raum. Dies ist die kleinstmögliche Topologie auf $X$.

Wir überprüfen, ob es sich wirklich um eine Topologie handelt:
\begin{itemize}
    \item[(O1)] zz: $\emptyset, X \in \mathfrak{Y}$ \newline
    Ist offensichtlich erfüllt, da $\mathfrak{Y}$ genau aus diesen Mengen besteht.
    \item[(O2)] zz: $\mathcal{O}_1, \mathcal{O}_2 \in \mathfrak{Y}:\ \mathcal{O}_1 \cap \mathcal{O}_2 \in \mathfrak{Y}$ \newline
    Die offenen Mengen von $\mathfrak{Y}$ sind $\emptyset$ und $X$ und es gilt
    \begin{equation*}
        \emptyset, X \in \mathfrak{Y}: \ \emptyset \cap X = \emptyset \in \mathfrak{Y}
    \end{equation*}
    \item[(O3)] zz: $(\mathcal{O}_\alpha)_{\alpha \in I} \subseteq \mathfrak{Y}:\ \bigcup_{\alpha \in I} \mathcal{O}_\alpha \in \mathfrak{Y}$
    Beliebige Vereinigungen aus $\{\emptyset, X\}$ \newline
    Die offenen Mengen von $\mathfrak{Y}$ sind $\emptyset$ und $X$ und es gilt
    \begin{equation*}
        \emptyset, X \in \mathfrak{Y}: \ \emptyset \cup X = X \in \mathfrak{Y}
    \end{equation*}
\end{itemize}

Damit ist $\mathfrak{Y}$ eine Topologie.
\end{example}

\begin{example} Sei $X$ eine beliebige Grundmenge und $\Theta = \mathcal{P}(X)$ die sogenannte \textbf{diskrete Topologie}. Dann ist $(X, \Theta)$ der diskrete topologische Raum. Dies ist die größtmögliche Topologie auf $X$.

Wir überprüfen wieder, ob es sich wirklich um eine Topologie handelt:
\begin{itemize}
    \item[(O1)] zz: $\emptyset, X \in \Theta$ \newline
    Ist erfüllt, da $\emptyset, X \in \mathcal{P}(X)$.
    \item[(O2)] zz: $\mathcal{O}_1, \mathcal{O}_2 \in \Theta:\ \mathcal{O}_1 \cap \mathcal{O}_2 \in \Theta$ \newline
    Die offenen Mengen von $\Theta = \mathcal{P}(X)$ sind die Teilmengen von $X$. Der Schnitt zweier Teilmengen von $X$ ist immer wieder Teilmenge von $X$ und liegt somit in $\Theta$.
    \item[(O3)] zz: $(\mathcal{O}_\alpha)_{\alpha \in I} \subseteq \Theta:\ \bigcup_{\alpha \in I} \mathcal{O}_\alpha \in \Theta$ \newline
    Auch beliebige Vereinigungen von Teilmengen von $X$ sind wieder eine Teilmenge von $X$ und liegen somit in $\Theta$.
\end{itemize}

Damit ist $\Theta$ eine Topologie.
\end{example}

\begin{definition}
Seien $\tau$ und $\sigma$ Topologien auf einer Menge $X$.

Gilt $\sigma \subseteq \tau$, so heißt $\tau$ \textbf{feiner} als $\sigma$ (bzw.\ $\sigma$ \textbf{gröber} als $\tau$). \newline
\end{definition}

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{Grob_fein.jpg}
\caption{$\sigma \subseteq \tau$; feiner: enthält mehr offene Mengen; gröber: enthält weniger offene Mengen}
\end{figure}

Damit ist die diskrete Topologie die feinstmögliche Topologie auf einer Menge, während die indiskrete Topologie die gröbstmögliche Topologie auf einer Menge ist. Für beliebige Topologien $\mathfrak{X}$ gilt also $\mathfrak{Y} \subseteq \mathfrak{X} \subseteq \Theta$.
\vspace{1em}

\begin{tcolorbox}[noVO]
\begin{remark}[Mögliche Prüfungsfrage] \breakafterhead

Gibt es Topologien überhaupt? \newline
Antwort: Ja: Beispiele sind die diskrete und indiskrete Topologie.
\end{remark}
\end{tcolorbox}
\vspace{1em}

\begin{tcolorbox}[noVO]
\begin{example}[Nicht Teil der VO] \breakafterhead

Betrachte $X = \{1,2\}$ und $\mathfrak{X} = \{\emptyset, \{1\}, \{1,2\}\} \subset \mathcal{P}(X)=\{\emptyset, \{1\}, \{2\}, \{1,2\}\}$
Dann gilt:
\begin{align*}
\text{(O1)}\ & \emptyset, X \in \mathfrak{X} \\
\text{(O2)}\ & \emptyset \cap \{1\} = \emptyset \in \mathfrak{X}, \\
             & \emptyset \cap \{1,2\}\ = \emptyset \in \mathfrak{X}, \\
             & \{1\} \cap \{1,2\} = \{1\} \in \mathfrak{X} \\
\text{(O3)}\ & \emptyset \cup \{1\} = \{1\} \in \mathfrak{X}, \\
             & \emptyset \cup \{1,2\} = \{1,2\} \in \mathfrak{X}, \\
             & \{1\} \cup \{1,2\} = \{1,2\} \in \mathfrak{X}
\end{align*}
Damit ist $\mathfrak{X}$ tatsächlich eine Topologie auf $X$. Hier wäre die indiskrete Topologie $\mathfrak{Y}=\{\emptyset, \{1,2\}\}$ und die diskrete Topologie $\Theta=\{\emptyset, \{1\}, \{2\}, \{1,2\}\}$. Man sieht somit gut, dass $\mathfrak{Y} \subseteq \mathfrak{X} \subseteq \Theta$ gilt.
\end{example}
\end{tcolorbox}

\section{Konvergenz einer Folge}

Erst durch die Ausstattung einer Menge mit einer Topologie wird es sinnvoll, über die Konvergenz von Folgen zu sprechen. Ohne eine solche zusätzliche Struktur gibt es keinen Begriff von "Nähe" und damit auch keinen Grenzwert.

\begin{definition}
Sei $(X,\mathfrak{X})$ ein topologischer Raum und $(x_n)_{n\in\mathbb{N}}$ eine Folge in $X$. Die Folge $(x_n)$ heißt \textbf{konvergent gegen $x\in X$} (bezüglich der Topologie $\mathfrak{X}$), in Zeichen $x_n \overset{\mathfrak{X}}{\to} x$, falls gilt:
\begin{equation*}
\underbrace{\forall \mathcal{O} \in \mathfrak{X}:\ x \in \mathcal{O} }_{\text{für alle offenen Umgebungen von }x}
\underbrace{\exists N_\mathcal{O} \in \mathbb{N} }_{\text{existiert ein Index }N_\mathcal{O}}
\underbrace{\forall n \ge N_\mathcal{O} \text{ sodass } x_n \in \mathcal{O}}_{\text{ab dem alle Folgenglieder }x_n\text{ in }\mathcal{O} \text{ liegen }}
\end{equation*}

Das bedeutet: Eine Folge konvergiert gegen $x$, wenn sich ihre Folgenglieder ab einem gewissen Index, beliebig nahe bei $x$ befinden. In der Topologie wird diese Nähe nicht über einen Abstand, sondern über offene Mengen beschrieben. Wir schreiben $N_\mathcal{O}$, da der Index von der offenen Menge $\mathcal{O}$ abhängt.
\end{definition}

\begin{figure}[H]
\centering
\includegraphics[width=0.3\textwidth]{Konvergieren.jpg}
\caption{Konvergenz einer Folge $(x_n)_{n\in\mathbb{N}}$ gegen ein $x$.}
\end{figure}

\vspace{1em}

Diese Definition zeigt bereits, dass Konvergenz stark von der vorliegenden Topologie abhängt. Insbesondere gilt:

\begin{itemize}
    \item Je \emph{gröber} eine Topologie ist (weniger offene Mengen), desto weniger offene Mengen müssen die Konvergenzbedingung erfüllen, desto \emph{schwächer} ist die Konvergenz.
    \item Je \emph{feiner} eine Topologie ist (mehr offene Mengen), desto mehr offene Mengen müssen die Konvergenzbedingung erfüllen, 
    desto \emph{stärker} ist die Konvergenz.
\end{itemize}

\begin{example}
Sei $(X,\mathfrak{Y})$ ein topologischer Raum mit der indiskreten Topologie $\mathfrak{Y}=\{\emptyset,X\}$.
Sei $\tilde{x} \in X$ und $(x_n)_{n\in\mathbb{N}} \subset X$ eine bel. Folge. Wann konvergiert sie gegen $\tilde{x}$? Die einzige in $\mathfrak{Y}$ offene Menge, die $\tilde{x}$ enthält, ist $X$. Also muss $(x_n)_{n\in\mathbb{N}}$ die folgende Bedingung erfüllen: 
\begin{equation*}
\text{für } X \in \mathfrak{Y} \
\exists N_X \in \mathbb{N} \
\forall n \ge N_X \text{ sodass } x_n \in X
\end{equation*}

Da aber die Folge bereits ganz in $X$ liegt, tritt dies schon ab dem 1. Folgenglied ein. So ein Index existiert also und somit konvergiert die Folge gegen $\tilde{x}$. Da die Folge beliebig gewählt war, konvergieren also alle Folgen aus $X$ gegen $\tilde{x}$. Dasselbe kann man nun für alle $x \in X$ durchspielen. Es folgt: alle Folgen in $X$ konvergieren gegen alle $x \in X$. Der Grenzwert der Folgen ist nicht eindeutig. Die Konvergenz bezüglich der indiskreten Topologie besitzt somit nur sehr geringe Aussagekraft.

Jede Folge  $(x_n)_{n\in\mathbb{N}} \subset X$ muss hier nur \underline{eine} Bedingung erfüllen: die Folgenglieder müssen ab einem Index $N_X$ in $X$ liegen.
\end{example}

\begin{example}
Sei $(X,\Theta)$ ein topologischer Raum mit der diskreten Topologie $\Theta=\mathcal{P}(X)$. Dann ist jede Teilmenge von $X$ offen, insbesondere auch die einelementige Menge $\{x\}$.

Für Folge $(x_n)_{n\in\mathbb{N}} \subset X$, die gegen $x$ konvergiert, muss daher gelten:
\begin{equation*}
\exists N_{\{x\}} \in \mathbb{N} \ \forall n \ge N_{\{x\}} \text{ sodass } x_n \in \{x\}
\end{equation*}
also
\begin{equation*}
x_n = x \quad \text{für alle } n \ge N_{\{x\}}
\end{equation*}

Eine Folge ist bezüglich der diskreten Topologie also genau dann konvergent, wenn sie ab einem gewissen Index konstant ist. Die Konvergenz ist hier eindeutig, aber sehr restriktiv: Nur sehr wenige Folgen konvergieren.

Es sei angemerkt, dass andere offene Mengen $\mathcal{O} \in \Theta$ keine weiteren Einschränkungen sind, denn für $x \in \mathcal{O} \in \Theta$ gilt bereits $\{x\} \subseteq \mathcal{O}$ und $x_n \in \{x\} \subseteq \mathcal{O} \ \forall n \ge N_{\{x\}}$.
\end{example}

Diese beiden Beispiele verdeutlichen den Zusammenhang zwischen Feinheit einer Topologie und Konvergenz: Je feiner die Topologie (je mehr offene Mengen), desto weniger Folgen konvergieren.

\begin{remark}
Seien $\tau_1$ und $\tau_2$ Topologien auf $X$ mit $\tau_1 \subseteq \tau_2$.
Dann gilt: \newline
Konvergiert eine Folge bezüglich $\tau_2$ so konvergiert sie auch bezüglich $\tau_1$.

Grund: wenn eine Bedingung (offene Menge) weggelassen wird, wird die Folge natürlich weiterhin konvergieren.
\end{remark}


\section{Stetigkeit}

\begin{definition}
Sei $f : X \to Y$. Das \textbf{Urbild} einer Teilmenge $B \subseteq Y$ unter $f$ ist definiert durch
\begin{equation*}
f^{-1}(B) := \{ x \in X \mid f(x) \in B \} \subseteq X
\end{equation*}

Die Menge $f^{-1}(B)$ ist immer definiert, $f$ muss dafür nicht invertierbar sein.
\end{definition}

\begin{definition}
Seien $(X,\mathfrak{X})$ und $(Y,\mathfrak{Y})$ topologische Räume.

Eine Abbildung $f : X \to Y$ heißt \textbf{stetig} :$\Leftrightarrow$ $\forall \ \mathcal{O}' \in \mathfrak{Y}: f^{-1}(\mathcal{O}') \in \mathfrak{X}$.
\end{definition}
Also hängt Stetigkeit ausschließlich von der zugrunde liegenden Topologie ab und benötigt keinen Abstandsbegriff.

Anmerkung: $f^{-1}(\mathcal{O}') \in \mathfrak{X}$ ist äquivalent zu $f^{-1}(\mathcal{O}') \subseteq X \, \wedge f^{-1}(\mathcal{O}') \text{ offen in } \mathfrak{X}$.

\begin{figure}[H]
\centering
\includegraphics[width=0.3\textwidth]{stetig.jpg}
\caption{Stetigkeit}
\end{figure}

\begin{remark}
Seien $(X,\mathfrak{X})$ und $(Y,\mathfrak{Y})$ topologische Räume und $f : X \to Y$ stetig. 
\begin{itemize}
\item Werden weitere Mengen zur Topologie $\mathfrak{X}$ hinzugefügt (die Topologie wird verfeinert), so bleibt $f$ stetig.
\item Werden Mengen aus der Topologie $\mathfrak{Y}$ entfernt (die Topologie wird gröber gemacht), so bleibt $f$ stetig.
\end{itemize}
\end{remark}

\begin{example}
Die Funktion $f: (X, \mathfrak{X}) \to (Y, \underbrace{\{\emptyset,Y\}}_{\text{indiskrete Top.}})$ ist stetig: \newline
$ f^{-1}(\emptyset)=\{x \in X \ | \ f(x) \in \emptyset\}=\emptyset$ und $ f^{-1}(Y)=\{x \in X \ | \ f(x) \in Y\}=X$. Da immer $\emptyset, X \in \mathfrak{X}$ gilt, ist $f$ stetig.

Die Funktion $f:(X,\underbrace{\mathcal{P}(X)}_{\text{diskrete Top.}}) \to (Y, \mathfrak{Y})$ ist stetig: \newline
$\forall \mathcal{O}' \in \mathfrak{Y}: f^{-1}(\mathcal{O}') \subseteq X$ und somit ist $f^{-1}(\mathcal{O}') \in \mathcal{P}(X)$.
\end{example}


\begin{example}

Seien $(X,\mathfrak{X})$ und $(Y,\mathfrak{Y})$ beliebige topologische Räume und sei $y_0 \in Y$ fest.
Die konstante Abbildung
\begin{equation*}
c : X \to Y, \qquad c(x) := y_0
\end{equation*}
ist stetig. 

\begin{minipage}[t]{0.70\textwidth}
Wir müssen also zeigen: $\forall \mathcal{O}' \in \mathfrak{Y}: c^{-1}(\mathcal{O}') \in \mathfrak{X}$

Sei $\mathcal{O}' \in \mathfrak{Y}$. Dann gilt
\begin{equation*}
c^{-1}(\mathcal{O}') = \{ x \in X \mid c(x) \in \mathcal{O}' \} = \{ x \in X \mid y_0 \in \mathcal{O}' \}
\end{equation*}

Es treten zwei Fälle auf:
\begin{itemize}
    \item Ist $y_0 \in \mathcal{O}'$, so ist $c^{-1}(\mathcal{O}') = X$
    \item Ist $y_0 \notin \mathcal{O}'$, so ist $c^{-1}(\mathcal{O}') = \emptyset$
\end{itemize}

Da immer $\emptyset, X \in \mathfrak{X}$ gilt, ist $c$ stetig.
\end{minipage}\hfill
\begin{minipage}[t]{0.30\textwidth}
% \centering
\vspace{2em}
\includegraphics[width=\textwidth]{Konstant.jpg}
\end{minipage}
\end{example}


\begin{proposition}[Zusammensetzung stetiger Funktionen ist stetig]\label{prop:zusammensetzung}
Seien $(X,\mathfrak{X}), \ (Y,\mathfrak{Y}), \ (Z,\mathfrak{Z})$ topologische Räume und $f : X \to Y$, $g : Y \to Z$. Dann gilt

$f$ und $g$ stetig $\Rightarrow \text{ die Hintereinanderausführung } g \circ f : X \to Z, \ (g \circ f)(x) := g(f(x))$ ist stetig.
\end{proposition}

\begin{proof}
zz: $\forall \ \mathcal{O}'' \in \mathfrak{Z}: \ (g \circ f)^{-1}(\mathcal{O}'') \in \mathfrak{X}$

Sei $\mathcal{O}'' \in \mathfrak{Z}$. Da $g$ stetig ist, gilt
\begin{equation*}
g^{-1}(\mathcal{O}'') \in \mathfrak{Y}
\end{equation*}
Da $f$ stetig ist, ist wiederum
\begin{equation*}
\forall \ \mathcal{O}' \in \mathfrak{Y}: \ f^{-1}(\mathcal{O}') \in \mathfrak{X}
\end{equation*}

und somit
\begin{equation*}
f^{-1}(g^{-1}(\mathcal{O}'')) \in \mathfrak{X}
\end{equation*}

Das heißt, wir müssen zeigen: 
\begin{equation*}
f^{-1}(g^{-1}(\mathcal{O}'')) = (g \circ f)^{-1}(\mathcal{O}'')
\end{equation*}

Tatsächlich gilt diese Beziehung sogar ganz allgemein für beliebige Mengen $C \subseteq Z$.
Wir zeigen, dass gilt $(g \circ f)^{-1}(C) \subseteq f^{-1}(g^{-1}(C))$ und $(g \circ f)^{-1}(C) \supseteq f^{-1}(g^{-1}(C))$.
\begin{equation*}
    x \in (g \circ f)^{-1}(C) \overset{\text{Def.}}{\Leftrightarrow} g(f(x)) \in C \overset{\substack{\text{Def.} \\ \text{Urbild}}}{\Leftrightarrow} f(x) \in g^{-1}(C) \overset{\substack{\text{Def.} \\ \text{Urbild}}}{\Leftrightarrow} x \in f^{-1}(g^{-1}(C))
\end{equation*}

Obige Richtung "$\Rightarrow$"\ zeigt "$\subseteq$"\ und "$\Leftarrow$"\ zeigt "$\supseteq$".
\end{proof}

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{Zusammensetzung.jpg}
\caption{Situation von \cref{prop:zusammensetzung}}
\end{figure}

Merksatz für $(g \circ f)^{-1}(C) = f^{-1}(g^{-1}(C))$: Wenn man Kleidung anzieht, muss man diese in umgekehrter Reihenfolge wieder ausziehen.

\begin{proposition}[Stetige Abbildungen erhalten Konvergenz]
Seien $(X,\mathfrak{X}), \ (Y,\mathfrak{Y})$ topologische Räume. Sei die Folge $(x_n)_{n \in \mathbb{N}} \subset X$ konvergent $x_n \overset{\mathfrak{X}}{\to} x \in X$ und $f : X \to Y$ stetig. 
Dann gilt
\begin{equation*}
f(x_n) \overset{\mathfrak{Y}}{\to} f(x) \in Y
\end{equation*}
\end{proposition}

\begin{proof}
zz: $\forall \mathcal{O} \in \mathfrak{Y}:\ f(x) \in \mathcal{O} \ \exists \ N_\mathcal{O} \in \mathbb{N} \ \forall n \ge N_\mathcal{O} \text{ sodass } f(x_n) \in \mathcal{O}$

Sei $\mathcal{O} \in \mathfrak{Y}$ beliebig mit $f(x) \in \mathcal{O}$. Dann ist $x \in f^{-1}(\mathcal{O})$.\newline
Da $f$ stetig ist, gilt $f^{-1}(\mathcal{O}) \in \mathfrak{X}$. Da $x_n \overset{\mathfrak{X}}{\to} x \in X$ gilt:
\begin{equation*}
\exists \ N_{f^{-1}(\mathcal{O})} \in \mathbb{N} \ \forall n \ge N_{f^{-1}(\mathcal{O})} \text{ sodass } x_n \in f^{-1}(\mathcal{O})
\end{equation*}

Daraus folgt (mit Def. Urbild)
\begin{equation*}
f(x_n) \in \mathcal{O} \quad \forall n \ge N_{f^{-1}(\mathcal{O})}
\end{equation*}
also $f(x_n) \overset{\mathfrak{Y}}{\to} f(x)$.
\end{proof}


\section{Topologisieren von Räumen}
Sei $(Y,\mathfrak{Y})$ ein topologischer Raum und $f : X \to Y$. Wir suchen die gröbstmögliche Topologie, bezüglich dieser $f$ stetig wird.

Da $f$ stetig sein soll, muss für jede offene Menge $\mathcal{O}' \in \mathfrak{Y}$ gelten, dass ihr Urbild unter $f$ offen in $X$ ist. Wir sammeln alle Urbilder der offenen Mengen in $\mathfrak{Y}$ und stecken sie in $\mathfrak{X}_f$:
\begin{equation*}
\mathfrak{X}_f := \{\, O \subseteq X \mid \exists\, \mathcal{O}' \in \mathfrak{Y} \text{ mit } O = f^{-1}(\mathcal{O}') \,\}
= f^{-1}(\mathfrak{Y})
\end{equation*}

Wir werden anschließend zeigen, dass es sich hierbei um eine Topologie handelt. Also sind alle Mengen daraus offen und $f$ somit stetig. 

$\mathfrak{X}_f$ holt in $\mathfrak{Y}$ offene Mengen nach $X$. 

Wir überprüfen nun, dass $\mathfrak{X}_f$ tatsächlich die Axiome einer Topologie erfüllt.

$\mathfrak{X}_f \subseteq \mathcal{P}(X)$ \checkmark

\begin{itemize}
    \item[(O1)] zz: $\emptyset, X \in \mathfrak{X}_f$ \newline
Da $\emptyset \in \mathfrak{Y}$ gilt, folgt
\begin{equation*}
f^{-1}(\emptyset)
=
\{x \in X \mid f(x) \in \emptyset\}
=
\emptyset,
\end{equation*}
also $\emptyset \in \mathfrak{X}_f$.

Weiter gilt $Y \in \mathfrak{Y}$ und damit
\begin{equation*}
f^{-1}(Y)
=
\{x \in X \mid f(x) \in Y\}
=
X,
\end{equation*}
also $X \in \mathfrak{X}_f$.

    \item[(O2)] zz: $\mathcal{O}_1, \mathcal{O}_2 \in \mathfrak{X}_f:\ \mathcal{O}_1 \cap \mathcal{O}_2 \in \mathfrak{X}_f$ \newline
Also zz: $\exists \mathcal{O}'  \in \mathfrak{Y}: \ \mathcal{O}_1 \cap \mathcal{O}_2 = f^{-1}(\mathcal{O}')$.

Seien $O_1, O_2 \in \mathfrak{X}_f$.
Dann existieren offene Mengen $\mathcal{O}_1', \mathcal{O}_2' \in \mathfrak{Y}$ mit $O_1 = f^{-1}(\mathcal{O}_1')$, $O_2 = f^{-1}(\mathcal{O}_2')$.

Somit ist
\begin{align*}
O_1 \cap O_2
&= f^{-1}(\mathcal{O}_1') \cap f^{-1}(\mathcal{O}_2')
\end{align*}

Da $\mathfrak{Y}$ eine Topologie ist, gilt (O2): $\mathcal{O}_1' \cap \mathcal{O}'_2 \in \mathfrak{Y}$.
Also müssen wir zeigen
\begin{equation*}
f^{-1}(\mathcal{O}_1') \cap f^{-1}(\mathcal{O}_2') = f^{-1}(\mathcal{O}'_1 \cap \mathcal{O}'_2)
\end{equation*}


Diese Beziehung kann sogar für beliebige Teilmengen $B_1, B_2 \subseteq Y$ gezeigt werden:
\begin{align*}
x \in f^{-1}(B_1) \cap f^{-1}(B_2)
& \ \, \Leftrightarrow x \in f^{-1}(B_1) \ \wedge\ x \in f^{-1}(B_2) \\
&\overset{\substack{\text{Def.} \\ \text{Urbild}}}{\Leftrightarrow} f(x) \in B_1 \ \wedge\ f(x) \in B_2 \\
& \ \, \Leftrightarrow f(x) \in B_1 \cap B_2 \\
&\overset{\substack{\text{Def.} \\ \text{Urbild}}}{\Leftrightarrow} x \in f^{-1}(B_1 \cap B_2)
\end{align*}

Damit folgt
\begin{equation*}
f^{-1}(B_1) \cap f^{-1}(B_2) = f^{-1}(B_1 \cap B_2)
\end{equation*}

Insbesondere gilt
\begin{equation*}
O_1 \cap O_2
=
f^{-1}(\mathcal{O}_1' \cap \mathcal{O}_2').
\end{equation*}

und es folgt
\begin{equation*}
O_1 \cap O_2 \in \mathfrak{X}_f.
\end{equation*}

    \item[(O3)] zz: $(\mathcal{O}_\alpha)_{\alpha \in I} \subseteq \mathfrak{X}_f:\ \bigcup_{\alpha \in I} \mathcal{O}_\alpha \in \mathfrak{X}_f$ \newline
Also zz: $\exists \mathcal{O}' \in \mathfrak{Y}: \ \bigcup_{\alpha \in I} \mathcal{O}_\alpha = f^{-1}(\mathcal{O}')$
    
Sei $(O_\alpha)_{\alpha \in I} \subseteq \mathfrak{X}_f$.  
Dann existieren offene Mengen $(\mathcal{O}'_\alpha)_{\alpha \in I} \subseteq \mathfrak{Y}$ mit
\begin{equation*}
O_\alpha = f^{-1}(\mathcal{O}'_\alpha)
\quad \text{für alle } \alpha \in I.
\end{equation*}

Wir betrachten die Vereinigung:
\begin{align*}
\bigcup_{\alpha \in I} O_\alpha
&= \bigcup_{\alpha \in I} f^{-1}(\mathcal{O}'_\alpha)
\end{align*}

Da $\mathfrak{Y}$ eine Topologie ist, gilt (O3): $\bigcup_{\alpha \in I} \mathcal{O}'_\alpha \in \mathfrak{Y}$.

Also müssen wir zeigen
\begin{equation*}
\bigcup_{\alpha \in I} f^{-1}(O_\alpha)
=
f^{-1}\!\left( \bigcup_{\alpha \in I} O_\alpha \right)
\end{equation*}

Diese Beziehung kann wieder sogar für beliebige Teilmengen $(B_\alpha)_{\alpha \in I} \subseteq \mathcal{P}(Y)$ gezeigt werden:
\begin{align*}
x \in \bigcup_{\alpha \in I} f^{-1}(B_\alpha)
&\ \, \Leftrightarrow  \exists\, \alpha_0 \in I : x \in f^{-1}(B_{\alpha_0}) \\
&\overset{\substack{\text{Def.} \\ \text{Urbild}}}{\Leftrightarrow} \exists\, \alpha_0 \in I : f(x) \in B_{\alpha_0} \\
&\ \, \Leftrightarrow  f(x) \in \bigcup_{\alpha \in I} B_\alpha \\
&\overset{\substack{\text{Def.} \\ \text{Urbild}}}{\Leftrightarrow} x \in f^{-1}\!\left( \bigcup_{\alpha \in I} B_\alpha \right)
\end{align*}

Daraus folgt
\begin{equation*}
\bigcup_{\alpha \in I} f^{-1}(B_\alpha)
=
f^{-1}\!\left( \bigcup_{\alpha \in I} B_\alpha \right)
\end{equation*}

Insbesondere gilt
\begin{equation*}
\bigcup_{\alpha \in I} O_\alpha
=
f^{-1}\!\left( \bigcup_{\alpha \in I} \mathcal{O}'_\alpha \right)
\end{equation*}

und damit
\begin{equation*}
\bigcup_{\alpha \in I} O_\alpha \in \mathfrak{X}_f
\end{equation*}

\end{itemize}

\begin{definition}
Die so definierte Topologie $\mathfrak{X}_f$ heißt die \textbf{initiale Topologie}
bezüglich der Abbildung $f : X \to (Y,\mathfrak{Y})$.
\end{definition}

Sie ist die \emph{gröbste} Topologie auf $X$, für die $f$ stetig ist. Jede andere Topologie auf $X$, bezüglich der $f$ stetig ist, ist feiner als $\mathfrak{X}_f$.

\begin{remark}
Das Konzept der initialen Topologie lässt sich unmittelbar auf Familien von Abbildungen
$f_i : X \to Y_i$ verallgemeinern.  
In diesem Fall ist die initiale Topologie die gröbste Topologie auf $X$, bezüglich der alle $f_i$ stetig sind.
\end{remark}


\section{Spurtopologie (Teilmengentopologie)}

Sei $(X,\mathfrak{X})$ ein topologischer Raum und sei $U \subseteq X$ eine Teilmenge. Wir möchten $U$ mit einer Topologie versehen, die in natürlicher Weise aus der gegebenen Topologie auf $X$ hervorgeht.

\begin{minipage}[t]{0.85\textwidth}
Zu diesem Zweck betrachten wir die sogenannte \textbf{kanonische Einbettungsabbildung}
\begin{equation*}
i_U : U \to X, \qquad i_U(x) := x.
\end{equation*}
\end{minipage}\hfill
\begin{minipage}[t]{0.15\textwidth}
\vspace{-1em}
\includegraphics[width=\textwidth]{Einbettung.jpg}
\end{minipage}

Diese Abbildung ordnet jedem Punkt aus $U$ denselben Punkt in $X$ zu und beschreibt damit die Einbettung der Teilmenge $U$ in den umgebenden Raum $X$.

Sei $\mathcal{O} \in \mathfrak{X}$. Dann gilt nach Definition des Urbildes:
\begin{equation*}
i_U^{-1}(\mathcal{O}) =
\{ x \in U \mid i_U(x) \in \mathcal{O} \} = \{ x \in U \mid x \in \mathcal{O} \} = \mathcal{O} \cap U \text{... repräsentiert die Spur von } \mathcal{O} \text{ in } U
\end{equation*}

\begin{definition}
Die \textbf{Spurtopologie} oder \textbf{Teilmengentopologie} auf $U$ ist definiert durch
\begin{equation*}
\mathfrak{U}
:=
i_U^{-1}(\mathfrak{X})
= \{ \mathcal{O}' \subseteq U \mid \mathcal{O}' = \mathcal{O} \cap U \wedge \mathcal{O} \in \mathfrak{X} \}
= \{ \mathcal{O} \cap U \mid \mathcal{O} \in \mathfrak{X} \} = \mathfrak{X} \cap U
\end{equation*}
und macht $(U,\mathfrak{U})$ zu einem topologischen Raum.
\end{definition}

Die offenen Mengen der Spurtopologie sind also genau diejenigen Mengen, die als Schnitt einer offenen Menge aus $X$ mit $U$ entstehen.

Die Spurtopologie beschreibt die "\emph{Spur}", welche die Topologie $\mathfrak{X}$ des umgebenden Raumes $X$ auf der Teilmenge $U$ hinterlässt. Sie ist die Einschränkung von $\mathfrak{X}$ auf $U$.

\section{Homöomorphismen}

Wir betrachten zwei topologische Räume $(X,\mathfrak{X})$ und $(Y,\mathfrak{Y})$. Wann sind zwei Räume aus topologischer Sicht \emph{gleichartig} (d.h. besitzen dieselben topologischen Eigenschaften)? 

\begin{definition}
Seien $(X,\mathfrak{X})$ und $(Y,\mathfrak{Y})$ topologische Räume.
Eine Abbildung $f : X \to Y$ heißt \textbf{Homöomorphismus} $:\Leftrightarrow$
\begin{itemize}
    \item f ist bijektiv. d.h. $\exists g : Y \to X$ (Inverse von $f$) sodass 
    \begin{equation*}
    g \circ f = \mathrm{id}_X \text{ (also f injektiv)}
    \qquad \text{und} \qquad
    f \circ g = \mathrm{id}_Y \text{ (also f surjektiv)}
    \end{equation*}
    \item $f$ und $g$ sind stetig
\end{itemize}

In diesem Fall heißen die Räume $(X,\mathfrak{X})$ und $(Y,\mathfrak{Y})$ \textbf{homöomorph}. In Zeichen $\mathfrak{X} \simeq \mathfrak{Y}$.
\end{definition}

\begin{remark}
homo = gleich, morph = Gestalt $\Rightarrow$ Homöomorphismus = gleichgestaltig. Homöomorphismen sind Isomorphismen auf topologischen Räumen. $\mathfrak{X}$ und $\mathfrak{Y}$ sind nicht gleich, aber aus topologischer Sicht ununterscheidbar. Sie haben dieselben 
topologischen Eigenschaften wie beispielsweise Kompaktheit, Zusammenhang und Trennsungseigenschaften.

Ein Homöomorphismus dehnt, staucht, verbiegt, verzerrt einen Gegenstand so, dass er dennoch die gleichen topologischen Eigenschaften wie zuvor hat. Zerschneiden ist nur erlaubt, wenn die Teile später genau an der Schnittfläche wieder zusammenfügt werden.

\begin{figure}[H]
\centering
\includegraphics[width=0.3\textwidth]{donut.jpg}
\caption{Eine Tasse ist homöomorph zu einem Donut. \newline
{\footnotesize Quelle: \url{https://commons.wikimedia.org/wiki/File:Topology_joke.jpg}}}
\end{figure}

\end{remark}

\begin{theorem}
    $(X,\mathfrak{X})$ und $(Y,\mathfrak{Y})$ topologische Räume und $f : X \to Y$ ein Homöomorphismus mit der Inversen $g: Y \to X$. Wenn wir $f$ nun auffassen als $f^{-1} : \mathcal{P}(Y) \to \mathcal{P}(X)$, ordnet $f^{-1}$ jeder offenen Menge $\mathcal{O}' \in \mathfrak{Y}$ genau ein $\mathcal{O} \in \mathfrak{X}$ zu sodass $f^{-1}(\mathcal{O}') = \mathcal{O}$.

    Also gilt $f^{-1}(\mathfrak{Y})=\mathfrak{X}$ und analog dazu $g^{-1}(\mathfrak{X})=\mathfrak{Y}$. Das heißt, die Topologien stehen in einer 1 zu 1 Beziehung, nicht nur die Punkte aus $X$ und $Y$.
\end{theorem}

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{Homöomorph.jpg}
\caption{Homöomorphismus $f$ zwischen $(X,\mathfrak{X})$ und $(Y,\mathfrak{Y})$}
\end{figure}

\begin{proof} \breakafterhead

\begin{itemize}
    \item zz: $g^{-1}(\mathfrak{X})=\mathfrak{Y}$
    
Aufgrund der Stetigkeit von $f$ gilt: $f^{-1}(\mathfrak{Y}) \subseteq \mathfrak{X}$

Wir möchten darauf $g^{-1}$ anwenden. Daher zeigen wir die folgende allgemeine Beziehung.

Für beliebige Teilmengen $B_1, B_2 \subseteq Y$ gilt:
\begin{equation}\label{eq:subset}
B_1 \subseteq B_2 \;\Rightarrow\; f^{-1}(B_1) \subseteq f^{-1}(B_2)
\end{equation}
denn
\begin{equation*}
x \in f^{-1}(B_1)
\Leftrightarrow
f(x) \in B_1 \subseteq B_2
\Rightarrow
f(x) \in B_2
\Leftrightarrow
x \in f^{-1}(B_2)
\end{equation*}

Wenden wir \eqref{eq:subset} nun an, so erhalten wir:
\begin{align*}
& f^{-1}(\mathfrak{Y}) \subseteq \mathfrak{X}
\Rightarrow
\underbrace{g^{-1}(f^{-1}(\mathfrak{Y}))}_{=(f \circ g)^{-1}(\mathfrak{Y})} \subseteq g^{-1}(\mathfrak{X}) \\
& \Leftrightarrow
\underbrace{\underbrace{(f \circ g)^{-1}}_{= id_Y}(\mathfrak{Y})}_{=\mathfrak{Y}} \subseteq g^{-1}(\mathfrak{X}) \\
& \Leftrightarrow
\mathfrak{Y} \subseteq g^{-1}(\mathfrak{X})
\end{align*}

Da $g$ stetig ist, gilt: $g^{-1}(\mathfrak{X}) \subseteq \mathfrak{Y}$

Zusammen folgt daher:
\begin{equation}\label{eq:inverse}
\mathfrak{Y} = g^{-1}(\mathfrak{X})
\end{equation}

\item zz: $f^{-1}(\mathfrak{Y})=\mathfrak{X}$

\begin{equation*}
f^{-1}(\mathfrak{Y}) \overset{\eqref{eq:inverse}}{=} \underbrace{f^{-1}(g^{-1}}_{(g \circ f)^{-1}}(\mathfrak{X})) = \underbrace{(g \circ f)^{-1}}_{= id_X}(\mathfrak{X}) = \mathfrak{X}
\end{equation*}

\end{itemize}
\end{proof}

\section{Trennungsaxiome}
{\footnotesize Quellen: \url{https://wwwold.mathematik.tu-dortmund.de/~lschwach/WS13/Proseminar/Dirk_Lauschner.pdf} \newline
\url{https://www.tuwien.at/index.php?eID=dumpFile&t=f&f=204097&token=564fbc343ac3892d97b2713d52d540fd084e8ca5} \newline
\url{https://www.math.tugraz.at/~ganster/lv_materialien/trennungseigenschaften.pdf}}

Trennungseigenschaften geben Auskunft darüber, ob "genügend offene Mengen” vorhanden sind, um Punkte bzw. Teilmengen voneinander zu "trennen". Aus dem Vorhandensein gewisser Trennungseigenschaften ergeben sich meist sehr wichtige Konsequenzen.

\begin{definition}
Ein topologischer Raum $(X,\mathfrak{X})$ heißt \textbf{$T_0$-Raum} :$\Leftrightarrow$
\begin{equation*}
\forall x,y \in X \text{ mit } x \neq y \ \exists\, \mathcal{O} \in \mathfrak{X} :
\bigl( x \in \mathcal{O} \land y \notin \mathcal{O} \bigr)
\; \lor \;
\bigl( y \in \mathcal{O} \land x \notin \mathcal{O} \bigr)
\end{equation*}
Das heißt: die Topologie kann verschiedene Punkte unterscheiden.
\end{definition}

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{T0.jpg}
\caption{$T_0$}
\end{figure}

\begin{definition}
Ein topologischer Raum $(X,\mathfrak{X})$ heißt \textbf{$T_1$-Raum} :$\Leftrightarrow$
\begin{equation*}
\forall x,y \in X \text{ mit } x \neq y \
\exists\, \mathcal{O}_x,\mathcal{O}_y \in \mathfrak{X} :
\bigl( x \in \mathcal{O}_x \land y \in \mathcal{O}_y \bigr)
\;\land\;
\bigl( x \notin \mathcal{O}_y \land y \notin \mathcal{O}_x \bigr)
\end{equation*}

Das heißt: je zwei verschiedene Punkte können durch offene Mengen getrennt werden. 
\end{definition}

\begin{figure}[H]
\centering
\includegraphics[width=0.25\textwidth]{T1.jpg}
\caption{$T_1$}
\end{figure}

Im Gegensatz zu $T_0$ verlangt $T_1$, dass es offene Mengen um \emph{beide} Punkte gibt, die jeweils den anderen Punkt ausschließen. Also anstelle des logischen \textquotedbl oder"\ von $T_0$ gilt hier das logische \textquotedbl und".

Es gilt:
\begin{equation*}
T_1 \Rightarrow T_0
\qquad
\text{aber } \
T_0 \nRightarrow T_1
\end{equation*}

\begin{figure}[H]
\centering
\includegraphics[width=0.2\textwidth]{NotT1.jpg}
\caption{$T_0$ aber nicht $T_1$}
\end{figure}

\begin{definition}
Eine Menge $A \subseteq X$ ist \textbf{abgeschlossen} : $\Leftrightarrow$ $A^c:= X \setminus A \in \mathfrak{X}$ (das Komplement ist offen)
\end{definition}

\begin{remark}
In der Vorlesung wird statt $X \setminus A$ geschrieben $X - A$.
\end{remark}

\begin{remark}
Die leere Menge $\emptyset$ und die Gesamtmenge $X$ sind immer offen und abgeschlossen zugleich:
Per Definition ist immer $\emptyset, X \in \mathfrak{X}$ (also die Mengen sind offen) und es gilt:
$\emptyset^c=X \setminus \emptyset = X$, $X^c = X \setminus X = \emptyset$ (also sind die Mengen abgeschlossen).

Fun fact: Wenn $\emptyset$ und $X$ die einzigen Mengen sind, die offen und abgeschlossen sind, ist der Raum \textit{zusammenhängend} (kann nicht in disjunkte nichtleere offene Mengen zerlegt werden).
\end{remark}

\begin{proposition}
In einem $T_1$-Raum sind alle einpunktigen Mengen abgeschlossen.
\end{proposition}

\begin{proof}
zz: $\forall x \in X: \ X \setminus \{x\} \in \mathfrak{X}$

\begin{minipage}[t]{0.8\textwidth}
Sei $x \in X$ beliebig. Sei dazu $x' \in X \setminus \{x\}$, also $x' \neq x$.
Da der Raum $T_1$ ist, existiert $\mathcal{O}_{x'} \in \mathfrak{X}$
mit
\begin{equation*}
x' \in \mathcal{O}_{x'} \quad \text{und} \quad x \notin \mathcal{O}_{x'}.
\end{equation*}
Damit gilt 
\begin{equation*}
\mathcal{O}_{x'} \subseteq X \setminus \{x\}
\end{equation*}

\end{minipage}\hfill
\begin{minipage}[t]{0.2\textwidth}
\centering
\vspace{-1em}
\includegraphics[width=\textwidth]{Abgeschlossen.jpg}
\end{minipage}

Somit auch
\begin{equation*}
\{x'\} \subseteq \mathcal{O}_{x'} \subseteq X \setminus \{x\}
\end{equation*}

Diese Kettenbeziehung bleibt durch Vereinigung über $x' \in X \setminus \{x\}$ erhalten
\begin{equation*}
\underbrace{\bigcup_{x' \in X \setminus \{x\}} \{x'\}}_{= X \setminus \{x\}}  \subseteq \bigcup_{x' \in X \setminus \{x\}} \mathcal{O}_{x'} \subseteq \underbrace{\bigcup_{x' \in X \setminus \{x\}} X \setminus \{x\}}_{= X \setminus \{x\}}
\end{equation*}

Wir erhalten
\begin{equation*}
X \setminus \{x\} \subseteq \bigcup_{x' \in X \setminus \{x\}} \mathcal{O}_{x'} \subseteq X \setminus \{x\}
\end{equation*}

Also
\begin{equation*}
X \setminus \{x\} = \bigcup_{x' \in X \setminus \{x\}} \mathcal{O}_{x'} \overset{\text{(O3)}}{\in} \mathfrak{X}
\end{equation*}

Damit ist $\{x\}$ abgeschlossen.
\end{proof}

\begin{definition}
Ein topologischer Raum $(X,\mathfrak{X})$ heißt \textbf{$T_2$-Raum} oder \textbf{Hausdorff-Raum} :$\Leftrightarrow$
\begin{equation*}
\forall x,y \in X \text{ mit } x \neq y \ \exists\, \mathcal{O}_x,\mathcal{O}_y \in \mathfrak{X} :
( x \in \mathcal{O}_x \land y \in \mathcal{O}_y ) 
\land
(\mathcal{O}_x \cap \mathcal{O}_y = \emptyset)
\end{equation*}

Das heißt: verschiedene einpunktige Mengen lassen sich durch disjunkte offene Mengen trennen.
\end{definition}

\begin{figure}[H]
\centering
\includegraphics[width=0.3\textwidth]{T2.jpg}
\caption{$T_2$}
\end{figure}


Hier werden die beiden Punkte nicht nur getrennt, sondern sogar durch disjunkte offene Mengen. Also eine Verschärfung von $T_1$. Insbesondere gilt:
\begin{equation*}
T_2 \Rightarrow T_1
\qquad
\text{aber } \ T_1 \nRightarrow T_2
\end{equation*}

\begin{figure}[H]
\centering
\includegraphics[width=0.2\textwidth]{T1.jpg}
\caption{Ist $T_1$ aber nicht $T_2$.}
\end{figure}

\begin{proposition}
In einem Hausdorff-Raum $(X, \mathfrak{X})$ ist der Grenzwert einer konvergenten Folge eindeutig.
\end{proposition}

\begin{proof}
Wir zeigen dies mit Kontraposition: \newline
Grenzwert einer konvergenten Folge ist nicht eindeutig $\Rightarrow$ nicht $T_2$

\begin{minipage}[t]{0.75\textwidth}
Angenommen, eine Folge $(x_n)_{n \in \mathbb{N}}$ konvergiert sowohl gegen $x$ als auch gegen $y$ mit $x \neq y$.

Aus der Konvergenz folgt, dann es Indizes $N_x,N_y$ gibt mit
\begin{align*}
\forall \mathcal{O}_x \in \mathfrak{X} \text{ mit } x \in \mathcal{O}_x \ \exists \, N_x \in \mathbb{N} \ \forall n \ge N_x \text{ sodass } x_n \in \mathcal{O}_x \\
\forall \mathcal{O}_y \in \mathfrak{X} \text{ mit } y \in \mathcal{O}_y \ \exists \, N_y \in \mathbb{N} \ \forall n \ge N_y \text{ sodass } x_n \in \mathcal{O}_y
\end{align*}
\end{minipage}\hfill
\begin{minipage}[t]{0.25\textwidth}
\centering
\vspace{-1em}
\includegraphics[width=\textwidth]{Eindeutiggw.jpg}
\end{minipage}

Daher müsste es einen Index $N^* = \text{max}\{N_x, N_y\}$ geben, für den gilt
\begin{equation*}
\forall x \ge N^*: \ x_n \in \mathcal{O}_x \land x_n \in \mathcal{O}_y
\end{equation*}

und somit ist
\begin{equation*}
\mathcal{O}_x \cap \mathcal{O}_y \neq \emptyset
\end{equation*}

Also ist der topologische Raum nicht Hausdorff.
\end{proof}


\begin{definition}
Ein topologischer Raum $(X,\mathfrak{X})$ heißt \textbf{$T_3$-Raum} :$\Leftrightarrow$
\begin{equation*}
\forall x \in X, A \subseteq X \text{ abgeschlossen}, x \notin A \
\exists\, \mathcal{O}_x,\mathcal{O}_A \in \mathfrak{X} : \
(x \in \mathcal{O}_x \land A \subseteq \mathcal{O}_A)
\land
(\mathcal{O}_x \cap \mathcal{O}_A = \emptyset)
\end{equation*}
Das heißt: Einpunktige und abgeschlossene Mengen lassen sich durch disjunkte offene Mengen trennen.
\end{definition}

\begin{figure}[H]
\centering
\includegraphics[width=0.3\textwidth]{T3.jpg}
\caption{$T_3$}
\end{figure}

\begin{definition}
Ein topologischer Raum $(X,\mathfrak{X})$ heißt \textbf{$T_4$-Raum} :$\Leftrightarrow$
\begin{equation*}
\forall A, B \subseteq X \text{ abgeschlossen mit } A \cap B = \emptyset \
\exists\, \mathcal{O}_A,\mathcal{O}_B \in \mathfrak{X} :
( A \subseteq \mathcal{O}_A \land B \subseteq \mathcal{O}_B)
\land
(\mathcal{O}_A \cap \mathcal{O}_B = \emptyset)
\end{equation*}
Das heißt: disjunkte abgeschlossene Mengen lassen sich durch disjunkte offene Mengen trennen.
\end{definition}

\begin{figure}[H]
\centering
\includegraphics[width=0.3\textwidth]{T4.jpg}
\caption{$T_4$}
\end{figure}

\begin{tcolorbox}[noVO]
\begin{definition}[Nicht Teil der VO] \breakafterhead
\vspace{1em}

Ein topologischer Raum $(X,\mathfrak{X})$ heißt 
\begin{itemize}
    \item \textbf{regulär}, wenn er $T_1$ \& $T_3$ ist
    \item \textbf{normal}, wenn er $T_1$ \& $T_4$ ist
\end{itemize}
In der Literatur ist "regulär", "normal"\ und $T_4$ nicht einheitlich definiert, siehe \href{https://de.wikipedia.org/wiki/Normaler_Raum}{hier}. 
\end{definition}

\begin{remark}
Zwischen den Trennungsaxiomen bestehen folgende wichtige Zusammenhänge:
\begin{align*}
T_1 + T_3 = T_2: \qquad & \text{einpunktige Mengen sind abgeschlossen ($T_1$)} \\
& \text{+ einpunktige Mengen lassen sich von abgeschlossene Mengen trennen ($T_3$)} \\
& \Rightarrow \text{einpunktige Mengen lassen sich voneinander trennen ($T_2$)}
\end{align*}

\begin{align*}
T_1 + T_4 = T_3: \qquad & \text{einpunktige Mengen sind abgeschlossen ($T_1$)} \\
& \text{+ abgeschlossene Mengen lassen sich voneinander trennen ($T_4$)} \\
& \Rightarrow \text{einpunktige \& abgeschlossene Mengen lassen sich voneinander trennen ($T_3$)}
\end{align*}
\end{remark}

\begin{example}
Es gibt topologische Räume, die $T_3$ sind, aber weder $T_1$ noch $T_2$ erfüllen (also $T_3 \nRightarrow T_2$ und $T_3 \nRightarrow T_1$).

Sei $X := \{x,y\}, \mathfrak{X} := \{\emptyset, X\}$ die indiskrete Topologie auf $X$.

\begin{itemize}
    \item Offene Mengen: $\emptyset$ und $\{x,y\}=X$.
    \item Abgeschlossene Mengen: $\emptyset^c=\{x,y\}=X$ und $X^c=\emptyset$.
\end{itemize}

Der Raum $(X,\mathfrak{X})$ ist $T_3$:
\begin{itemize}
    \item trenne $x$ und $\emptyset$: $x \in X$ und $\emptyset \subseteq \emptyset$
    \item trenne $y$ und $\emptyset$: $y \in X$ und $\emptyset \subseteq \emptyset$
\end{itemize}
Der Raum ist \emph{nicht} $T_1$:
Versuche $x$ und $y$ zu trennen. Es gibt nur eine offene Menge, in der bereits sowohl $x$ als auch $y$ liegen: $X$. Somit können $x$ und $y$ nicht durch verschiedene offene Mengen getrennt werden.

Der Raum auch \textit{nicht} $T_2$: Versuche $x$ und $y$ zu trennen. Da es keine verschiedene offene Mengen gibt, die $x$ bzw. $y$ enthalten, gibt es insbesondere keine verschiedene disjunkte offene Mengen, die dies tun.
\end{example}
\end{tcolorbox}

\begin{remark}
Metrische Räume sind $T_2$ und $T_4$. Da $T_2 \Rightarrow T_1 \Rightarrow T_0$ und $T_1 + T_4 = T_3$ erfüllen sie also alle hier genannten Trennungsaxiome. Der $\mathbb{R}^n$ ist ein metrischer Raum.
\end{remark}


\section{Die euklidische Topologie}

Wir wollen nun untersuchen, mit welcher Topologie der Raum $\mathbb{R}^n$ \emph{standardmäßig} ausgestattet wird, sodass die aus der reellen Analysis bekannten Begriffe von Konvergenz und $\varepsilon$--$\delta$-Stetigkeit mit der topologischen Struktur übereinstimmen. Diese Topologie heißt die \textbf{euklidische Topologie} oder auch \textbf{Standardtopologie des $\mathbb{R}^n$}.


\begin{definition}
Eine Abbildung
\begin{equation*}
d : X \times X \to \mathbb{R}
\end{equation*}
heißt \textbf{Metrik}, falls für alle $x,y,z \in X$ gilt:
\begin{align*}
d(x,y) \geq 0 \quad & \text{und} \quad d(x,y) = 0 \Leftrightarrow x=y &\text{(positive Definitheit)} \\
d(x,y) &= d(y,x) &\text{(Symmetrie)} \\
d(x,y) &\le d(x,z) + d(z,y) &\text{(Dreiecksungleichung)}
\end{align*}

Ein Raum $(M,d)$, der mit einer Metrik $d$ versehen ist, heißt \textbf{metrischer Raum}.
\end{definition}

\begin{figure}[H]
\centering
\includegraphics[width=0.2\textwidth]{Dreiecksungl.jpg}
\caption{Dreiecksungleichung}
\end{figure}

Der Abstand zwischen zwei Punkten $x,y \in \mathbb{R}^n$ wird durch die \textbf{euklidische Metrik} beschrieben
\begin{equation*}
d(x,y) := \|x-y\|_2 := \sqrt{\sum_{i=1}^n (x_i-y_i)^2}
\end{equation*}
wobei $\| \cdot \|_2$ die \textbf{euklidische Norm} ist.

Mithilfe dieses Abstands definieren wir sogenannte \textbf{offene Kugeln}.
Für einen Mittelpunkt $x \in \mathbb{R}^n$ und einen Radius $\varepsilon > 0$
sei
\begin{equation*}
K(x,\varepsilon)
:= \{\, y \in \mathbb{R}^n \mid \|y - x\|_2 < \varepsilon \,\}
\end{equation*}

Wir definieren die \textbf{euklidische Topologie} $\mathfrak{R}$ als die Menge aller Teilmengen $O \subseteq \mathbb{R}^n$, die sich als Vereinigung offener Kugeln $K_\alpha := K(x_\alpha, \varepsilon_\alpha)$
schreiben lassen:
\begin{equation*}
\mathfrak{R}
:= \bigl\{\, O \subseteq \mathbb{R}^n \mid
O = \bigcup_{\alpha \in I} K_\alpha \,\bigr\}
\end{equation*}

Dann heißt $(\mathbb{R}^n,\mathfrak{R})$ \textbf{euklidischer Raum}.

Diese Definition liefert zunächst lediglich eine Kandidatenmenge. Wir müssen nun zeigen, dass $\mathfrak{R}$ tatsächlich eine Topologie ist.
Dazu überprüfen wir die drei Topologieaxiome.

\begin{itemize}
    \item[(O1)] zz: $\emptyset, \mathbb{R}^n \in \mathfrak{R}$
    
    Wir definieren
    \begin{equation*}
    \emptyset := \bigcup_{\alpha \in \emptyset} K_\alpha
    \end{equation*}
    Damit ist $\emptyset \in \mathfrak{R}$.

    Für jedes $x \in \mathbb{R}^n$ existiert ein $n \in \mathbb{N}$ mit $\|x\|_2 < n$, also $x \in K(0,n)$ und
    \begin{align*}
    \{x\} &\subseteq K(0,n) \\
    \Leftrightarrow \underbrace{\bigcup_{x \in\mathbb{R}^n}\{x\}}_{=\mathbb{R}^n} & \subseteq \underbrace{\bigcup_{x \in \mathbb{R}^n} K(0,n)}_{=K(0,n)} \\
    \Leftrightarrow \mathbb{R}^n & \subseteq K(0,n)
    \end{align*}
    Außerdem gilt 
    \begin{equation*}
        K(0,n) \subseteq \bigcup_{n \in \mathbb{N}} K(0,n) \subseteq \mathbb{R}^n
    \end{equation*}
    
    Somit folgt
    \begin{equation*}
    \mathbb{R}^n = \bigcup_{n \in \mathbb{N}} K(0,n) \in \mathfrak{R}
    \end{equation*}

    \item[(O2)] zz: $\forall O_1, O_2 \in \mathfrak{R}: \mathcal{O}_1 \cap \mathcal{O}_2 \in \mathfrak{R}$

    Braucht mehr Überlegungen, da der Durchschnitt von Kugeln nicht notwendigerweise eine Kugel sein muss. Seien nun $\mathcal{O}_1, \mathcal{O}_2 \in \mathfrak{R}$. Dann existieren Familien offener Kugeln $(K_\alpha)_{\alpha\in I}$ und
    $(K_\beta)_{\beta\in J}$ mit
    \begin{equation*}
    \mathcal{O}_1 = \bigcup_{\alpha\in I} K_\alpha,
    \qquad
    \mathcal{O}_2 = \bigcup_{\beta\in J} K_\beta.
    \end{equation*}
    Der Schnitt lässt sich schreiben als
    \begin{equation*}
    \mathcal{O}_1 \cap \mathcal{O}_2
    = \bigcup_{\substack{\alpha \in I \\ \beta \in J}} K_\alpha \cap K_\beta
    \end{equation*}
    
    Fall 1: für $\alpha \in I, \beta \in J: \ K_\alpha \cap K_\beta = \emptyset \in \mathfrak{R} \ \checkmark$
    
    Fall 2: für $\alpha \in I, \beta \in J: K_\alpha \cap K_\beta \neq \emptyset$. Dieser Fall benötigt mehr Arbeit.  

    Dazu werden wir zeigen, dass für $y \in K_\alpha \cap K_\beta$ und ein spezielles $\delta^* >0$ gilt: 
    \begin{equation*}
    K_\alpha \cap K_\beta = \bigcup_{y \in K_\alpha \cap K_\beta} K(y,\delta^*) \in \mathfrak{R}
    \end{equation*}
    \begin{minipage}[t]{0.65\textwidth}
    Sei also $y \in K_\alpha \cap K_\beta$ beliebig.

    Dann gelten die Abstände
    \begin{equation*}
    \eta_\alpha := \|y - x_\alpha\|_2 < \varepsilon_\alpha
    \qquad
    \eta_\beta := \|y - x_\beta\|_2 < \varepsilon_\beta
    \end{equation*}
    
    Wir setzen
    \begin{equation*}
    \delta_\alpha := \varepsilon_\alpha - \eta_\alpha > 0,
    \qquad
    \delta_\beta := \varepsilon_\beta - \eta_\beta > 0,
    \end{equation*}
    und definieren
    \begin{equation*}
    \delta^* := \min\{\delta_\alpha,\delta_\beta\}
    \end{equation*}
    \end{minipage}\hfill
    \begin{minipage}[t]{0.345\textwidth}
    \centering
    \vspace{-1em}
    \includegraphics[width=\textwidth]{Euklid1.jpg}
    \includegraphics[width=\textwidth]{Euklid2.jpg}
    \end{minipage}
    
    Wir zeigen nun:
    \begin{equation*}
    K(y,\delta^*) \subseteq K_\alpha \cap K_\beta
    \end{equation*}
    Dafür müssen wir zeigen, dass $\forall z \in K(y,\delta^*): \ z \in K_\alpha \land z \in K_\beta$ \newline
    Also müssen wir zeigen: $\|z-x_\alpha \|_2 < \varepsilon_\alpha \land \|z-x_\beta \|_2 < \varepsilon_\beta$ \newline
    Sei dazu $z \in K(y,\delta^*$ beliebig.
    Dann gilt
    \begin{align*}
    \|z-x_\alpha\|_2 = \|z-y + y-x_\alpha\|_2
    &\overset{\triangle \text{Ungl.}}{\leq} \|z-y\|_2 + \|y-x_\alpha\|_2 \\
    & < \delta^* + \eta_\alpha \\
    &\le \delta_\alpha + \eta_\alpha \\
    & = \varepsilon_\alpha - \eta_\alpha + \eta_\alpha = \varepsilon_\alpha
    \end{align*}
    Also ist $z \in K_\alpha$.
    Die gleiche Rechnung liefert
    \begin{equation*}
    \|z-x_\beta\| < \varepsilon_\beta,
    \end{equation*}
    also $z \in K_\beta$.
    
    Damit gilt
    \begin{equation*}
    K(y,\delta^*) \subseteq K_\alpha \cap K_\beta.
    \end{equation*}
    und auch
    \begin{align*}
    \{y\} &\subseteq K(y,\delta^*) \subseteq K_\alpha \cap K_\beta \\
    \Leftrightarrow \underbrace{\bigcup_{y \in K_\alpha \cap K_\beta} \{y\}}_{=K_\alpha \cap K_\beta} &\subseteq \bigcup_{y \in K_\alpha \cap K_\beta} K(y,\delta^*) \subseteq \underbrace{\bigcup_{y \in K_\alpha \cap K_\beta} K_\alpha \cap K_\beta}_{=K_\alpha \cap K_\beta}
    \end{align*}
    
    Somit folgt:
    \begin{equation*}
    K_\alpha \cap K_\beta
    = \bigcup_{y \in K_\alpha \cap K_\beta} K(y,\delta^*)
    \in \mathfrak{R}.
    \end{equation*}

    Damit haben wir erhalten
    \begin{equation*}
    \mathcal{O}_1 \cap \mathcal{O}_2
    = \bigcup_{\substack{\alpha \in I \\ \beta \in J}} K_\alpha \cap K_\beta
    = \bigcup_{\substack{\alpha \in I \\ \beta \in J}} \bigcup_{y \in K_\alpha \cap K_\beta} K(y,\delta^*)
    \in \mathfrak{R}.
    \end{equation*}

    \item[(O3)] zz: $\forall (O_\alpha)_{\alpha\in I} \subseteq \mathfrak{R}: \ \bigcup_{\alpha \in I} \mathcal{O}_\alpha \in \mathfrak{R}$
    
    Sei nun $(O_\alpha)_{\alpha\in I} \subseteq \mathfrak{R}$.
    Dann existieren für jedes $\alpha$ offene Kugeln $(K_{\beta})_{\beta\in I_\alpha}$
    mit
    \begin{equation*}
    O_\alpha = \bigcup_{\beta\in I_\alpha} K_\beta.
    \end{equation*}
    Daraus folgt
    \begin{equation*}
    \bigcup_{\alpha\in I} O_\alpha
    = \bigcup_{\alpha\in I} \bigcup_{\beta\in I_\alpha} K_\beta \in \mathfrak{R}.
    \end{equation*}
\end{itemize}


Damit sind alle drei Topologieaxiome erfüllt, und $\mathfrak{R}$ ist eine Topologie auf $\mathbb{R}^n$. 
\vspace{1em}


\begin{definition}
Ein Vektorraum $V$ über dem Körper $\mathbb{K}\in\{\mathbb{R},\mathbb{C}\}$, versehen mit einer Topologie $\mathfrak{X}$, heißt \textbf{topologischer Vektorraum}, wenn die algebraischen Operationen Vektoraddition und Skalarmultiplikation stetig sind.
\end{definition}

\begin{tcolorbox}[noVO]
\begin{remark}[Nicht Teil der VO] \breakafterhead

Sei $(V,\|\cdot\|)$ ein Vektorraum mit einer Norm.

\begin{enumerate}
    \item Die Norm induziert durch Differenzenbildung eine Metrik
    \begin{equation*}
        d(x,y) := \|x-y\|, \qquad x,y \in V.
    \end{equation*}
    Damit wird der Vektorraum $(V,d)$ ein metrischer Raum.

    \item Mit dieser Metrik definieren wir für $x\in V$ und $\varepsilon>0$
    die offene Kugel
    \begin{equation*}
        U_\varepsilon(x) := \{\, y\in V \mid d(x,y)<\varepsilon \,\}
    \end{equation*}
    Damit heißt eine Teilmenge $M\subseteq V$ \emph{offen}, falls
    \begin{equation*}
        \forall x\in M\ \exists\,\varepsilon>0:\ U_\varepsilon(x)\subseteq M.
    \end{equation*}

    \item Über diese offenen Mengen induziert die Metrik nun auf $V$ eine Topologie:
    \begin{equation*}
        \mathfrak{X} := \{\, M\subseteq V \mid M \text{ ist offen} \,\}
    \end{equation*}
    \item Mit dieser Topologie wird der Vektorraum $V$ zu einem \textit{topologischen Vektorraum} $(V,\mathfrak{X})$. 
\end{enumerate}

Die so entstehende Topologie heißt die von der Norm induzierte \textbf{Normtopologie}. Das heißt, jeder Normierte Raum kann zu einem metrischen Raum gemacht werden und jeder metrische Raum kann zu einem topologischen Raum gemacht werden. Damit gelten alle Aussagen über topologische Räume auch für metrische und normierte Räume.
\end{remark}
\end{tcolorbox}


\begin{remark}
In endlichdimensionalen Vektorräumen sind alle Normen äquivalent. Daher induzieren die zugehörigen Metriken dieselbe Topologie: im $\mathbb{R}^n$ und $\mathbb{C}^n$ wird diese die \textbf{euklidische Topologie} oder auch die \textbf{Standardtopologie} des $\mathbb{R}^n$ und $\mathbb{C}^n$ genannt.
\end{remark}

\section{Kompaktheit und Parakompaktheit}

Kompaktheit und Parakompaktheit sind zentrale Begriffe der Topologie und spielen später insbesondere bei der Theorie der Mannigfaltigkeiten eine wichtige Rolle.

\begin{definition}
Sei $(X,\mathfrak{X})$ ein topologischer Raum. Eine Familie offener Mengen $(O_\alpha)_{\alpha\in I} \subseteq \mathfrak{X}$
heißt \textbf{offene Überdeckung} von $X$, falls
\begin{equation*}
X = \bigcup_{\alpha\in I} O_\alpha
\end{equation*}
\end{definition}

Eine offene Überdeckung heißt \textbf{endlich}, wenn es endlich viele Indizes
$\alpha_1,\dots,\alpha_N\in I$ gibt mit
\begin{equation*}
X = O_{\alpha_1}\cup \dots \cup O_{\alpha_N}
\end{equation*}

\begin{definition}
Ein topologischer Raum $(X,\mathfrak{X})$ heißt \textbf{kompakt}, wenn gilt: zu jeder offenen Überdeckung von $X$ existiert eine endliche Subüberdeckung/Teilüberdeckung. Das heißt
\begin{equation*}
\forall (O_\alpha)_{\alpha\in I} \subseteq \mathfrak{X} \text{ mit } X = \bigcup_{\alpha\in I} O_\alpha \ \exists \, \mathcal{O}_{\alpha_1}, ..., \mathcal{O}_{\alpha_N} \in \mathfrak{X}: \ X=O_{\alpha_1}\cup \dots \cup O_{\alpha_N}
\end{equation*}
\end{definition}

\begin{figure}[H]
\centering
\includegraphics[width=0.3\textwidth]{Endlüberdeck.jpg}
\caption{$\bigcup_{i=1}^3\mathcal{O}_i$ ist eine endliche Teilüberdeckung von X}
\end{figure}

\begin{remark}
In der Literatur findet man die Definition der Kompaktheit teils auch mit der Hausdorff-Bedingung.
\end{remark}

\begin{remark}
Im $\mathbb{R}^n$ gilt: Eine Teilmenge $M\subseteq\mathbb{R}^n$ ist genau dann kompakt, wenn sie abgeschlossen und beschränkt ist (Satz von Heine-Borel). Insbesondere ist $\mathbb{R}^n$ selbst nicht kompakt, während z.B. die Oberfläche einer Kugel kompakt ist.
\end{remark}

\begin{definition}
Seien $(O_\alpha)_{\alpha\in I}$ und $(O_\beta)_{\beta\in J}$ offene
Überdeckungen von $X$.
Die Überdeckung $(O_\beta)_{\beta\in J}$ heißt \textbf{Verfeinerung} von
$(O_\alpha)_{\alpha\in I}$, falls
\begin{equation*}
\forall \beta\in J\ \exists \alpha\in I:\ O_\beta \subseteq O_\alpha
\end{equation*}
\end{definition}

\begin{figure}[H]
\centering
\includegraphics[width=0.3\textwidth]{Verfeinerung.jpg}
\caption{Offene Überdeckung $X=\bigcup_{i=1}^3\mathcal{O}_i$; Verfeinerung $X=\bigcup_{i=1}^5\mathcal{O}_i$, \newline 
denn: 
$\mathcal{O}_1 \subseteq \mathcal{O}_1, \ \mathcal{O}_2 \subseteq \mathcal{O}_2, \ \mathcal{O}_3 \subseteq \mathcal{O}_3, \ \mathcal{O}_4 \subseteq \mathcal{O}_1, \ \mathcal{O}_5 \subseteq \mathcal{O}_2$}
\end{figure}

\begin{definition}
Eine offene Überdeckung $(O_\alpha)_{\alpha\in I}$ von $X$ heißt
\textbf{lokal endlich}, wenn zu jedem Punkt $x\in X$ eine Umgebung $U(x)$
existiert, die nur endlich viele Mengen der Überdeckung schneidet, d.h.
\begin{equation*}
\forall x \in X \ \exists \, U(x) \in \mathfrak{X} \text{ mit } x \in U(x): \ |\{\alpha\in I \mid O_\alpha \cap U(x) \neq \emptyset \}| < \infty
\end{equation*}
\end{definition}

\begin{definition}
Ein topologischer Raum $(X,\mathfrak{X})$ heißt \textbf{parakompakt}, wenn zu jeder offenen Überdeckung von $X$ eine lokal endliche Verfeinerung existiert.
\end{definition}

\begin{remark}
Parakompaktheit ist eine schwächere Eigenschaft als Kompaktheit.
Insbesondere gilt:
\begin{equation*}
\text{kompakt} \;\Rightarrow\; \text{parakompakt}
\end{equation*}
Metrische Räume sind parakompakt.
\end{remark}



\chapter{Mannigfaltigkeiten}\label{chap:Mannigfaltigkeiten}
\section{Der Begriff der Mannigfaltigkeit}

In der Allgemeinen Relativitätstheorie wird Gravitation durch gekrümmte Räume beschrieben, die mathematisch als Mannigfaltigkeiten modelliert werden.

\begin{definition}
Eine \textbf{(topologische) Mannigfaltigkeit} ist ein topologischer Raum $(M,\mathfrak{M})$ mit den folgenden Eigenschaften:
\begin{itemize}
    \item $M$ ist ein Hausdorff-Raum \qquad (Folgen haben eindeutigen Grenzwert)
    \item $M$ ist parakompakt \qquad (um lokale Begriffe zu globalisieren)
    \item $M$ ist lokal homöomorph zum $(\mathbb{R}^n, \mathfrak{R})$ \qquad ("dem $\mathbb{R}^n$ sehr ähnlich" / sieht im Herangezoomten aus wie der $\mathbb{R}^n$)
\end{itemize}
Die Zahl $n$ heißt dann die \textbf{Dimension} der Mannigfaltigkeit $M$.
\end{definition}

Die \textbf{lokale Homöomorphie} zum $\mathbb{R}^n$ bedeutet, dass für jeden Punkt $p\in M$ eine offene Umgebung existiert, die (mit der von $M$ induzierten Spurtopologie) homöomorph zum euklidischen Raum ist:
\begin{equation*}
\forall p\in M\ \exists\, \mathcal{O}_p \in \mathfrak{M} \text{ mit } p\in \mathcal{O}_p: \
(\mathcal{O}_p,\mathfrak{M} \cap{\mathcal{O}_p}) \simeq (\mathbb{R}^n,\mathfrak{R})
\end{equation*}

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{Lochom.jpg}
\caption{Homöomorphismus $\phi$}
\end{figure}

Wir können also die ganze Mannigfaltigkeit mit offenen Mengen überdecken, die homöomorph zum $\mathbb{R}^n$ sind.

\begin{remark}
In der Allgemeinen Relativitätstheorie betrachtet man in der Regel 4-dimensionale Mannigfaltigkeiten, wobei drei Dimensionen räumlich und eine zeitlich interpretiert werden.
\end{remark}

\begin{definition}
Das Paar $(\mathcal{O},\phi)$ bestehend aus einer offenen Menge $\mathcal{O} \in \mathfrak{M}$ und einem Homöomorphismus
\begin{equation*}
\phi : \mathcal{O} \to \phi(\mathcal{O}) \subseteq \mathbb{R}^n, \quad \phi(p)=(x^1(p),\dots,x^n(p))
\end{equation*}
heißt \textbf{Karte} der Mannigfaltigkeit $M$.

Da $\phi$ ein Homöomorphismus ist, besitzt die Abbildung eine stetige Inverse
\begin{equation*}
\phi^{-1} : \phi(\mathcal{O}) \subseteq \mathbb{R}^n \to \mathcal{O}
\end{equation*}
\end{definition}

\begin{definition}
Eine Familie von Karten
\begin{equation*}
(\mathcal{O}_\alpha,\phi_\alpha)_{\alpha\in I}
\end{equation*}
heißt ein \textbf{Atlas} der Mannigfaltigkeit $M$, falls gilt:
\begin{equation*}
M = \bigcup_{\alpha\in I} \mathcal{O}_\alpha
\end{equation*}
\end{definition}

Eine Mannigfaltigkeit kann sich somit also eine Ansammlung von $\mathbb{R}^n$s vorstellen.

\begin{example}
Die Erdoberfläche ist eine 2-dimensionale Mannigfaltigkeit. Jede Region $\mathcal{O}_i$ der gekrümmten Erdoberfläche kann mit einer Karte $(\mathcal{O}_i,\phi_i)$ auf eine Ebene $\phi_i(\mathcal{O}_i) \subseteq \mathbb {R}^2$ abgebildet werden. Ein Atlas ist die Sammlung all dieser Karten, bei der auch Überlappungen zugelassen werden.
\begin{figure}[H]
\centering
\includegraphics[width=1\textwidth]{Erde.jpg}
\caption{Die Erdoberfläche als Mannigfaltigkeit. Der gelbe Bereich auf der Kugel (links) ist die Überlappung $\mathcal{O}_1 \cap \mathcal{O}_2$, der gelbe Bereich rechts im $\mathbb{R}^2$ ist die Abbildung davon unter $\phi_1$ und $\phi_2$. Um zwischen $\phi_1(\mathcal{O}_1 \cap \mathcal{O}_2)$ und $\phi_2(\mathcal{O}_1 \cap \mathcal{O}_2)$ zu wechseln, also auf die andere Karte zu wechseln, benötigt es die \textit{Kartenwechselabbildung}. Diese beschreibt auf welcher anderen Seite im Atlas die Karte weiter geht.}
\end{figure}
\end{example}

Also sind Mannigfaltigkeiten eine Ansammlung von $\mathbb{R}^n$s, die auf geeignete Weise miteinander verklebt sind.  

\begin{example}
Weitere Beispiele für Mannigfaltigkeiten sind der $\mathbb{R}^n, \mathbb{C}^n$, die Kleinflasche und das Möbiusband.
\end{example}

\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Klein_bottle_1.jpg}
        \caption{Die Kleinflasche besitzt nur eine Seite und keine Innen- \textit{und} Außenseite. Sie besitzt auch keinen Rand.}
        {\footnotesize{Quelle: \url{https://de.laborxing.com/produkten/kleinsche-flasche}}}
    \end{subfigure}
    \hspace{1em}
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Möbius_Strip.jpg}
        \caption{Das Möbiusband besitzt ebenso nur eine Seite und keine Innen- \textit{und} Außenseite, aber hat einen Rand.}
        {\footnotesize{Quelle: \url{https://de.wikipedia.org/wiki/M%C3%B6biusband#/media/Datei:M%C3%B6bius_Strip.jpg}}}
    \end{subfigure}
\end{figure}
\vspace{1em}


\section{Kartenwechsel}
Überlappen sich zwei offene Umgebungen $\mathcal{O}_\alpha, \mathcal{O}_\beta \in \mathfrak{M}$, die homöomorph zum $\mathbb{R}^n$ sind, so wird dessen Durchschnitt durch verschiedene Karten abgebildet. Der Übergang zwischen diesen Darstellungen erfolgt über die \textbf{Kartenwechselabbildung}.

\begin{definition}
Für zwei Karten $(O_\alpha,\phi_\alpha)$ und $(O_\beta,\phi_\beta)$ mit
$O_\alpha\cap O_\beta\neq\emptyset$ ist der \textbf{Kartenwechsel}
\begin{equation*}
\psi_{\beta\alpha}
:= \phi_\beta \circ \phi_\alpha^{-1}
: \phi_\alpha(O_\alpha\cap O_\beta) \subseteq \mathbb{R}^n
\to \phi_\beta(O_\alpha\cap O_\beta) \subseteq \mathbb{R}^n
\end{equation*}
\end{definition}

Der inverse Kartenwechsel ist gegeben durch
\begin{equation*}
\psi_{\alpha\beta} := \phi_\alpha \circ \phi_\beta^{-1}
\end{equation*}
und es gilt
\begin{equation*}
\psi_{\alpha\beta}\circ\psi_{\beta\alpha} = \mathrm{id}_{\phi_\alpha(O_\alpha\cap O_\beta)}
\qquad
\psi_{\beta\alpha}\circ\psi_{\alpha\beta} = \mathrm{id}_{\phi_\beta(O_\alpha\cap O_\beta)}
\end{equation*}

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{Kartenw1.jpg}
\caption{Kartenwechsel zwischen zwei Karten}
\end{figure}

Seien drei offenen Mengen $\mathcal{O}_\alpha, \mathcal{O}_\beta, \mathcal{O}_\gamma \in \mathfrak{M}$, die homöomorph zum $\mathbb{R}^n$ sind, mit
\begin{equation*}
\mathcal{O}_\alpha \cap \mathcal{O}_\beta \cap \mathcal{O}_\gamma \neq \emptyset,
\end{equation*}
so können drei Kartenwechsel definiert werden:
\begin{align*}
\psi_{\beta\alpha} &= \phi_\beta \circ \phi_\alpha^{-1}, \\
\psi_{\gamma\beta} &= \phi_\gamma \circ \phi_\beta^{-1}, \\
\psi_{\gamma\alpha} &= \phi_\gamma \circ \phi_\alpha^{-1}.
\end{align*}

\begin{proposition}[Dreifachdurchschnittsbedingung]
Das Diagramm der Kartenwechsel kommutiert, d.h.
\begin{equation}\label{eq:dreifach}
\psi_{\gamma\alpha}
= \psi_{\gamma\beta} \circ \psi_{\beta\alpha}
\end{equation}
\end{proposition}

\begin{proof}
Es gilt
\begin{equation*}
\psi_{\gamma\beta} \circ \psi_{\beta\alpha}
= (\phi_\gamma \circ \phi_\beta^{-1}) \circ (\phi_\beta \circ \phi_\alpha^{-1})
\overset{\text{assoziativ}}{=} \phi_\gamma \circ \underbrace{(\phi_\beta^{-1}\circ\phi_\beta)}_{=\mathrm{id}} \circ \phi_\alpha^{-1}
= \phi_\gamma \circ \phi_\alpha^{-1}
= \psi_{\gamma\alpha}
\end{equation*}
\end{proof}

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{Kartenw2.jpg}
\caption{Kartenwechsel zwischen drei Karten}
\end{figure}

Diese Dreifachdurchschnittsbedingung lässt sich auf beliebige Mehrfachüberlappungen verallgemeinern.

Die Vierfachdurchschnittsbedingung ist $\psi_{\alpha \delta} = \psi_{\alpha \beta} \circ ( \psi_{\beta \gamma} \circ \psi_{\gamma \delta})$. Diese folgt direkt aus der Dreifachdurchschnittsbedingung:
\begin{equation*}
\psi_{\alpha \beta} \circ ( \psi_{\beta \gamma} \circ \psi_{\gamma \delta})
\overset{\eqref{eq:dreifach}}{=} \psi_{\alpha \beta} \circ \psi_{\beta \delta} 
\overset{\eqref{eq:dreifach}}{=} \psi_{\alpha \delta}
\end{equation*}

Das kann induktiv weitergeführt werden zur Mehrfachdurchschnittsbedingung
\begin{equation*}
\psi_{\alpha_0\alpha_k}=\psi_{\alpha_0\alpha_1}\circ\psi_{\alpha_1\alpha_2}\circ\dots\circ\psi_{\alpha_{k-1}\alpha_k}
\end{equation*}

\begin{remark}
Eine Karte beschreibt, \textit{wie} $\mathcal{O}$ im $\mathbb{R}^n$ dargestellt wird. Beispielsweise durch kartesische Koordinaten $\phi_\alpha(p) = (x, y, z)$ oder durch Zylinderkoordinaten $\phi_\beta(p) = (r, \phi, z)$. Der Kartenwechsel $\psi_{\alpha \beta}$ ist dann die Transformation von den Zylinderkoordinaten in kartesische Koordinaten: $\psi_{\alpha \beta}(r,\phi,z)=(r\cdot \mathrm{cos}\phi,r \cdot \mathrm{sin}\phi, z)=(x,y,z)$.
\end{remark}

\underline{Kartenwechsel in Koordinaten}\newline
Sei $p \in \mathcal{O}_\alpha \cap \mathcal{O}_\beta$. Dann besitzt $p$ bezüglich der Karten $(\mathcal{O}_\alpha,\phi_\alpha)$ und
$(\mathcal{O}_\beta,\phi_\beta)$ die Koordinatendarstellungen
\begin{equation*}
x_\alpha^i := \phi^i_\alpha(p) \in \mathbb{R}^n \ \text{ und } \
x_\beta^i := \phi^i_\beta(p) \in \mathbb{R}^n
\end{equation*}
Obwohl $x_\alpha$ und $x_\beta$ im Allgemeinen verschieden sind, repräsentieren beide denselben Punkt der Mannigfaltigkeit
\begin{equation*}
\phi^{-1}_\alpha(x_\alpha^i) = \phi^{-1}_\beta(x_\beta^i) = p \in M
\end{equation*}

Zwischen diesen Koordinaten besteht eine bijektive Beziehung, gegeben durch die Kartenwechselabbildungen
\begin{equation*}
x_\alpha^i = \psi^i_{\alpha\beta}(x_\beta^k) = (\phi_\alpha \circ \phi_\beta^{-1})^i(x_\beta^k)
 \ \text{ und } \
x_\beta^i = \psi^i_{\beta\alpha}(x_\alpha^k) = (\phi_\beta \circ \phi_\alpha^{-1})^i(x_\alpha^k)
\end{equation*}

Statt der Kartenwechselabbildung (Koordinatenwechselfunktion) können die Koordinaten $x_\alpha^i$ auch als Funktion der Koordinaten einer anderen Karte $x_\beta^k$ aufgefasst werden
\begin{align*}
x_\beta^k(x_\alpha^i):= \psi_{\beta\alpha}^k(x_\alpha^i)=x_\beta^k \\
x_\alpha^k(x_\beta^i):= \psi_{\alpha\beta}^k(x_\beta^i)=x_\alpha^k
\end{align*}

\begin{definition}\label{def:kartekoord}
Für zwei Karten $(O,\phi)$ und $(\tilde{O},\tilde{\phi})$ mit $O\cap \tilde{O}\neq\emptyset$ ist der \textbf{Kartenwechsel in Koordinatendarstellung} gegeben durch
\begin{equation*}
\psi: \phi(O \cap \tilde{O}) \subseteq \mathbb{R}^n
\to \tilde{\phi}(O \cap \tilde{O}) \subseteq \mathbb{R}^n : x^k \mapsto \tilde{x}^i(x^k):= \psi^i(x^k)=\tilde{x}^i
\end{equation*}
\end{definition}

\section{Skalarfelder auf Mannigfaltigkeiten}
Da wir nun Mannigfaltigkeiten kennen, können wir Skalarfelder definieren. 

\begin{definition}
Sei $(M,\mathfrak{M})$ eine Mannigfaltigkeit.
Eine Abbildung
\begin{equation*}
F : M \to \mathbb{R}
\end{equation*}
heißt \textbf{reellwertiges Skalarfeld} auf $M$.
\end{definition}

Skalarfelder bilden also jeden Punkt $p$ einer Mannigfaltigkeit auf einen \textit{Skalar} $F(p)$ ab. Dabei kann es sich beispielsweise um ein Temperaturfeld handeln, das jedem Punkt im Raum eine Temperatur zuweist.

Ein Skalarfeld $F$ ist zunächst nur als abstrakte Abbildung auf der Mannigfaltigkeit definiert. Um ein Skalarfeld rechnerisch zu untersuchen, betrachten wir daher seine Darstellung in lokalen Koordinaten, die durch die Karten der Mannigfaltigkeit gegeben sind.

\begin{definition}
Sei $(M,\mathfrak{M})$ eine Mannigfaltigkeit, $F : M \to \mathbb{R}$ ein Skalarfeld und $(\mathcal{O}_\alpha,\phi_\alpha)$ eine Karte der Mannigfaltigkeit.
Dann heißt die numerische Funktion
\begin{equation*}
f_\alpha := F \circ \phi_\alpha^{-1}
: \phi_\alpha(\mathcal{O}_\alpha) \subseteq \mathbb{R}^n \to \mathbb{R}
\end{equation*}
\textbf{lokaler Repräsentant} oder auch \textbf{lokale Darstellung} von $F$ in der $\alpha$-Karte.
\end{definition}

$f_\alpha$ ist keine Koordinatentransformation, sondern die Darstellung eines Skalarfeldes in den Koordinaten der Karte $(\mathcal{O}_\alpha,\phi_\alpha)$. Ob diese Koordinaten kartesisch, zylindrisch oder sphärisch sind, hängt allein von der gewählten Karte ab.

Entsprechend erhält man zu einem Atlas $(\mathcal{O}_\alpha,\phi_\alpha)_{\alpha\in I}$
eine Familie numerischer Funktionen
\begin{equation*}
(f_\alpha)_{\alpha\in I} \ 
\text{ mit }
\ f_\alpha : \mathbb{R}^n \to \mathbb{R}
\end{equation*}

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{Skalarfeld.jpg}
\end{figure}

Sollen lokale Darstellungen $(f_\alpha)_{\alpha\in I}$ ein gemeinsames (globales) Skalarfeld $F$ beschreiben, dürfen sie nicht unabhängig voneinander gewählt werden. Überlappen sich zwei Karten $\mathcal{O}_\alpha$ und $\mathcal{O}_\beta$, so müssen die entsprechenden Funktionen auf dem Überlappungsbereich zueinander passen.

\begin{proposition}[Kompatibilitätsbedingung]
Seien $(\mathcal{O}_\alpha,\phi_\alpha)$ und $(\mathcal{O}_\beta,\phi_\beta)$ zwei Karten mit $\mathcal{O}_\alpha \cap \mathcal{O}_\beta \neq \emptyset$. Seien $f_\alpha$ und $f_\beta$ lokale Darstellungen desselben Skalarfeldes $F$. Dann gilt
\begin{equation}\label{eq:komp}
\forall x \in \phi_\beta(\mathcal{O}_\alpha \cap \mathcal{O}_\beta): \
f_\beta(x) = (f_\alpha \circ \psi_{\alpha\beta})(x)
\end{equation}
wobei $\psi_{\alpha\beta} := \phi_\alpha \circ \phi_\beta^{-1}$
\end{proposition}
\begin{proof}
Dies geht aus der Definition der lokalen Darstellung hervor. \newline
Sei $x \in \phi_\beta(\mathcal{O}_\alpha \cap \mathcal{O}_\beta)$. Dann ist
\begin{equation*}
f_\beta(x) = (F \circ \phi_\beta^{-1})(x) = (F \circ \underbrace{(\phi_\alpha^{-1} \circ \phi_\alpha)}_{=\mathrm{id}} \circ \phi_\beta^{-1})(x) = ((F \circ \phi_\alpha^{-1}) \circ (\phi_\alpha \circ \phi_\beta^{-1}))(x) = (f_\alpha \circ \psi_{\alpha\beta})(x)
\end{equation*}
\end{proof}
Die beiden lokalen Darstellungen $f_{\alpha}$ und $f_{\beta}$ beschreiben denselben physikalischen/geometrischen Wert, nur in verschiedenen Koordinaten, da sie vom selben globalen Skalarfeld $F$ stammen. 

\subsubsection*{Skalarfelder sind kartenunabhängig (koordinatenunabhängig)}
Die lokale Darstellung $f_\alpha$ eines Skalarfeldes $F$ erlaubt es, den abstrakten Begriff einer Funktion auf einer Mannigfaltigkeit auf bekannte numerische Funktionen zurückzuführen. Dabei ist zu beachten, dass ein Punkt $p \in M$ je nach Karte durch unterschiedliche Koordinaten beschrieben wird. 

Sei $p \in \mathcal{O}_\alpha \cap \mathcal{O}_\beta$. Dann besitzt $p$ bezüglich der Karten $(\mathcal{O}_\alpha,\phi_\alpha)$ und
$(\mathcal{O}_\beta,\phi_\beta)$ die Koordinatendarstellungen, die mit der Kartenwechselabbildung (Koordinatenwechselfunktion) \cref{def:kartekoord} ineinander umgerechnet werden können
\begin{align*}
x_\beta^k(x_\alpha^i)=x_\beta^k \\
x_\alpha^k(x_\beta^i)=x_\alpha^k
\end{align*}

Die Kompatibilitätsbedingung der lokalen Darstellungen stellt sicher, dass die numerischen Funktionen $f_\alpha$ und $f_\beta$ denselben Funktionswert liefern, wenn sie auf Koordinaten angewandt werden, die demselben Punkt $p$ entsprechen:
\begin{equation}\label{eq:numfunk}
f_\alpha(x_\alpha^i)
= f_\alpha(\psi_{\alpha\beta}^i(x_\beta^k))
= (f_\alpha \circ \psi_{\alpha\beta})(x_\beta^k)
\overset{\eqref{eq:komp}}{=} f_\beta(x_\beta^k)
\end{equation}

Dies entspricht der physikalischen Forderung, dass eine skalare Größe – etwa eine Temperatur – unabhängig von der gewählten Karte denselben Wert besitzt. Die numerischen Funktionen hängen von der gewählten Karte (Koordinatendarstellung) ab, der physikalische Wert jedoch nicht.

\underline{Rekonstruktion des Skalarfeldes aus der lokalen Darstellung}\newline
Umgekehrt kann zu gegebenen lokalen Darstellungen $(f_\alpha)_{\alpha\in I}, \ f_\alpha : \mathbb{R}^n \to \mathbb{R}$ das zugehörige Skalarfeld $F$ gewonnen werden.

Für einen Punkt $p \in M$ existiert eine Karte $(\mathcal{O}_\alpha,\phi_\alpha)$ mit $p \in \mathcal{O}_\alpha$, sodass wir definieren können
\begin{equation*}
F(p) := f_\alpha(\phi_\alpha^i(p)) = f_\alpha(x_\alpha^i)
\end{equation*}
Liegt der Punkt $p$ zusätzlich in einer weiteren Karte $(\mathcal{O}_\beta,\phi_\beta)$, so können wir ebenso definieren
\begin{equation*}
F(p) := f_\beta(\phi_\beta^i(p)) = f_\beta(x_\beta^i) \overset{\eqref{eq:numfunk}}{=}(f_\alpha \circ \psi_{\alpha\beta})(x_\beta^i)=f_\alpha(x_\alpha^i)
\end{equation*}

Da per Definition der Mannigfaltigkeit für jeden Punkt $p \in M$ eine Karte existiert, gilt obiges für alle $p\in M$. Der Wert des Skalarfeldes ist somit unabhängig von der gewählten Karte und eindeutig durch die lokalen Darstellungen bestimmt.

Diese Konstruktion ist analog zur Darstellung eines Vektors in verschiedenen Basen: Die einzelnen Komponenten hängen von der gewählten Basis ab, der Vektor selbst, als abstraktes Objekt, jedoch nicht. Ebenso hängen die lokalen Darstellungen $f_\alpha$ von der Wahl der Karte ab, während das Skalarfeld $F$ kartenunabhängig ist.

Mit der Kartenwechselabbildung (Koordinatenwechselfunktion) \eqref{def:kartekoord} kann \eqref{eq:numfunk} nun folgendermaßen dargestellt werden
\begin{equation*}
f_\alpha(x_\alpha^i) = f_\beta(x_\beta^j) = f_\beta(x_\beta^j(x_\alpha^i))
\end{equation*}

\begin{definition}
Eine Mannigfaltigkeit heißt \textbf{$C^k$-differenzierbare Mannigfaltigkeit}, wenn alle Kartenwechselabbildungen $\psi$ $k$-mal stetig differenzierbar sind, d.h. $\psi \in C^k$.

Eine Mannigfaltigkeit heißt \textbf{glatte Mannigfaltigkeit} oder \textbf{$C^\infty$-Mannigfaltigkeit}, wenn alle Kartenwechselabbildungen unendlich oft stetig differenzierbar sind, d.h. $\psi \in C^\infty$.
\end{definition}

Wir fordern hier glatte Mannigfaltigkeiten, da andernfalls auch die lokalen Darstellungen von Skalarfeldern nur $C^k$ wären, da sie über
\begin{equation*}
f_\alpha = F \circ \phi_\alpha^{-1}
\end{equation*}
definiert sind. D.h. das Skalarfeld wäre lokal nur $k$-mal stetig differenzierbar.

Als anschauliches Beispiel für ein Skalarfeld betrachten wir eine Wetterkarte. Bei einer solchen wird jedem Punkt eines zweidimensionalen Raumes (der Landkarte) ein Skalar zugeordnet, beispielsweise die Temperatur oder der Luftdruck. Formal handelt es sich dabei um eine Abbildung, die jedem Punkt einen reellen Wert zuweist.

Im Alltag kann eine Wetterkarte meist sinnvoll interpretiert werden, ohne die gekrümmte Gestalt der Erdoberfläche, also die zweidimensionale Mannigfaltigkeit, zu berücksichtigen. 

\subsection{Skalarfelder und Karten am Beispiel der stereographischen Projektion}
Als Beispiel betrachten wir die Kugeloberfläche $S^2$ (zweidimensionale Mannigfaltigkeit).
Zur Beschreibung von $S^2$ genügen bereits zwei Karten, die gemeinsam einen Atlas bilden: die Nordpolprojektion und die Südpolprojektion.

Jede dieser Projektionen bildet alle Punkte der Kugeloberfläche, mit Ausnahme des jeweiligen Projektionspols, auf eine Ebene ab, die am gegenüberliegenden Pol tangential anliegt.

Die Abbildung erfolgt dabei folgendermaßen: Die direkte Verbindungslinie zwischen dem Projektionspol und einem Punkt $p\in S^2$ wird so weit verlängert, bis sie die entsprechende Tangentialebene schneidet. Der Schnittpunkt ist das Bild von $p$ in der Ebene.

Damit gilt:
\begin{itemize}
\item Die Nordpolprojektion $\phi_N$ (vom Norpol ausgehend) bildet $S^2\setminus\{N\}$ auf eine Ebene in $\mathbb{R}^2\setminus\{N\}$ ab.
\item Die Südpolprojektion $\phi_S$ (vom Südpol ausgehend) bildet $S^2\setminus\{S\}$ auf eine Ebene in $\mathbb{R}^2\setminus\{S\}$ ab.
\end{itemize}

Zusammen beschreiben diese beiden Karten die gesamte Mannigfaltigkeit $S^2$ und bilden somit einen Atlas.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{images/Proj3d.jpg}
\caption{stereographische Projektion in 3D Ansicht}
{\footnotesize{Quelle: ART Mitschrift Berndl-Forstner (WS20/21) \url{https://forum.fstph.at/t/latex-skriptum-allg-relativitaetstheorie/4021}}}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{images/Proj2d.jpg}
\caption{stereographische Projektion in 2D Ansicht}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.6\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/Südpolproj.jpg}
        \caption{Südpolprojektion}
        {\footnotesize{Quelle: \url{https://www.geogebra.org/m/d6jucfnc}}}        
    \end{subfigure}
    \hspace{1em}
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/Nordpolproj.jpg}
        \caption{Nordpolprojektion}
    \end{subfigure}
    \caption{Projektion der Erde}
    \label{fig:erdproj}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.4\textwidth]{images/Lichtproj.jpg}
\caption{Nordpolprojektion mithilfe von Licht}
\end{figure}

\underline{Herleitung der Gestalt der Kartenwechselabbildung} \newline
Sei $p\in S^2$. Dann ist $\phi_N(p)=x^i \in \mathbb{R}^2 \setminus \{N\}$ und $\phi_S(p)=\tilde{x}^i \in \mathbb{R}^2 \setminus \{S\}$. Für die Koordinaten von $x^i$ und $\tilde{x}^i$ verwenden wir Polarkoordinaten. Dabei ist der Winkel (Längengrad) $\varphi$ für beide Projektionen identisch. Der Unterschied zwischen den beiden Darstellungen liegt ausschließlich im radialen Abstand $\rho$ bzw. $\tilde{\rho}$, zum Süd- bzw. Nordpol, welchen wir als Koordinatenursprung für die jeweilige Ebene festlegen. Um die Rechnungen zu vereinfachen, setzen wir den Durchmesser der Kugel auf $d=1$.

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{images/Nordpolproj2.jpg}
\caption{Die Position eines Punktes $x^i$ in der projizierten Ebene wird durch den Winkel $\varphi$ und Radius $\rho$ beschrieben: $\rho \cdot e_\rho^i(\varphi)=x^i$}
\end{figure}

Aus der Geometrie ergibt sich
\begin{equation*}
\tan \alpha = \frac{\rho}{d} = \frac{\rho}{1} = \rho, \qquad \tan \beta = \tilde{\rho}
\end{equation*}

\begin{minipage}[t]{0.83\textwidth}
Da $p$ auf der Kugeloberfläche liegt, liegt dieser Punkt auf einem Thaleskreis (verbindet man einen bel. Punkt auf einem Halbkreis mit dessen Endpunkten schließen die beiden Strecken einen rechten Winkel ein). Daher stehen die Verbindungsstrecken $\overline{Np}$ und $\overline{Sp}$ im rechten Winkel aufeinander. Da die Summe der Winkel in einem Dreieck $180^{\circ}=\pi$ beträgt, ist somit $\beta = \pi - \frac{\pi}{2}-\alpha = \frac{\pi}{2} - \alpha$. Somit folgt

\begin{equation}\label{eq:rho}
\tilde{\rho}
= \tan \beta
= \tan\!\left(\frac{\pi}{2} - \alpha\right)
= \frac{1}{\tan \alpha}
= \frac{1}{\rho}
\end{equation}
\end{minipage}\hfill
\begin{minipage}[t]{0.15\textwidth}
\centering
\vspace{-2em}
\includegraphics[width=\textwidth]{images/Thaleskreis.jpg}
\small Thaleskreis
\vspace{1em}

\includegraphics[width=\textwidth]{images/Thaleskreis1.jpg}
\end{minipage}

Sei $e_\rho^i(\varphi) = (\cos\varphi,\sin\varphi)$ der Einheitsvektor in radialer Richtung im $\mathbb{R}^2$. Dann sind die Koordinaten des Bildpunktes bezüglich der Nordpolprojektion
\begin{equation*}
\phi_N(p) = x^i = \rho\, e_\rho^i(\varphi)
\end{equation*}

Es folgt die Kartenwechselabbildung zwischen Nord- und Südpolprojektion
\begin{equation*}
x^i = \rho\, e_\rho^i(\varphi) \overset{\eqref{eq:rho}}{=}
\frac{1}{\tilde{\rho}}\, e_\rho^i(\varphi) \overset{\frac{1}{\tilde{\rho}} \cdot \tilde{x}^i = e_{\tilde{\rho}}^i(\varphi)}{=}
\frac{1}{\tilde{\rho}^2}\, \tilde{x}^i \overset{\substack{\tilde{\rho}^2=|\tilde{x}^m|^2 \\ =\tilde{x}^m\tilde{x}^m}}{=} 
\frac{1}{\tilde{x}^m\tilde{x}^m} \tilde{x}^i 
\qquad \tilde{x}^i  \in \mathbb{R}^2\setminus\{0\}.
\end{equation*}

und
\begin{equation*}
\tilde{x}^i
= \frac{1}{x^m x^m}\, x^i,
\qquad x^i \in \mathbb{R}^2\setminus\{0\}.
\end{equation*}

Zusammengefasst: Auf dem Überlappungsbereich der beiden Karten sind die Koordinaten durch die Kartenwechselabbildung miteinander verknüpft. Da dort $\tilde{\rho} = \tfrac{1}{\rho}$ gilt, können die beiden Darstellungen ineinander umgerechnet werden.

\underline{Darstellung eines Skalarfeldes} \newline
Wir betrachten nun ein Skalarfeld $F:S^2 \to \mathbb{R}$ auf der Mannigfaltigkeit $S^2$.
Bezüglich der Nordpolprojektion $\phi_N$ sei das Skalarfeld lokal dargestellt durch
\begin{equation*}
f:= F \circ \phi_N^{-1} : \mathbb{R}^2 \to \mathbb{R},
\qquad
f(x^i) := \frac{1}{1 + \rho^2}
= \frac{1}{1 + x^i x^i}
\end{equation*}

Da es sich um ein Skalarfeld handelt, muss es auch bezüglich der anderen Karte darstellbar sein. Bezeichnet $\tilde{f}$ die lokale Darstellung bezüglich der Südpolprojektion $\phi_S$
\begin{equation*}
\tilde{f}:= F \circ \phi_S^{-1} : \mathbb{R}^2 \to \mathbb{R},
\qquad
\tilde{f}(x^i) := \frac{1}{1 + \tilde{\rho}^2}
= \frac{1}{1 + \tilde{x}^i \tilde{x}^i}
\end{equation*}

Dann muss für $p \in S^2$ gelten
\begin{equation*}
F(p) = \tilde{f}(\tilde{x}^i) = f(x^i)
\end{equation*}

und es kann die Darstellung des Skalarfeldes bezüglich der anderen Karten angeschrieben werden
\begin{equation*}
\tilde{f}(\tilde{x}^i) = f(x^i) = \frac{1}{1 + \rho^2} \overset{\eqref{eq:rho}}{=} \frac{1}{1 + \frac{1}{\tilde{\rho}^2}}
= \frac{\tilde{\rho}^2}{1 + \tilde{\rho}^2}
= \frac{\tilde{x}^i \tilde{x}^i}{1 + \tilde{x}^i \tilde{x}^i}
\end{equation*}

Für $\rho \to \infty$ geht die Funktion $f(x^i)=\tfrac{1}{1+\rho^2}$ gegen $0$. Dieser Grenzfall entspricht geometrisch dem Annähern an den Nordpol. (Der Nordpol ist hier wie der Punkt im unendlichen bei der Riemannschen Zahlenkugel.)

Da die Nord- und Südpolprojektion zusammen einen Atlas der Kugeloberfläche bilden, existiert ein globales Skalarfeld $F$ auf $S^2$,
dessen lokale Darstellungen durch $f$ und $\tilde{f}$ gegeben sind.

Die Mannigfaltigkeit muss nicht als eingebettetes Objekt im $\mathbb{R}^3$ aufgefasst werden. Die Beschreibung durch Karten und Kartenwechsel enthält bereits die vollständige geometrische Information über das Skalarfeld auf $S^2$.

Anhand der stereographischen Projektion erkennt man sehr deutlich, dass verschiedene Karten ein und dieselbe Mannigfaltigkeit in den $\mathbb{R}^n$ auf sehr unterschiedliche Weise abbilden können. Obwohl stets dieselbe Punktmenge der Mannigfaltigkeit $(M,\mathfrak{M})$ beschrieben wird, können die Bilder im Zielraum völlig verschieden aussehen. Vgl. \cref{fig:erdproj} bzw. auch die Tatsache, dass die Kartenwechselabbildung nicht linear ist: \newline
Wenn wir eine Gerade in $\mathbb{R}^2 \setminus{N}$ durch die Kartenwechselabbilung $\psi: x^i= \rho\ e^i_\rho(\varphi) \mapsto \tilde{x}^i= \frac{1}{\rho}\ e^i_\rho(\varphi)$ in $\mathbb{R}^2 \setminus{S}$ abbilden, wird diese Gerade gekrümmt.

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{images/Kartenwech1.jpg}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{images/Kartenwech2.jpg}
\end{figure}

Wenn wir diese Gerade nun ins Unendliche verlängern, wird der Abstand $\rho$ der Punkte auf der Gerade zu $S$ immer größer und somit $\tilde{\rho} = \frac{1}{\rho}$ immer kleiner. Es bildet sich in $\mathbb{R}^2 \setminus{S}$ ein Kreis. Beim Grenzübergang $\rho \to \infty$ gilt $lim_{\rho \to \infty} \tilde{\rho}= 0$, wir treffen auf den Nordpol. 

Das heißt, jede Gerade wird durch die Kartenwechselabbilung auf einen Kreis abgebildet, der durch den Pol verläuft. Außerdem werden Kurven auf Kurven abgebildet.

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{images/Kartenwech3.jpg}
\end{figure}

\section{Vektoren auf Mannigfaltigkeiten (Tangentialraum)}
Ziel dieser Sektion ist es, Vektorfelder bzw. allgemein Tensorfelder auf Mannigfaltigkeiten zu definieren. Vektorfelder bzw. Tensorfelder auf dem $\mathbb{R}^n$ bilden auf einen Vektorraum $V$ bzw. Tensorproduktraum $T^r_s$ ab
\begin{equation*}
v : \mathbb{R}^n \to V
\qquad
t : \mathbb{R}^n \to T^r_s
\end{equation*}
Wenn wir dies auf allgemeine Mannigfaltigkeiten übertragen möchten, benötigen wir einen Vektorraum bzw. Tensorraum auf der Mannigfaltigkeit, in den die Vektorfelder bzw. Tensorfelder abgebildet werden.

Wie wir anhand der stereographischen Projektion gesehen haben, ist die Kartenwechselabbilung im allgemeinen nicht linear. Somit wird die Vektorraumstruktur beim Kartenwechsel nicht erhalten. Es stellt sich nun die Frage, wie wir den Kartenwechsel linearisieren können. Der Kartenwechsel bildet Kurven auf Kurven ab. Wir betrachten nun mehrere Kurven, die durch einen fixierten Punkt $p\in M$ bzw. in Koordinatendarstellungen $x_0^i$ in einer Karte verlaufen und dessen Tangentialvektoren $v^i := \dot{x}_0^i$ in diesem Punkt. Wie sich herausstellen wird, spannen diese Tangentialvektoren einen Vektorraum auf, den \textit{Tangentialraum}. Wir erhalten also für jeden Punkt $p \in M$ einen Vektorraum. Das Vektorraumkonzept wird lokalisiert. 

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{images/Tpm.jpg}
\end{figure}

Sei nun $(M, \mathfrak{M})$ eine beliebige Mannigfaltigkeit. Wir betrachten eine Kurve $\gamma: I \to M$ mit $I \subset \mathbb{R}$ beliebiges Intervall, die bei $p=\gamma(0)$ differenzierbar ist. Wir wählen zwei Karten $(\mathcal{O},\phi)$, $(\tilde{\mathcal{O}},\tilde{\phi})$ mit $\gamma(I) \subset {\mathcal{O}} \cap \tilde{\mathcal{O}}$, dann erhält die Kurve in Koordinaten die Darstellung
\begin{equation*}
x^i(t) = \phi^i(\gamma(t)) \qquad 
\tilde{x}^i(t) = \tilde{\phi}^i(\gamma(t))
\end{equation*}

Dann ist der Tangentialvektor der Kurve in der jeweiligen Karte an $t=0$ gegeben durch:
\begin{equation*}
v^i := \left.\frac{d}{dt}\right|_{t=0} x^i(t) = \dot{x}^i(0)
\qquad
\tilde{v}^i := \left.\frac{d}{dt}\right|_{t=0} \tilde{x}^i(t) = \dot{\tilde{x}}^i(0)
\end{equation*}

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{images/Curve.jpg}
\end{figure}

Wir suchen nun nach einer linearen Beziehung zwischen $v^i$ und $\tilde{v}^i$.

Auf dem Überlappungsbereich hängen die Koordinaten über den Kartenwechsel zusammen. Wie in \cref{def:kartekoord} beschrieben können wir die \glqq neuen\grqq\ Koordinaten als Funktionen der \glqq alten\grqq\ auffassen:
\begin{equation*}\label{eq:kartenwechselkoord}
\tilde{x}^i(x^k):= \psi^i(x^k)=\tilde{x}^i \in \phi^k(\mathcal{O} \cap \tilde{\mathcal{O}})
\end{equation*}

Wenden wir nun die Kettenregel auf $\tilde{x}^i(t)=\tilde{x}^i(x^k(t))$ an, so erhalten wir:
\begin{equation}\label{eq:tangent_transform}
\tilde{v}^i
= \left.\frac{d}{dt}\right|_{t=0}\tilde{x}^i(t)
= \left.\frac{d}{dt}\right|_{t=0}\tilde{x}^i(x(t))
= \frac{\partial \tilde{x}^i}{\partial x^l}(\underbrace{x^k(0)}_{=:x_0^k})\ v^l
= \frac{\partial \tilde{x}^i}{\partial x^l}(x_0^k)\ v^l
\end{equation}

Die Matrix $\frac{\partial \tilde{x}^i}{\partial x^l}$ ist die \textbf{Jacobi-Matrix} des Kartenwechsels. Sie gibt an, wie sich $v^i$ unter einem Kartenwechsel verändert und ist \textbf{linear}. Somit ist diese Transformation linear in $v$. Sie hängt nur vom Punkt $p$ (bzw. $x_0$) ab, nicht von der konkreten Kurve $\gamma$. Wichtig: wir beschränken uns hier auf den einen Punkt $x_0^i$! Global bleibt der Kartenwechsel im Allgemeinen nichtlinear.

Es kann viele verschiedene Kurven geben, die durch denselben Punkt $p$ bzw. in Koordinaten $x_0^i$ laufen und dort dieselbe \glqq Anfangsrichtung\grqq\ $v^i$ besitzen. Wir fassen diese zusammen mit der folgenden \textbf{Äquivalenzrelation}:

\begin{equation*}
x_1^i(t) \sim x_2^i(t)  :\Leftrightarrow
\begin{cases}
x_1^i(0)=x_2^i(0) =: x_0^i \\
\dot{x}_1^i(0) = \dot{x}_2^i(0) = v^i
\end{cases}
\end{equation*}

Die Äquivalenzklasse ist dann $[x^i(t)]_\sim = \{y^i(t) \ | \ y^i(0)=x_0^i, \ \dot{y}^i(0)=v^i\}$.

Diese Äquivalenz ist \textbf{kartenunabhängig} und hängt nur von $p \in M$ ab. D.h. $x_1^i(t) \sim x_2^i(t) \, \Leftrightarrow \, \tilde{x}_1^i(t) \sim \tilde{x}_2^i(t)$. Das folgt aus dem Transformationsgesetz \eqref{eq:tangent_transform}: Wenn zwei Kurven in einer Karte denselben Tangentialvektor besitzen, dann haben sie wegen der linearen Jacobi-Transformation auch in jeder anderen Karte denselben Tangentialvektor:

Transformieren wir in $x_0^k$ die Tangentialvektoren $v_1^i = v_2^i$ zweier Kurven mit \eqref{eq:tangent_transform}
\begin{equation*}
\tilde{v}_1^i= \frac{\partial \tilde{x}^i}{\partial x^l}(x_0^k)\ v^l_1
\qquad
\tilde{v}_2^i= \frac{\partial \tilde{x}^i}{\partial x^l}(x_0^k)\ v^l_2
\end{equation*}
Da die Jacobi-Matrix nur von $x_0^k$ bzw. $p \in M$ abhängt, wird dieselbe Matrix auf $v_1^i = v_2^i$ angewandt und wir erhalten $\tilde{v}_1^i=\tilde{v}_2^i$. 

Koordinatenunabhängig geschrieben:
\begin{equation*}
\gamma_1(t) \sim \gamma_2(t)  :\Leftrightarrow
\begin{cases}
\gamma_1(0)=\gamma_2(0) \\
\dot{\gamma}_1(0) = \dot{\gamma}_2(0)
\end{cases}
\end{equation*}

\begin{definition}
Der \textbf{Tangentialraum} an einem Punkt $p\in M$ ist die Menge aller obiger Äquivalenzklassen von Kurven $\gamma$ durch $p$, die in $p$ differenzierbar sind:
\begin{equation*}
T_pM := \{[\gamma]_\sim \mid \gamma: I\to M \text{ differenzierbar in }p\in M, \ \gamma(0)=p\}
\end{equation*}
\end{definition}

\underline{Vektorraumstruktur des Tangentialraums}\newline
Damit der Tangentialraum zu einem Vektorraum wird, muss er bzgl. der Addition und der Skalarmultiplikation abgeschlossen sein. Zudem muss die Addition und die Skalarmultiplikation unabhängig von der Wahl der Karte sein. 

In einer fixierten Karte $(\mathcal{O},\phi)$ betrachten dazu zwei Kurven $\gamma_x$ und $\gamma_y$ mit Koordinaten $x^i(t)$ und $y^i(t)$ bzgl. dieser Karte, die beide durch den Punkt $\gamma_x(0)=\gamma_y(0)=p \in \mathcal{O} \subset M$ bzw. in Koordinaten $x^i_0$ verlaufen und in diesem Punkt die Tangentialvektoren 
\begin{align*}
v= [\gamma_x]_\sim\in T_pM
\text{ mit Komponenten } 
v^i := \dot{x}^i_0 \in \mathbb{R} \\
w= [\gamma_y]_\sim\in T_pM
\text{ mit Komponenten } 
w^i := \dot{y}^i_0 \in \mathbb{R}
\end{align*}
besitzen.

Um die Summe der beiden Tangentialvektoren zu definieren, konstruieren wir eine dritte Kurve $\gamma_z$ mit Koordinaten $z^i(t)$, die ebenso durch den Punkt $x^i_0$ verläuft
\begin{equation*}
z^i(t) := x_0^i + t\,(v^i + w^i) \qquad \text{(also eine Gerade)}
\end{equation*}

Diese Kurve besitzt den Tangentialvektor
\begin{equation*}
\dot{z}^i(0) = v^i + w^i
\end{equation*}

\begin{figure}[H]
\centering
\includegraphics[width=0.3\textwidth]{images/Tpmvek.jpg}
\end{figure}

Wir definieren die Summe $v+w := [\gamma_z] \in T_pM$ als die Äquivalenzklasse der Kurve $\gamma_z$.

Es bleibt zu zeigen, dass diese Definition unabhängig von der Wahl der Karte ist. Dazu betrachten wir eine zweite Karte $(\tilde{\mathcal{O}},\tilde{\phi})$ mit
$p\in\tilde{\mathcal{O}}$ und die entsprechenden Koordinatendarstellungen nach dem Kartenwechsel $\psi$ (siehe \eqref{eq:kartenwechselkoord})
\begin{align*}
\tilde{x}^i(t) = \psi^i(x^k(t))=\tilde{x}^i(x^k(t))\\
\tilde{y}^i(t) = \psi^i(y^k(t))=\tilde{y}^i(y^k(t))\\
\tilde{z}^i(t) = \psi^i(z^k(t))=\tilde{z}^i(z^k(t))
\end{align*}

Nach dem Transformationsgesetz \eqref{eq:tangent_transform} für Tangentialvektoren gilt
\begin{equation*}
\tilde{v}^i
= \frac{\partial \tilde{x}^i}{\partial x^k}(x_0^m)\, v^k,
\qquad
\tilde{w}^i
= \frac{\partial \tilde{x}^i}{\partial x^k}(x_0^m)\, w^k,
\end{equation*}
Für die Kurve $\gamma_z$ ergibt sich entsprechend
\begin{equation*}
\dot{\tilde{z}}^i(0)
= \frac{\partial \tilde{x}^i}{\partial x^k}(x_0^m)\,(v^k + w^k)
= \tilde{v}^i + \tilde{w}^i
\end{equation*}

Damit besitzt die Kurve $\tilde{z}^i(t)$ in der neuen Karte genau den Tangentialvektor, der der Summe der beiden transformierten Tangentialvektoren entspricht. Die Addition ist somit unabhängig von der Wahl der Karte.

Folglich ist die Addition von Tangentialvektoren eine koordinatenunabhängige Operation. Der Beweis für die Skalarmultiplikation wird mit $z^i(t):=x_0^i+(\lambda v^i)t$ geführt und geht analog.

Folglich ist der Tangentialraum $T_pM$ ein \textbf{Vektorraum}. Allerdings haben wir bisher eine \textbf{koordinaten- bzw. basisabhängige Darstellung} der Elemente von $T_pM$:\newline
$v\in T_pM$ (Tangentialvektor) $\Leftrightarrow \exists$ eine Kurve in Koordinaten $x^i(t)$ mit $\phi^i(p)=x^i(0)=: x_0^i$ für ein $p\in \mathcal{O} \subset M$ und $v^i=\dot{x}^i(0)$ und der Kartenwechseltransformation $\tilde v^i = \frac{\partial \tilde{x}^i}{\partial x^k}(x_0^m)\,v^k$.
\vspace{1em}

\underline{Koordinanten- bzw. basisunabhängige Darstellung vom Tangentialraum}\newline
Schreiben wir den Tangentialvektor mit Hilfe eines abstrakten Index $a$, so ist der Vektor selbst unabhängig von der Wahl der Karte. Bezüglich einer Karte $(\mathcal{O},\phi)$ lässt sich $v^a$ als Linearkombination von Komponenten $v^i$ und Basiselementen $E_i^a$ darstellen:
\begin{equation}\label{eq:tpmabstrakt}
v^a = v^i E_i^a
\end{equation}
In einer anderen Karte $(\tilde{\mathcal{O}},\tilde{\phi})$ gilt entsprechend
\begin{equation*}
v^a = \tilde{v}^i \tilde{E}_i^a
\end{equation*}

Die Komponenten eines Tangentialvektors transformieren beim Kartenwechsel gemäß \eqref{eq:tangent_transform}
\begin{equation*}
\tilde{v}^i = \frac{\partial \tilde{x}^i}{\partial x^k}(x_0^m)\,v^k
\end{equation*}

Somit ist
\begin{equation*}
v^a = \tilde{v}^i \tilde{E}_i^a = v^k  \frac{\partial \tilde{x}^i}{\partial x^k}(x_0^m)\tilde{E}_i^a \overset{\eqref{eq:tpmabstrakt}}{=} v^i E_i^a
\end{equation*}

Also
\begin{equation*}
\frac{\partial \tilde{x}^i}{\partial x^k}(x_0^m)\tilde{E}_i^a = E_i^a
\end{equation*}

bzw.
\begin{equation}\label{eq:partdiff1}
\tilde{E}_i^a = \frac{\partial x^k}{\partial \tilde{x}^i}(x_0^m)\,E_k^a
\end{equation}

Also ist das Transformationsverhalten der Basisvektoren invers zum Transformationsverhalten der Tangentialvektoren.

Wir suchen nun ein natürliches Objekt mit genau diesem Transformationsverhalten. Betrachte dazu den Kartenwechsel
\begin{equation*}
\tilde{x}^k(t) = \tilde{x}^k(x^i(t))
\end{equation*}

und dessen Ableitung
\begin{equation*}
\partial_i := \frac{\partial }{\partial x^i} \overset{\substack{\text{Ketten-} \\ \text{regel}}}{=} \underbrace{\frac{\partial}{\partial \tilde{x}^k}}_{=:\tilde{\partial}_k}\frac{\partial \tilde{x}^k}{\partial x^i} 
 = \tilde{\partial}_k \frac{\partial \tilde{x}^k}{\partial x^i} 
\end{equation*}

Also
\begin{equation}\label{eq:partdiff3_lang}
\partial_i \big|_{x_0^m}= \tilde{\partial}_k \big|_{x_0^m} \underbrace{\frac{\partial \tilde{x}^k}{\partial x^i}(x_0^m)}_{=\partial_i\tilde{x}^k\big|_{x^m_0}}
\end{equation}
bzw.
\begin{equation}\label{eq:partdiff2_lang}
\tilde{\partial}_i \big|_{x_0^m}= \partial_k \big|_{x_0^m} \underbrace{\frac{\partial x^k}{\partial \tilde{x}^i}(x_0^m)}_{=\tilde{\partial}_ix^k\big|_{x^m_0}}
\end{equation}

Die partiellen Ableitungen sind immer bzgl. $x_0^m$ zu verstehen. Um die Schreibweise zu vereinfachen wird $\big|_{x_0^m}$ ab jetzt weggelassen.

Somit wird \eqref{eq:partdiff3_lang} und \eqref{eq:partdiff2_lang} zu:

\begin{equation}\label{eq:partdiff3}
\boxed{
\partial_i = \partial_i\tilde{x}^k\tilde{\partial}_k 
}
\end{equation}

\begin{equation}\label{eq:partdiff2}
\boxed{
\tilde{\partial}_i = \tilde{\partial}_ix^k \partial_k 
}
\end{equation}


Vergleiche \eqref{eq:partdiff1} mit \eqref{eq:partdiff2}: die Operatoren $\partial_i$ besitzen genau das Transformationsverhalten der Basiselemente $E_i^a$ und sind von der Struktur des Vektorraums her ununterscheidbar.

 Wir identifizieren daher
\begin{equation*}
E_i^a \leftrightarrow \partial_i^a
\end{equation*}

Damit kann jeder Tangentialvektor geschrieben werden als
\begin{equation}\label{eq:vektortrafo}
v^a = v^i \partial_i^a = \tilde{v}^i \tilde{\partial}_i^a
\end{equation}

Der Tangentialraum am Punkt $p\in M$ lässt sich somit \textbf{basis- und koordinatenunabhängig} als Vektorraum darstellen als
\begin{equation*}
T_pM = \left\{ v^a \;\big|\; v^a = v^i \partial_i^a \text{ in \textbf{jeder} Karte } x^i: p \mapsto x^i(p) \text{ die p enthält}\right\}
\end{equation*}
\vspace{1em}

\begin{tcolorbox}[noVO]
\begin{remark}[Nicht Teil der VO] \breakafterhead

Besser verständlich würde ich es so anschreiben:
\begin{equation*}
T_pM = \left\{
v^a \;\big|\; 
\forall\, \text{Karten } (\mathcal{O},\phi) \text{ mit } p \in \mathcal{O}, \, \phi^i(p)=x_0^m: \; v^a = v^i \partial_i^a \big|_{x_0^m} \right\}
\end{equation*}

Also Vektoren $v^a$, die in \textbf{jeder} Karte $(\mathcal{O},\phi)$ mit $p \in \mathcal{O}, \, \phi^m(p)=x_0^m$ darstellbar sind als $v^a = v^i \partial_i^a \big|_{x_0^m}$.
\end{remark}
\end{tcolorbox}
\vspace{1em}

\begin{remark}
Aus \eqref{eq:vektortrafo} erhalten wir wieder \eqref{eq:tangent_transform}:
\begin{equation*}
v^a = v^i\,\partial_i^a \overset{\eqref{eq:partdiff3}}{=} v^i\,\partial^a_i \tilde{x}^k\,\tilde{\partial}_k^a
= \left( \partial^a_i \tilde{x}^k \,v^i\right) \tilde{\partial}_k^a\overset{\eqref{eq:vektortrafo}}{=} \tilde{v}^k \tilde{\partial}_k^a
\end{equation*}

Somit folgt das \textbf{Transformationsverhalten der Komponenten des Tangentialvektors beim Kartenwechsel} \eqref{eq:tangent_transform} 
\begin{equation}\label{eq:komptrafovek}
\boxed{
\tilde{v}^k = \partial^a_i \tilde{x}^k \,v^i
}
\qquad
\boxed{
v^k = \tilde{\partial}^a_i x^k \tilde{v}^i
}
\end{equation}

Also muss man sich nur \eqref{eq:vektortrafo} und die Kettenregel \eqref{eq:partdiff3} merken und kann sich daher das Transformationsverhalten der Tangentialvektoren herleiten.
\end{remark}

\medskip

Wir können nun den erarbeiteten Vektorraum verwenden, um Vektorfelder auf Mannigfaltigkeiten zu definieren. 
\begin{definition}
Sei $(M, \mathfrak{M})$ eine $C^k$-Mannigfaltigkeit. Ein \textbf{$C^k$-Vektorfeld} ist dann
\begin{equation*}
v^a: M \to T_pM: p \mapsto v^a(p) \quad \forall p \in M
\end{equation*}
Das Vektorfeld bildet also jeden Punkt $p \in M$ auf einen Tangentialvektor $v^a(p) \in T_pM$ ab.

Für jede Karte $(\mathcal{O},\phi)$ mit Koordinaten $x^i$ besitzt $v^a$ lokal eine Darstellung
\begin{equation*}
v^a = v^i(x^k)\,\partial_i^a\big|_{x_0^m}
\end{equation*}
wobei die \textbf{numerischen Funktionen/Vektorfelder (Komponenten des Vektorfelds)}
\begin{equation*}
v^i: v^a \circ (\phi^i)^{-1} : \phi^i(\mathcal{O}) \subseteq \mathbb{R}^n \to \mathbb{R}
\end{equation*}
$k$-mal stetig differenzierbar sind.
\end{definition}


\begin{tcolorbox}[noVO]
\begin{definition}[Nicht Teil der VO] \breakafterhead

Die disjunkte Vereinigung aller Tangentialräume von $M$ heißt \textbf{Tangentialbündel} und wird definiert als
\begin{equation*}
TM := \dot{\bigcup_{p \in M}} T_pM
\end{equation*} 
D.h.
\begin{equation*}
TM = \bigcup_{p \in M} T_pM  \quad
\text{ und } \quad
T_pM \cap T_{\tilde{p}}M = \emptyset \text{ (paarweise disjunkt)}
\end{equation*} 
\end{definition}

Mit dem Tangentialbündel lautet die Definition:
\begin{definition}[Nicht Teil der VO] \breakafterhead

Sei $(M, \mathfrak{M})$ eine $C^k$-Mannigfaltigkeit und die kanonische Projektion $\pi:TM \to M$. Ein \textbf{$C^k$-Vektorfeld} ist dann
\begin{equation*}
v^a: M \to TM: p \mapsto v^a(p) \in T_pM
\end{equation*} 
mit
\begin{equation*}
\pi \circ v^a = \mathrm{id}_M
\end{equation*} 

wobei die numerischen Funktionen/Vektorfelder (Komponenten des Vektorfelds) $k$-mal stetig differenzierbar sind.
\end{definition}
\end{tcolorbox}

\begin{figure}[H]
\centering
\includegraphics[width=0.3\textwidth]{Vector_field_E.png}
\caption{Vektorfeld auf der Mannigfaltigkeit $S^2$}
\end{figure}

\subsection{Dualraum des Tangentialraums (Kotangentialraum)}
Wir erinnern uns an die Definition von (r,s)-Tensoren \eqref{eq:tensordef}
\begin{equation*}
\mathfrak{Mult}_{r,s}(V^\sim,V,\Gamma) = \bigotimes\nolimits^r V \otimes \bigotimes\nolimits^s V^\sim = \bigotimes\nolimits^r_s V =: T^r_s
\end{equation*}

Also brauchen wir für Tensorfelder auf Mannigfaltigkeiten noch das Konzept des Dualraums vom Tangentialraum. Da der Tangentialraum $T_pM$ ein Vektorraum ist, besitzt er einen Dualraum. Diesen bezeichnen wir als den \textbf{Kotangentialraum} $(T_pM)^\sim$.

Ein Kovektor $\omega_a\in (T_pM)^\sim$ ist somit eine lineare Abbildung
\begin{equation*}
\omega_a : T_pM \to \mathbb{R}: v^a \mapsto \omega_av^a
\end{equation*}

Bezüglich einer Basis $E_i^a$ des Tangentialraums existiert eine duale Basis $e^i_a$ des Kotangentialraums, definiert durch die Dualitätsrelation
\begin{equation}\label{eq:duarel}
e^i_a(E_j^a) = \delta^i_j
\end{equation}
Damit lässt sich ein Kovektor schreiben als
\begin{equation*}
\omega_a = \omega_i\, e^i_a
\end{equation*}

Da wir das Transformationsverhalten der Basis $E_i^a$ des Tangentialraums kennen (siehe \eqref{eq:partdiff1}) und in \cref{sec:basistrafo} gezeigt haben, dass die duale Basis $e^i_a$ stets invers zur Vektorbasis transformiert, folgt für einen Kartenwechsel $\tilde{x}^i=\tilde{x}^i(x^k)$
\begin{equation*}
\tilde{e}^i_a = \underbrace{\frac{\partial \tilde{x}^i}{\partial x^k}(x_0^m)}_{=\partial_k \tilde{x}^i \big|_{x_0^m} }  \, e^k_a
\end{equation*}

Wir suchen wieder nach einem Objekt, das obiges Transformationsverhalten aufweist. Dazu betrachten wir das totale Differential
\begin{equation}\label{eq:dualtrafo_lang}
d\tilde{x}^i\big|_{x_0^m} = \underbrace{\frac{\partial \tilde{x}^i}{\partial x^k}(x_0^m)}_{=\partial_k\tilde{x}^i \big|_{x_0^m}}\, dx^k\big|_{x_0^m}
\end{equation}

Die Ableitungen sind wieder immer bzgl. dem Punkt $x_0^m$ zu verstehen. Wir werden nun wieder $\big|_{x_0^m}$ weglassen, um die Schreibweise zu vereinfachen. Damit wird \eqref{eq:dualtrafo_lang} zu

\begin{equation}\label{eq:dualtrafo}
\boxed{
d\tilde{x}^i= \partial_k\tilde{x}^i dx^k
}
\end{equation}

bzw. 
\begin{equation}\label{eq:dualtrafo1}
\boxed{
dx^i= \tilde{\partial}_k x^i d\tilde{x}^k
}
\end{equation}

Wir sehen, dass $dx^i$ exakt dasselbe Transformationsverhalten wie die dualen Basiselemente $e^i_a$ besitzt und somit von der Struktur des Vektorraums her ununterscheidbar sind. Wir identifizieren daher
\begin{equation*}
e^i_a \leftrightarrow dx^i_a
\end{equation*}

Damit kann jeder Kovektor $\omega_a$ bzgl. verschiedenen Basen zerlegt werden
\begin{equation}\label{eq:kovektortrafo}
\omega_a = \omega_i dx^i_a = \tilde{\omega}_i d\tilde{x}^i_a
\end{equation}

Somit kann der Kotangentialraum am Punkt $p\in M$ \textbf{basis- und koordinatenunabhängig} dargestellt werden als
\begin{equation*}
(T_pM)^\sim = \left\{
\omega_a \;\big|\; \omega_a = \omega_i dx^i_a \text{ in \textbf{jeder} Karte } x^i: p \mapsto x^i(p) \text{ die p enthält}\right\}
\end{equation*}
\vspace{1em}

\begin{tcolorbox}[noVO]
\begin{remark}[Nicht Teil der VO] \breakafterhead

Besser verständlich würde ich es so anschreiben:
\begin{equation*}
(T_pM)^\sim = \left\{
\omega_a \;\big|\; 
\forall\, \text{Karten } (\mathcal{O},\phi) \text{ mit } p \in \mathcal{O}, \, \phi^m(p)=x_0^m: \; \omega_a = \omega_i dx^i_a\big|_{x_0^m} \right\}
\end{equation*}
Also Kovektoren $\omega^a$, die in \textbf{jeder} Karte $(\mathcal{O},\phi)$ mit $p \in \mathcal{O}, \, \phi^m(p)=x_0^m$ darstellbar sind als $\omega_a = \omega_i dx^i_a\big|_{x_0^m}$.
\end{remark}
\end{tcolorbox}
\vspace{1em}


\begin{remark}[Dualitätsrelation] \breakafterhead

Mit der Identifikation $E_i^a \leftrightarrow \partial_i^a$ und $e^i_a\leftrightarrow dx^i_a$ kann die Dualitätsrelation \eqref{eq:duarel} nun geschrieben werden als
\begin{equation*}
dx^i_a(\partial_j^a) = \delta^i_j
\end{equation*}
\end{remark}

Dies ist die kanonische Paarung zwischen Basis und dualer Basis und \emph{nicht} als Ableitungsgleichung zu interpretieren.

\begin{remark}[Transformation Kovektor bzgl. Kartenwechsel] \breakafterhead

Ein Kovektor $\omega_a$ kann bezüglich zweier Karten $(\mathcal O,\phi)$ und $(\tilde{\mathcal O},\tilde\phi)$ geschrieben werden als
\begin{equation}\label{eq:kovektor}
\omega_a = \omega_i dx^i_a = \tilde{\omega}_i d\tilde{x}^i_a
\end{equation}

Setze das Transformationsverhalten der Basiselemente \eqref{eq:dualtrafo} bzw. \eqref{eq:dualtrafo1} in \eqref{eq:kovektor} ein 
\begin{equation*}
\omega_i\, dx^i_a = \omega_i \tilde{\partial}_k x^i d\tilde{x}_a^k,
\qquad
\tilde{\omega}_i d\tilde{x}^i_a = \tilde{\omega}_i \partial_k\tilde{x}^i dx_a^k
\end{equation*}

und vergleiche die Koeffizienten vor $dx_a^k$ bzw. $d\tilde{x}_a^k$ mit \eqref{eq:kovektor}, so folgt das \textbf{Transformationsverhalten der Komponenten beim Kartenwechsel}
\begin{equation}\label{eq:komptrafokovek}
\boxed{
\omega_k  = \partial_k\tilde{x}^i \tilde{\omega}_i 
}
\qquad
\boxed{
\tilde{\omega}_k = \tilde{\partial}_kx^i \omega_i 
}
\end{equation}

Das Transformationsverhalten der Komponenten ergibt sich somit \emph{automatisch} aus dem Transformationsverhalten der Basisobjekte
$dx^i_a$ und muss nicht auswendig gelernt werden.
\end{remark}

\begin{remark}[Interpretation von Differentialen und Koordinatenunabhängigkeit] \breakafterhead

Die Einführung der Differentiale $dx^i_a$ bzw. der Ableitungsoperatoren $\partial_i^a$ ist als \emph{Merkregel für das Transformationsverhalten} der Komponenten von Vektoren $v^i$ und Kovektoren $w_i$ zu verstehen.

Wir haben nach natürlichen Objekten gesucht, deren Transformationsverhalten beim Kartenwechsel genau dem der Komponenten entspricht:
\begin{itemize}
\item die partiellen Ableitungsoperatoren $\partial_i^a$ besitzen genau das Transformationsverhalten der Basiselemente $E_i^a$ des Vektorraums (Kettenregel)
\item die totalen Differentiale $dx^i_a$ besitzen dasselbe Transformationsverhalten wie die Basiselemente $e^i_a$ des Dualraums
\end{itemize}

Die Komponenten sind \emph{keine intrinsischen Eigenschaften}, sondern hängen von der gewählten Basis ab. Wechselt man die Karte, so ändern sich sowohl die Komponenten $v^i$ bzw. $w_i$ als auch die zugehörigen Basisobjekte $\partial_i^a$ bzw. $dx^i_a$ und zwar so, dass sich die Änderungen gegenseitig kompensieren und sich somit das Objekt selbst, der Vektor $v^a$ bzw. der Kovektor $\omega_a$, nicht ändert. (Vergleiche \eqref{eq:vektortrafo} und \eqref{eq:kovektortrafo})
\end{remark}

\begin{remark}[Koordinatenunabhängigkeit der dualen Paarung] \breakafterhead

Wir zeigen, dass gilt
\begin{equation*}
\omega_a v^a = \omega_i v^i = \tilde{\omega}_j\tilde{v}^j
\end{equation*}

\begin{equation*}
\omega_a v^a = \omega_i v^i 
\overset{\substack{\eqref{eq:komptrafovek} \\ \eqref{eq:komptrafokovek}}}{=}
\partial_i\tilde{x}^j \tilde{\omega}_j  \, \tilde{\partial}_l x^i \tilde{v}^l 
= \underbrace{\partial_i\tilde{x}^j \tilde{\partial}_l x^i}_
{= \frac{\partial\tilde{x}^j}{\partial x^i} \frac{\partial x^i}{\partial\tilde{x}^l} = \delta^j_l} \tilde{\omega}_j \tilde{v}^l 
= \delta^j_l \, \tilde{\omega}_j \tilde{v}^l 
= \tilde{\omega}_j \tilde{v}^j
\end{equation*}
\end{remark}

\medskip

Ausgehend vom Tangential- und Kotangentialraum ist nun der Tensorproduktraum vom Typ $(r,s)$ am \newline
Punkt $p$
\begin{equation*}
\bigotimes\nolimits^r_s T_pM =
\underbrace{T_pM \otimes \cdots \otimes T_pM}_{r\text{-mal}}
\otimes
\underbrace{(T_pM)^\sim \otimes \cdots \otimes (T_pM)^\sim}_{s\text{-mal}}
\end{equation*}

Ein Tensor $t^{a_1\ldots a_r}_{b_1\ldots b_s}\in \bigotimes^r_s T_pM$ besitzt bezüglich einer Karte, die $p$ enthält, die Darstellung
\begin{align*}
t^{a_1\ldots a_r}_{b_1\ldots b_s} &= t^{i_1\ldots i_r}_{j_1\ldots j_s} \partial^{a_1}_{i_1}\otimes\cdots\otimes\partial^{a_r}_{i_r} \otimes dx_{b_1}^{j_1}\otimes\cdots\otimes dx_{b_s}^{j_s} \\
&\overset{\eqref{eq:multtensorprod}}{=}
t^{i_1\ldots i_r}_{j_1\ldots j_s} \, \partial^{a_1}_{i_1}\cdot ... \cdot\partial^{a_r}_{i_r} \cdot dx_{b_1}^{j_1}\cdot ... \cdot dx_{b_s}^{j_s} \\
&= \tilde{t}^{i_1\ldots i_r}_{j_1\ldots j_s} \, \tilde{\partial}^{a_1}_{i_1}\cdot ... \cdot\tilde{\partial}^{a_r}_{i_r} \cdot d\tilde{x}_{b_1}^{j_1}\cdot ... \cdot d\tilde{x}_{b_s}^{j_s} 
\end{align*}

Nun können wir Tensorfelder auf Mannigfaltigkeiten global und kartenunabhängig definieren. 
\begin{definition}
Sei $(M, \mathfrak{M})$ eine $C^k$-Mannigfaltigkeit. Ein \textbf{(r,s)-$C^k$-Tensorfeld} ist dann
\begin{equation*}
t^{a_1\ldots a_r}_{b_1\ldots b_s}: M \to \bigotimes\nolimits^r_s T_pM: p \mapsto t^{a_1\ldots a_r}_{b_1\ldots b_s}(p) \quad \forall p \in M
\end{equation*}
Das Tensorfeld bildet also jeden Punkt $p \in M$ auf einen Tensor $t^{a_1\ldots a_r}_{b_1\ldots b_s}(p) \in \bigotimes\nolimits^r_s T_pM$ ab.

Für jede Karte $(\mathcal{O},\phi)$ mit Koordinaten $x^i$ besitzt $t^{a_1\ldots a_r}_{b_1\ldots b_s}$ lokal eine Darstellung
\begin{equation*}
t^{a_1\ldots a_r}_{b_1\ldots b_s} = t^{i_1\ldots i_r}_{j_1\ldots j_s} \, \partial^{a_1}_{i_1}\cdot ... \cdot\partial^{a_r}_{i_r} \cdot dx_{b_1}^{j_1}\cdot ... \cdot dx_{b_s}^{j_s}
\end{equation*}
wobei die \textbf{numerischen Funktionen/Tensorfelder (Komponenten des Tensorfelds)}
\begin{equation*}
t^{i_1\ldots i_r}_{j_1\ldots j_s}: t^{a_1\ldots a_r}_{b_1\ldots b_s} \circ \phi^{-1} : \phi(\mathcal{O}) \subseteq \mathbb{R}^n \to \mathbb{R}
\end{equation*}
$k$-mal stetig differenzierbar sind.
\end{definition}

\begin{tcolorbox}[noVO]
Sei $T^r_s(T_pM) :=\bigotimes\nolimits^r_s T_pM$ Tensorproduktraum des Vektorraums $T_pM$.

\begin{definition}[Nicht Teil der VO] \breakafterhead

Die disjunkte Vereinigung aller Tensorprodukträume von $M$ heißt \textbf{(r,s)-Tensorbündel} und wird definiert als
\begin{equation*}
T^r_s(TM) := \dot{\bigcup_{p \in M}} T^r_s(T_pM)
\end{equation*} 
\end{definition}

Mit dem (r,s)-Tensorbündel lautet die Definition:
\begin{definition}[Nicht Teil der VO] \breakafterhead

Sei $(M, \mathfrak{M})$ eine $C^k$-Mannigfaltigkeit und die kanonische Projektion $\pi^r_s:T^r_s(TM) \to M$. \newline
Ein \textbf{(r,s)-$C^k$-Tensorfeld} ist dann
\begin{equation*}
t^{a_1\ldots a_r}_{b_1\ldots b_s}: M \to T^r_s(TM): p \mapsto t^{a_1\ldots a_r}_{b_1\ldots b_s}(p) \in T_pM
\end{equation*} 
mit
\begin{equation*}
\pi^r_s \circ t^{a_1\ldots a_r}_{b_1\ldots b_s} = \mathrm{id}_M
\end{equation*} 

wobei die numerischen Funktionen/Tensorfelder (Komponenten des Tensorfelds) $k$-mal stetig differenzierbar sind.
\end{definition}
\end{tcolorbox}
\vspace{1em}

Ein Vektorfeld ist dabei ein Spezialfall eines Tensorfeldes mit kontravariantem Grad $1$ und kovariantem Grad $0$.

\begin{definition}
Der \textbf{Raum der (r,s)-$C^\infty$-Tensorfelder} auf der Mannigfaltigkeit $M$ ist definiert als
\begin{equation*}
\mathcal T^r_s(M) := \left\{ t^{a_1\ldots a_r}_{b_1\ldots b_s} \text{ ist ein } C^\infty \text{-Tensorfeld}\right\}
\end{equation*}
\end{definition}

Insbesondere gilt:
\begin{align*}
\mathcal T^0_0(M)&=C^\infty(M) \;\text{ist der Raum der $C^\infty$-Skalarfelder}\\
\mathcal T^1_0(M) &\;\text{ist der Raum der $C^\infty$-Vektorfelder}\\
\mathcal T^0_1(M) &\;\text{ist der Raum der $C^\infty$-Kovektorfelder (1-Formen)} \\
\mathcal T^1_1(M) &\;\text{ist der Raum der (1,1)-$C^\infty$-Tensorfelder}
\end{align*}


\chapter{Differentialgeometrie und Einsteingleichungen}\label{chap:Differentialgeometrie und Einsteingleichungen}
\section{Kovariante Ableitung}
Im flachen Raum $\mathbb{R}^n$ (euklidische Geometrie) wird ein Vektor $v^a = v^i\,\partial_i^a$ entlang einer Kurve $\gamma(t)$ mit Koordinaten $\gamma^i(t)=x^i(t)$ als \emph{parallel} betrachtet, wenn er sich entlang der Kurve nicht ändert. Da die Basisvektoren $\partial_i^a$ im $\mathbb{R}^n$ ortsunabhängig sind, reduziert sich diese Bedingung auf die Konstanz der Komponenten:
\begin{equation*}
\frac{d}{dt} v^i(x^m(t)) = 0
\end{equation*}
Die gewöhnliche Ableitung misst in diesem Fall genau die Abweichung von der Parallelität.

Auf einer gekrümmten Mannigfaltigkeit ist diese Situation jedoch nicht mehr so einfach. 
Ein Tangentialvektor entlang einer Kurve $\gamma(t)$ besitzt die Darstellung
\begin{equation*}
v^a(x^m(t)) = v^i(x^m(t))\,\partial_i^a\big|_{x^m(t)}
\end{equation*}

Bei einer Ableitung nach dem Kurvenparameter $t$ ändern sich nun nicht nur die Komponenten $v^i(x^m(t))$, sondern auch die Basisvektoren $\partial_i^a\big|_{x^m(t)}$ selbst, da diese vom Punkt $p=\gamma(t)\in M$ via $\phi^i(\gamma(t))=x^i(t)$ abhängen.

Die gewöhnliche Ableitung misst daher nicht ausschließlich eine tatsächliche geometrische Änderung des Vektors, sondern enthält zusätzliche Beiträge, die durch den Wechsel der lokalen Basis entstehen. Eine verschwindende gewöhnliche Ableitung ist somit im Allgemeinen nicht gleichbedeutend mit Parallelität.

Ziel der kovarianten Ableitung ist es, eine Ableitung zu definieren, die genau die Änderung eines Vektors misst, die einer Abweichung von der Parallelität entspricht. Sie erlaubt es, den Begriff der Parallelverschiebung auf gekrümmten Mannigfaltigkeiten sinnvoll zu definieren.

\subsection{Koordinatenkovariante Ableitung auf Skalarfeldern}
Wir betrachten zunächst ein Skalarfeld
$f \in \mathcal T^0_0(M)$,
also eine Abbildung
\begin{equation*}
f : M \to \mathbb{R}
\end{equation*}
die bezüglich einer Karte $(\mathcal{O},\phi)$ mit Koordinaten $x^i$ lokal durch $f(x^i)$ dargestellt wird.
Auf diese Darstellung können wir die partielle Ableitung anwenden:
\begin{equation*}
\partial_i f := \frac{\partial f}{\partial x^i}
\end{equation*}

Diese Größe ist jedoch \emph{nicht} koordinatenunabhängig. Wir suchen daher eine Ableitung, deren Wirkung auf Skalarfelder unabhängig von der Wahl der Karte ist.

Sei 

\begin{definition}
Die \textbf{koordinatenkovariante Ableitung eines Skalarfeldes} ist definiert als
\begin{equation}\label{eq:koordkovskalar}
\partial_a: \mathcal{T}^0_0(M) \to \mathcal{T}^0_1(M): f \mapsto \partial_a f := \partial_i f\, dx^i_a 
\end{equation}
$\partial_a f$ ist also ein Kovektorfeld. Hierbei wirken Ableitungssymbole nur auf das direkt folgende Objekt, sofern nicht durch Klammern anders gekennzeichnet.
\end{definition}

Die partielle Ableitung wirkt auf der lokalen Darstellung des Skalarfelds, die koordinatenkovariante Ableitung wirkt auf dem Skalarfeld selbst.

Wir zeigen nun, dass diese Definition tatsächlich koordinatenunabhängig ist, also $\tilde{\partial}_a f=\partial_a f$.

Seien dazu $(\mathcal{O},\phi)$ und $(\tilde{\mathcal{O}},\tilde{\phi})$ zwei Karten mit Kartenwechsel $\tilde{x}^k = \tilde{x}^k(x^i)$ bzw. $x^k = x^k(\tilde{x}^i)$. Gilt für die lokale Darstellung des Skalarfeldes
\begin{equation*}
f(x^k) = f(x^k(\tilde{x}^i))
\end{equation*}

Mit der Kettenregel folgt
\begin{equation}\label{eq:kettenreg}
\tilde{\partial}_i f
= \frac{\partial f}{\partial \tilde{x}^i}(x^k(\tilde{x}^i))
= \frac{\partial x^k}{\partial \tilde{x}^i}\, \frac{\partial f}{\partial x^k}
= \tilde{\partial}_i x^k\, \partial_k f
\end{equation}

Zugleich gilt für das totale Differential
\begin{equation}\label{eq:totdiff}
dx^i_a = dx^i_a(\tilde{x}^k) = \frac{\partial x^i}{\partial \tilde{x}^k}\, d_a\tilde{x}^k = \tilde{\partial}_k x^i\, d_a\tilde{x}^k
\end{equation}

Setzt man beides zusammen, so ergibt sich
\begin{equation}\label{eq:skalartrafo}
\tilde{\partial}_a f 
= \tilde{\partial}_i f\, d\tilde{x}^i_a 
\overset{\eqref{eq:kettenreg}}{=} \tilde{\partial}_i x^k\, \partial_k f \, d\tilde{x}^i_a 
= \underbrace{\tilde{\partial}_i x^k\, d\tilde{x}^i_a}_{\overset{\eqref{eq:totdiff}}{=}dx^k_a} \, \partial_k f
= dx^k_a \,\partial_k f
= \partial_a f
\end{equation}

Damit ist die koordinatenkovariante Ableitung auf Skalarfeldern unabhängig von der Wahl der Karte. 

\begin{remark}
Da aus \eqref{eq:kettenreg} folgt, dass $\partial_if$ beim Kartenwechsel kontravariant transformiert und wir bereits wissen, dass $dx^i$ kovariant transformiert, hätte man sofort schließen können, dass die Koordinatenkovariante Ableitung auf Skalarfeldern koordinatenunabhängig ist, da sich die jeweilige Koordinatenabhängigkeit in der Kombination $\partial_i f\,dx^i_a$ aufhebt.
\end{remark}

\subsection{Koordinatenkovariante Ableitung auf Vektorfeldern}
Nachdem wir gesehen haben, dass alle koordinatenkovarianten Ableitungen auf Skalarfeldern in allen Koordinaten übereinstimmen, untersuchen wir nun, ob dies auch für Vektorfelder gilt.

Sei $v^a = v^i(x^m)\,\partial_i^a \in \mathcal T^1_0(M)$ ein Vektorfeld.

\begin{definition}\label{def:koordkovar}
Die \textbf{koordinatenkovariante Ableitung eines Vektorfeldes} bezüglich der Karte $(\mathcal O,\phi)$ ist definiert durch
\begin{equation*}
\partial_b: \mathcal{T}^1_0(M) \to \mathcal{T}^1_1(M): v^a \mapsto \partial_b v^a := \partial_j v^i\, dx^j_b \, \partial_i^a
\end{equation*}
$\partial_b v^a$ ist also ein Tensorfeld.
\end{definition}

Mit der Leibnizregel können wir schreiben:
\begin{equation}\label{eq:leibniz}
\partial_b v^a = \partial_b\bigl(v^i\,\partial_i^a\bigr)
= \partial_b v^i\,\partial_i^a + v^i\,\partial_b \partial_i^a 
\end{equation}

Vergleicht man diesen Ausdruck mit \cref{def:koordkovar}, so folgt
\begin{equation*}
\partial_b v^i = \partial_j v^i\, dx^j_b
\quad
\text{und}
\quad \partial_b \partial_i^a = 0
\end{equation*}

$\partial_b \partial_i^a = 0$ bedeutet, dass diese Basisvektoren $\partial_i^a$ bezüglich der koordinatenkovarianten Ableitung $\partial_b$ konstant sind. Da jede Karte/Koordinatensystem ihre eigene Basis besitzt, liefert jedes Koordinatensystem seinen eigenen koordinatenkovarianten Ableitungsoperator. Also gilt im Allgemeinen für verschiedene Karten $(\mathcal O,\phi)$ und $(\tilde{\mathcal O},\tilde{\phi})$: $\partial_b \tilde{\partial}_i^a \neq 0$, $\tilde{\partial}_b \partial_i^a \neq 0$ aber $\tilde{\partial}_b \tilde{\partial}_i^a = 0$.

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{images/Parallelversch1.jpg}
\caption{Wenn wir einen Vektor $v$ im ungetildeten Koordinatensystem parallel verschieben, dann ändern sich dessen Basisvektoren $\partial_i^a$ nicht bzgl. des eigenen Ableitungsoperators $\partial_b$, also $\partial_b \partial_i^a = 0$, aber im allgemeinen schon bzgl. des Ableitungsoperators $\tilde{\partial}_b$ des getildeten Koordinantensystems, also $\tilde{\partial}_b \partial_i^a \neq 0$. Dasselbe gilt auch für das getildete Koordinatensystem. Parallelverschiebung hängt also vom Koordinatensystem ab.}
\end{figure}

Somit ist die koordinatenkovarianten Ableitung auf Vektorfeldern \textit{abhängig} von der Wahl der Karte. Also gilt im Allgemeinen $\partial_b v^a \neq \tilde{\partial}_b v^a$. Wir haben somit keinen globalen Ableitungsbegriff.

\medskip
\underline{Transformationsverhalten der koordinatenkovarianten Ableitung auf einem Vektorfeld}\newline
Wir betrachten dazu zwei Karten $(\mathcal O,\phi)$ und $(\tilde{\mathcal O},\tilde{\phi})$ mit Koordinaten $x^i$ bzw. $\tilde{x}^i$. Das Vektorfeld kann bezüglich beider Basen dargestellt werden:
\begin{equation*}
v^a = v^i\,\partial_i^a = \tilde v^i\,\tilde\partial_i^a 
\end{equation*}

Wir berechnen nun $\partial_b v^a$ in der Tilde Darstellung:
\begin{align*}
\partial_b v^a
&= \partial_b(\tilde{v}^i\,\tilde{\partial}_i^a) \\
&\overset{\text{Leibniz}}{=} 
\underbrace{\partial_b \tilde{v}^i}_{\overset{\substack{\text{num. VF} \\ \text{ist skalare Fkt.} \\ \text{\eqref{eq:skalartrafo}}}}{=} \tilde{\partial}_b \tilde{v}^i}\tilde{\partial}_i^a 
+ \tilde{v}^i\partial_b \underbrace{\tilde{\partial}_i^a}_{\overset{\substack{\text{Basis-} \\ \text{Trafo}}}{=}\tilde{\partial}_ix^j \partial_j^a}
\end{align*}
\begin{equation}\label{eq:star}
= \tilde{\partial}_b \tilde{v}^i\tilde{\partial}_i^a
+ \tilde{v}^i \partial_b \, (\tilde{\partial}_ix^j \partial_j^a) \qquad \qquad
\end{equation}

Da 
\begin{equation*}
\tilde{\partial}_b (\tilde{v}^i\tilde{\partial}_i^a) = \tilde{\partial}_b \tilde{v}^i \, \tilde{\partial}_i^a + \tilde{v}^i \, \underbrace{\tilde{\partial}_b\tilde{\partial}_i^a}_{=0} \, \Rightarrow \, \tilde{\partial}_b (\tilde{v}^i\tilde{\partial}_i^a) = \tilde{\partial}_b \tilde{v}^i \tilde{\partial}_i^a
\end{equation*}
und 
\begin{equation*}
\partial_b (\tilde{\partial}_ix^j \partial_j^a) = \partial_b \tilde{\partial}_ix^j \, \partial_j^a + \tilde{\partial}_ix^j \, \underbrace{\partial_b \partial_j^a}_{=0} \, \Rightarrow \, \partial_b \, (\tilde{\partial}_ix^j \partial_j^a) = \partial_b \tilde{\partial}_ix^j \, \partial_j^a
\end{equation*}

folgt
\begin{align*}
\eqref{eq:star}&= \tilde{\partial}_b (\underbrace{\tilde{v}^i\tilde{\partial}_i^a}_{=v^a}) + \underbrace{\tilde{v}^i}_{=\tilde{v}^j \delta^i_j = \tilde{v}^j \tilde{\partial}^c_jd\tilde{x}^i_c = v^c d\tilde{x}^i_c} \partial_b \, \tilde{\partial}_ix^j \partial_j^a\\
&= \tilde{\partial}_b v^a + d\tilde{x}^i_c\, v^c \partial_b \, \tilde{\partial}_ix^j \underbrace{\partial_j^a}_{\overset{\substack{\text{Basis-} \\ \text{Trafo}}}{=}\partial_j\tilde{x}^k \tilde{\partial}^a_k} \\
&= \tilde{\partial}_b v^a + d\tilde{x}^i_c\, v^c \underbrace{\partial_b \, \tilde{\partial}_ix^j\partial_j\tilde{x}^k \tilde{\partial}^a_k}_{\overset{\text{skalare Fkt.}}{=} \tilde{\partial}_b \, \tilde{\partial}_ix^j\partial_j\tilde{x}^k \tilde{\partial}^a_k} \\
&= \tilde{\partial}_b v^a + d\tilde{x}^i_c\, v^c \underbrace{\tilde{\partial}_b}_{\overset{\eqref{eq:koordkovskalar}}{=}\tilde{\partial}_l d\tilde{x}^l_b} \, \tilde{\partial}_ix^j\partial_j\tilde{x}^k \tilde{\partial}^a_k \\
&= \tilde{\partial}_b v^a + d\tilde{x}^i_c\, d\tilde{x}^l_b \tilde{\partial}_l \, \tilde{\partial}_ix^j\partial_j\tilde{x}^k \tilde{\partial}^a_k \ v^c\\
\end{align*}

Also insgesamt
\begin{equation*}
\partial_b v^a = \tilde{\partial}_b v^a + \tilde{\partial}_l \, \tilde{\partial}_ix^j\partial_j\tilde{x}^k \tilde{\partial}^a_k \, d\tilde{x}^i_c\, d\tilde{x}^l_b \ v^c
\end{equation*}

Wir definieren das \textbf{Christoffel-Symbol}
\begin{equation}\label{eq:chris}
\Gamma^a_{cb} := \underbrace{\tilde{\partial}_l \tilde{\partial}_ix^j}_{= \frac{\partial^2 x^j}{\partial \tilde{x}^l \partial \tilde{x}^i}}\partial_j\tilde{x}^k \tilde{\partial}^a_k \, d\tilde{x}^i_c d\tilde{x}^l_b
\end{equation}

und erhalten damit
\begin{equation}\label{eq:koordkovartrafo}
\boxed{
\partial_b v^a = \tilde{\partial}_b v^a + \underbrace{\Gamma^a_{cb}\, v^c}_{\substack{\text{linearer Zusatzterm} \\ \text{ (Korrekturterm)}}}
}
\end{equation}

bzw.
\begin{equation}\label{eq:difftensor}
\boxed{
(\partial_b - \tilde{\partial}_b)v^a = \Gamma^a_{cb}\, v^c  \in \mathcal{T}_1^1(M)
}
\end{equation}

wobei a, b, c abstrakte Indizes und i, j, k, l Zählindizes sind. 

Zusammengefasst erhalten wir:
\begin{itemize}
\item Auf Skalarfeldern stimmen alle koordinatenkovarianten Ableitungen überein.
\item Auf Vektorfeldern unterscheiden sich die koordinatenkovarianten Ableitungen verschiedener Koordinatensysteme um einen linearen Zusatzterm. Dieser Zusatzterm beschreibt die Änderung der Basisvektoren beim Kartenwechsel und kann als Maß für die Abweichung von Parallelität interpretiert werden.
\end{itemize}

\begin{remark}
Das Christoffel-Symbol $\Gamma^a_{cb}$ ist ein lokal im Kartenüberlapp definierter Ausdruck, der vom Kartenwechsel abhängt. Er beschreibt die Differenz zweier Ableitungsoperatoren \eqref{eq:difftensor}, von denen mindestens einer eine koordinatenkovariante Ableitung ist. Das Christoffel-Symbol ist nur \textit{lokal} ein Tensor. Global gesehen, ist es kein Tensor da es bzgl. dem Kartenwechsel nicht wie ein Tensor transformiert. Erst die Differenz \eqref{eq:difftensor} ist (global) ein Tensor, der sogenannte \textbf{Differenzentensor}, da dabei die auftretenden nicht-tensoriellen Anteile wegfallen. \href{https://en.wikipedia.org/wiki/Christoffel_symbols#Transformation_law_under_change_of_variable}{Quelle}
\end{remark}


\begin{tcolorbox}[noVO]
\begin{remark}[Nicht Teil der VO] \breakafterhead

{\footnotesize Quelle: \url{https://en.wikipedia.org/wiki/Christoffel_symbols#Transformation_law_under_change_of_variable}}

Angenommen, $\Gamma^a_{cb}$ aus \eqref{eq:chris} wäre ein Tensor vom Typ $(1,2)$. Dann müssten seine Komponenten beim Kartenwechsel $\hat{x}^i = \hat{x}^i(\tilde{x}^j)$ ohne zusätzliche Terme transformieren: 
\begin{equation*}
\hat{\Gamma}^d _{fe} = \tilde{\partial}_a \hat{x}^d \hat{\partial}_f \tilde{x}^c \hat{\partial}_e \tilde{x}^b \Gamma^a _{cb}
\end{equation*}

Es gilt aber
\begin{equation*}
\hat{\Gamma}^d _{fe} = \tilde{\partial}_a \hat{x}^d \hat{\partial}_f \tilde{x}^c \hat{\partial}_e \tilde{x}^b \Gamma^a _{cb} + 
\underbrace{\hat{\partial}_f \hat{\partial}_e \tilde{x}^a \, \tilde{\partial}_a \hat{x}^d}_{\text{inhomogener Zusatzterm}}
\end{equation*}
\end{remark}

\begin{remark}[Nicht Teil der VO] \breakafterhead

{\footnotesize Quellen:
\url{https://en.wikipedia.org/wiki/Christoffel_symbols}\newline
\url{https://einsteinrelativelyeasy.com/index.php?option=com_content&view=article&id=25&catid=10&Itemid=112}\newline
\url{https://profoundphysics.com/christoffel-symbols-a-complete-guide-with-examples/}}

Es gilt $\Gamma^a_{cb}(p) \in \mathbb{R}$ für Punkte aus dem Kartenüberlapp $p \in \mathcal{O} \cap \tilde{\mathcal{O}} \subset M$. Als Abbildung gilt $\Gamma^a_{cb} \in C^\infty(\mathcal{O} \cap \tilde{\mathcal{O}})$.

Für ein besseres Verständnis betrachte ich das Christoffel-Symbol $\Gamma^a_{cb}$ in \textbf{Koordinatendarstellung}.

Betrachte wieder \eqref{eq:leibniz} 
\begin{equation*}
\partial_b v^a = \partial_b\bigl(v^i\,\partial_i^a\bigr)
= (\partial_b v^i)\, \partial_i^a + v^i \underbrace{\partial_b \partial_i^a }_{= \Gamma^k_{ib}\,\partial^a_k}
= \underbrace{(\partial_b v^i)}_{\substack{\text{Ableitung} \\ \text{Komponenten}}}
\underbrace{\partial_i^a }_{\text{Basisvektor}}
+ \underbrace{v^i}_{\text{Komponenten}}
\underbrace{\Gamma^k_{ib}\,\partial^a_k}_{\substack{\text{Änderung der} \\ \text{Basisvektoren}}}
\end{equation*}

Also in koordinatenabhängiger/basisabhängiger Darstellung
\begin{equation*}
\partial_b \partial_i^a := \Gamma^k_{ib}\,\partial^a_k \in \mathcal{T}_0^1(M)
\end{equation*}
wobei
\begin{itemize}
\item der Zählindex $i$ gibt den Basisvektor an, dessen Änderung betrachtet wird ($\partial_i^a$)
\item der abstrakte Index $b$ gibt die Richtung an, entlang der abgeleitet wird
\item der Zählindex $k$ gibt den Basisvektor $\partial_k^a$ an, in dessen Richtung die Ableitung vom Basisvektor $\partial_i^a$ zeigt (Vektorrichtung den Zusatzterm zeigt)
\end{itemize}

Beispiel in Polarkoordinaten (siehe anschließend genauer):
\begin{equation*}
\partial_\varphi e_r = 0 \cdot e_r + \frac{1}{r}e_\varphi = \Gamma^r_{r\varphi} \cdot e_r + \Gamma^\varphi_{r\varphi}\cdot e_\varphi
\end{equation*}

Das heißt, bei einer Ableitung eines Vektors $\partial_b v^a= \partial_b (v^i \partial^a_i)$ in Richtung $b$ beschreibt das Christoffel-Symbol $\Gamma^k_{ib}$ die Änderung des Basisvektors $\partial_i^a$, indem es den Koeffizienten der Komponente des Basisvektors $\partial_k^a$ angibt, in dessen Richtung die Ableitung vom Basisvektor $\partial_i^a$ zeigt. Aus diesem Grund können Christoffel-Symbole selbst in flachen Räumen auftreten, sofern die Koordinatenbasis nicht konstant ist (z.\,B. in Polarkoordinaten).

Das Christoffel-Symbol misst \emph{nicht} die Änderung der Komponenten $v^i$ eines Vektorfeldes. 

\end{remark}


\begin{example}[Nicht Teil der VO] \breakafterhead

{\footnotesize Quelle: \url{https://en.wikipedia.org/wiki/Polar_coordinate_system#Differential_geometry}}

Wir betrachten die Ebene $\mathbb{R}^2$ als differenzierbare Mannigfaltigkeit.
In kartesischen Koordinaten $(x,y)$ sind die Koordinatenbasisvektoren
\begin{equation*}
e_x = \partial_x = (1,0),
\qquad
e_y = \partial_y = (0,1)
\end{equation*}
\textit{ortsunabhängig}, sodass ihre Ableitungen verschwinden:
\begin{equation*}
\partial_x \partial_x = \partial_x \partial_y
= \partial_y \partial_x = \partial_y \partial_y = 0
\end{equation*}

Bei einem Vektorfeld in kartesischen Koordinaten $v(x,y) = v_1(x,y)\partial_x + v_2(x,y)\partial_y$ ändern sich bei der Ableitung somit nur die Komponenten $v_1(x,y)$ und $v_2(x,y)$ und nicht die Basisvektoren $\partial_x$ und $\partial_y$.

Die Christoffel-Symbole sind somit $\Gamma^x_{xx}=\Gamma^y_{xx}=\Gamma^x_{xy}=\Gamma^y_{xy}=\Gamma^x_{yx}=\Gamma^y_{yx}=\Gamma^x_{yy}=\Gamma^y_{yy}=0$.

Transformieren wir in Polarkoordinaten $(r,\varphi)$ (führen einen Kartenwechsel durch) mit
\begin{equation*}
x = r\cos\varphi,
\qquad
y = r\sin\varphi 
\end{equation*}
lauten die Basisvektoren
\begin{equation*}
e_r= \partial_r  = \cos\varphi\,\partial_x + \sin\varphi\,\partial_y,
\qquad
r\,e_\varphi= \partial_\varphi = -r\sin\varphi\,\partial_x + r\cos\varphi\,\partial_y
\end{equation*}
Diese Basisvektoren sind nun \emph{ortsabhängig}. Betrachten wir nun ihre Ableitungen
\begin{align*}
&\partial_r \partial_r = \partial_r(\cos\varphi\,\partial_x + \sin\varphi\,\partial_y) = {\color{blue}0} \cdot \partial_r + {\color{red}0} \cdot \partial_\varphi\  
\rightarrow \ {\color{blue}\Gamma^r_{rr} = 0}, \  {\color{red}\Gamma^\varphi_{rr} = 0}\\
&\partial_\varphi \partial_r = \partial_\varphi(\cos\varphi\,\partial_x + \sin\varphi\,\partial_y)
= -\sin\varphi\,\partial_x + \cos\varphi\,\partial_y 
= {\color{blue}0} \cdot \partial_r + {\color{red}\frac{1}{r}}\,\partial_\varphi 
\rightarrow \ {\color{blue}\Gamma^r_{\varphi r} = 0}, \  {\color{red}\Gamma^\varphi_{\varphi r} = \frac{1}{r}}\\
&\partial_r \partial_\varphi
= \partial_r(-r\sin\varphi\,\partial_x + r\cos\varphi\,\partial_y)
= -\sin\varphi\,\partial_x + \cos\varphi\,\partial_y = = {\color{blue}0} \cdot \partial_r + {\color{red}\frac{1}{r}}\,\partial_\varphi 
\rightarrow \ {\color{blue}\Gamma^r_{r \varphi} = 0}, \  {\color{red}\Gamma^\varphi_{r \varphi} = \frac{1}{r}} \\
&\partial_\varphi \partial_\varphi = \partial_\varphi(-r\sin\varphi\,\partial_x + r\cos\varphi\,\partial_y) = 
-r\cos\varphi\,\partial_x - r\sin\varphi\,\partial_y = {\color{blue}-r} \cdot\partial_r + {\color{red}0} \cdot \partial_\varphi
\rightarrow \ {\color{blue}\Gamma^r_{\varphi \varphi} = -r}, \  {\color{red}\Gamma^\varphi_{\varphi \varphi} = 0}
\end{align*}

Daraus folgt, dass sich bei einem Vektorfeld in Polarkoordinaten $v(r,\varphi) = v_1(r,\varphi)\partial_r + v_2(r,\varphi)\partial_\varphi$ bei der Ableitung sowohl die Komponenten $v_1(r,\varphi)$ und $v_2(r,\varphi)$ als auch die Basisvektoren $\partial_r$ und $\partial_\varphi$ ändern.

Obwohl der Raum selbst flach ist, treten diese Terme aufgrund der nichtlinearen Koordinatenwahl auf (ist ein Kreis!).
\end{example}
\end{tcolorbox}
\vspace{1em}

Im $\mathbb{R}^n$ gibt es nicht \underline{den} Gradienten, sondern jedes Koordinatensystem definiert seinen eigenen Gradienten in dessen Koordinaten.

\subsection{Von der koordinatenkovarianten Ableitung zur kovarianten Ableitung}
Ausgehend von der koordinatenkovarianten Ableitung definieren wir nun den Begriff der kovarianten Ableitung.
Sie soll in koordinatenunabhängiger Weise beschreiben, wie Tensorfelder auf einer Mannigfaltigkeit parallel verschoben bzw. differenziert werden. Wie bereits bei der koordinatenkovarianten Ableitung erhöht auch die kovariante Ableitung
den kovarianten Grad eines Tensorfeldes um eins, worin sich die Namensgebung begründet.

\begin{definition}
Sei $(M,\mathfrak{M})$ eine glatte Mannigfaltigkeit. Eine \textbf{kovariante Ableitung} ist eine Abbildung
\begin{equation*}
\nabla_a : \mathcal T^r_s(M) \longrightarrow \mathcal T^r_{s+1}(M):
t^{b_1\ldots b_r}_{c_1\ldots c_s}
\mapsto
\nabla_a t^{b_1\ldots b_r}_{c_1\ldots c_s}
\end{equation*}
mit folgenden Eigenschaften:
\begin{itemize}
\item[\textbf{(K1)}] $\nabla_a$ ist linear:
\begin{equation*}
\nabla_a\ ( t^{b_1\ldots b_r}_{c_1\ldots c_s} + \tilde{t}^{b_1\ldots b_r}_{c_1\ldots c_s} )
= \nabla_a t^{b_1\ldots b_r}_{c_1\ldots c_s} + \nabla_a \tilde{t}^{b_1\ldots b_r}_{c_1\ldots c_s}
\end{equation*}
sowie
\begin{equation*}
\nabla_a\ (\lambda \cdot t^{b_1\ldots b_r}_{c_1\ldots c_s})
=\lambda \cdot \nabla_a t^{b_1\ldots c_r}_{c_1\ldots c_s},
\qquad
\lambda \in \mathbb{R}
\end{equation*}

\item[\textbf{(K2)}] $\nabla_a$ erfüllt die Leibniz-Regel (Produktregel):
\begin{equation*}
\nabla_a
(t^{b_1\ldots b_p}_{c_1\ldots c_q} \otimes \tilde{t}^{d_1\ldots d_r}_{e_1\ldots e_s})
= (\nabla_a t^{b_1\ldots b_p}_{c_1\ldots c_q}) \otimes \tilde{t}^{d_1\ldots d_r}_{e_1\ldots e_s} + t^{b_1\ldots b_p}_{c_1\ldots c_q}
\otimes (\nabla_a \tilde{t}^{d_1\ldots d_r}_{e_1\ldots e_s})
\end{equation*}

\item[\textbf{(K3)}] $\nabla_a$ stimmt auf Skalarfeldern lokal (d.h. nach Wahl einer Karte) mit der koordinatenkovarianten Ableitung überein:
\begin{equation*}
\nabla_a f = \underbrace{\partial_a f,}_{\substack{\text{lokal} \\ \text{definiert}}}
\qquad
\forall f \in \underbrace{C^\infty(M)}_{\substack{= \mathcal{T}^0_0(M) \\ \text{ (Skalarfelder)}}}
\end{equation*}

$\nabla_a$ ist auf der ganzen Mannigfaltigkeit definiert (global). $\partial_a$ ist nur lokal definiert (erst nach Wahl einer Karte). Sobald eine Karte gewählt wird, dürfen sich beide Ableitungen auf Skalarfeldern nicht unterscheiden. 

Es ist egal, ob man zuerst mit $\nabla_a$ ableitet und dann eine Karte wählt oder ob zuerst eine Karte gewählt wird und dann mit $\partial_a$ abgeleitet wird.

\item[\textbf{(K4)}] Satz von Schwarz (Torsionsfreiheit/Symmetrie):
\begin{equation*}
\nabla_a \nabla_b f = \nabla_b \nabla_a f,
\qquad
\forall f \in C^\infty(M)
\end{equation*}
\end{itemize}
\end{definition}

Jede koordinatenkovariante Ableitung ist eine Kovariante Ableitung, die Umkehrung gilt im Allgemeinen nicht. Die kovariante Ableitung liefert uns einen globalen Ableitungsbegriff im Gegensatz zur koordinatenkovariante Ableitung.

Die kovariante Ableitung ist nicht kanonisch (geht nicht aus der Struktur der Mannigfaltigkeit hervor). Sie ist basisabhängig/kartenabhängig. Daher gibt es viele verschiedene kovariante Ableitungen auf derselben Mannigfaltigkeit. 

\begin{definition}
Seien $\nabla_a$ und $\tilde{\nabla}_a$ zwei kovariante Ableitungen auf einer Mannigfaltigkeit $M$ und ein Vektorfeld $v^b \in \mathcal{T}^1_0(M)$. Der Unterschied zwischen den beiden definiert den \textbf{Differenzentensor}
\begin{equation*}
C^b_{ca}v^c := (\tilde{\nabla}_a - \nabla_a) v^b = \tilde{\nabla}_a v^b - \nabla_a v^b
\end{equation*}
\end{definition}

Also reicht es bereits aus, eine kovariante Ableitung zu kennen, um für ein Vektorfeld $v^b$ alle anderen von der Mannigfaltigkeit zu erhalten mit
\begin{equation}\label{eq:vektorkovar}
\boxed{
\tilde{\nabla}_a v^b = \nabla_a v^b + C^b_{ca}v^c
}
\end{equation}

Wir werden später nachweisen, dass es sich bei $C^b_{ca}v^c$ tatsächlich um einen Tensor handelt.

\begin{lemma}
Seien $\nabla_a$ und $\tilde{\nabla}_a$ zwei kovariante Ableitungen auf einer Mannigfaltigkeit $M$ und ein Vektorfeld $v^b \in \mathcal{T}^1_0(M)$.

Dann können Skalarfelder $f \in C^\infty(M)$ aus der Wirkung des Differenzentensors herausgezogen werden
\begin{equation}\label{eq:skalarheraus}
C^b_{ca}(fv^c) = (\tilde{\nabla}_a - \nabla_a) (fv^b) = f \cdot (\tilde{\nabla}_a - \nabla_a) v^b = f \cdot C^b_{ca}v^c
\end{equation}

Man sagt der Differenzentensor ist \emph{$C^\infty(M)$-linear}.
\end{lemma}

\begin{proof}
\begin{equation}\label{eq:skalarzero}
(\tilde{\nabla}_a - \nabla_a) f \overset{K3}{=} (\tilde{\partial}_a - \partial_a) f
= \tilde{\partial}_af - \partial_a f \overset{\substack{\eqref{eq:skalartrafo} \\ \text{stimmen auf} \\ \text{Überlapp} \\ \text{überein}}}{=}0
\end{equation}
Thus
\begin{equation*}
(\tilde{\nabla}_a - \nabla_a) (fv^b) \overset{K2}{=} 
\underbrace{(\tilde{\nabla}_a - \nabla_a) f}_{=0} \cdot v^b + f \cdot (\tilde{\nabla}_a - \nabla_a) v^b =
f \cdot (\tilde{\nabla}_a - \nabla_a) v^b
\end{equation*}
\end{proof}

\begin{remark}
Beachte allerdings, dass Skalarfelder aus der kovariante Ableitungen nicht herausgezogen werden können
\begin{equation*}
\nabla_a (fv^b) \overset{K2}{=} \nabla_a f \cdot v^b + f \cdot \nabla_a  v^b
\end{equation*}
\end{remark}

\medskip

Wir leiten nun die \textbf{Koordinatendarstellung des Differenzentensors} her.
\begin{align*}
C^b_{ca}v^c = (\tilde{\nabla}_a - \nabla_a) v^b & = (\tilde{\nabla}_a - \nabla_a) (v^i\partial^b_i) \\
&\overset{\eqref{eq:skalarheraus}}{=} \underbrace{v^i}_{\substack{=v^i \delta^i_j=  v^i\partial^c_jdx^i_c \\ =v^cdx^i_c}}(\tilde{\nabla}_a - \nabla_a) \partial^b_i \\
& = dx_c^i(\tilde{\nabla}_a - \nabla_a) \partial^b_i \ v^c
\end{align*}
Damit folgt die koordinatenabhängige Definition des Differenzentensors in der Karte $x^i$
\begin{equation}
C^b_{ca} = \underbrace{dx_c^i}_{\substack{\text{lokal} \\ \text{definiert}}} (\tilde{\nabla}_a - \nabla_a) \underbrace{\partial^b_i}_{\substack{\text{lokal} \\ \text{definiert}}}
\end{equation}

\begin{proposition}
$C^b_{ca}$ ist unabhängig von der Wahl der Karte und somit global definiert. Insbesondere ist $C^b_{ca}$ tatsächlich ein (1,2)-Tensorfeld.
\end{proposition}

\begin{proof}
Wähle eine Karte $\hat{x}^i = \hat{x}^i(x^k)$. 
\begin{equation*}
\text{zz}: \ \hat{C}^b_{ca} = C^b_{ca}
\end{equation*}

Es gilt
\begin{align*}
\hat{C}^b_{ca} &= d\hat{x}^i_c \, (\tilde{\nabla}_a - \nabla_a)\underbrace{\hat{\partial}_i^{\,b}}_{\overset{\substack{\text{Basis} \\ \text{trafo}}}{=} \hat{\partial}_i x^k \, \partial_k^b} \\
&= d\hat{x}^i_c \, (\tilde{\nabla}_a - \nabla_a) (\underbrace{\hat{\partial}_i x^k}_{\in C^\infty(M)} \, \partial_k^b) \\
&\overset{\eqref{eq:skalarheraus}}{=} 
\underbrace{\hat{\partial}_i x^k \, d\hat{x}^i_c}_{\overset{\substack{\text{Basis} \\ \text{trafo}}}{=} dx_c^k} \, (\tilde{\nabla}_a - \nabla_a)\partial_k^b \\
&= dx^k_c \, (\tilde{\nabla}_a - \nabla_a)\partial_k^b \\
&= C^b_{ca}
\end{align*}
\end{proof}

\begin{lemma}[Wirkung kovariante Ableitung auf Kovektorfeld]
Seien $\nabla_a$ und $\tilde{\nabla}_a$ zwei kovariante Ableitungen und
$\omega_b \in \mathcal T^0_1(M)$ ein Kovektorfeld.
Dann gilt
\begin{equation*}\label{eq:kovekdiff}
(\tilde{\nabla}_a - \nabla_a)\,\omega_b =- C^c_{ba}\,\omega_c
\end{equation*}
\end{lemma}

\begin{proof}
Seien $\omega_b \in \mathcal T^0_1(M)$ und $v^b \in \mathcal T^1_0(M)$ beliebig. Dann ist
\begin{equation*}
(\tilde{\nabla}_a - \nabla_a)(\underbrace{\omega_b v^b}_{\in T^0_0(M)}) \overset{\eqref{eq:skalarzero}}{=} 0
\end{equation*}
Mit der Leibniz-Regel ergibt sich andererseits auch
\begin{align*}
& 0 =(\tilde{\nabla}_a - \nabla_a)(\omega_b v^b) = (\tilde{\nabla}_a - \nabla_a)\omega_b \cdot v^b
+ \omega_b \cdot \underbrace{(\tilde{\nabla}_a - \nabla_a)v^b}_{=C^c_{ba}\,v^b} \\
&\Leftrightarrow  (\tilde{\nabla}_a - \nabla_a)\omega_b \cdot v^b = - \omega_b C^c_{ba}\,v^b
\end{align*}

Da diese Gleichung für alle Vektorfelder $v^b$ gilt, folgt
\begin{equation*}
(\tilde{\nabla}_a - \nabla_a)\,\omega_b =- C^c_{ba}\,\omega_c
\end{equation*}
\end{proof}

Wir erhalten also nun auch alle anderen kovarianten Ableitungen, die auf einen Kovektor wirken, mit der Transformation
\begin{equation*}
\boxed{
\tilde{\nabla}_a\omega_b = \nabla_a \omega_b - C^c_{ba} \omega_c
}
\end{equation*}
(vgl. \eqref{eq:vektorkovar})

\begin{proposition}
Der Differenztensor ist in den unteren Indizes symmetrisch:
\begin{equation*}
C^c_{ab} = C^c_{ba}
\end{equation*}
\end{proposition}

\begin{proof}
Sei $f \in C^\infty(M)$.
Aus der Torsionsfreiheit (K4) folgt
\begin{equation*}
\nabla_a \nabla_b f = \nabla_b \nabla_a f
\quad\text{und}\quad
\tilde{\nabla}_a \tilde{\nabla}_b f = \tilde{\nabla}_b \tilde{\nabla}_a f
\end{equation*}

Also ist 
\begin{equation*}
\nabla_a \nabla_b f - \nabla_b \nabla_a f = \tilde{\nabla}_a \tilde{\nabla}_b f - \tilde{\nabla}_b \tilde{\nabla}_a f
\end{equation*}

Umordnen ergibt
\begin{align*}
&\underbrace{(\tilde{\nabla}_a - \nabla_a)\nabla_b f }_{\overset{\eqref{eq:kovekdiff}}{=} - C^c_{ba}\,\nabla_c f}
= \underbrace{(\tilde{\nabla}_b - \nabla_b)\nabla_a f}_{\overset{\eqref{eq:kovekdiff}}{=} - C^c_{ab}\,\nabla_c f}\\
&\Leftrightarrow -C^c_{ba}\nabla_c f = -C^c_{ab} \nabla_c f
\end{align*}
Da diese Gleichung für alle Kovektorfelder $\nabla_c f$ gilt, folgt
\begin{equation*}
C^c_{ab} = C^c_{ba}
\end{equation*}
\end{proof}

\heading{Abschließender Überblick}\newline
Die kovarianten Ableitungen $\nabla_a$ sind auf der gesamten Mannigfaltigkeit (d.h. global) definiert und somit unabhängig von der Wahl einer Karte. Sie sind jedoch \textbf{nicht} eindeutig. Die koordinatenkovarianten Ableitungen $\partial_a$ sind bezüglich einer gewählten Karte (d.h. lokal) definiert. $\nabla_a$ angewandt auf ein Skalarfeld stimmt lokal (nach Wahl einer Karte) mit $\partial_a$ überein.

Die Differenz zweier kovarianter Ableitungen
\begin{equation*}
C^b_{ca} v^c = (\tilde{\nabla}_a - \nabla_a) v^b
\end{equation*}
ist global definiert, während die Koordinatendarstellung
\begin{equation*}
C^b_{ca} = dx^i_c \, (\tilde{\nabla}_a - \nabla_a)\partial_i^{\,b}
\end{equation*}
nur lokal bezüglich der gewählten $x^i$-Karte existiert.

Damit sind kovariante Ableitungen leicht zu konstruieren: Kennt man eine kovariante Ableitung $\nabla_a$, so erhält man alle anderen mit dem Differenzentensor $C^b_{ca}$:
\begin{equation*}
\tilde{\nabla}_a v^b = \nabla_a v^b + C^b_{ca} v^c,
\qquad
\tilde{\nabla}_a \omega_b = \nabla_a \omega_b - C^c_{ba}\,\omega_c
\end{equation*}

\begin{remark}
Die Menge der kovarianten Ableitungen bildet einen affinen Raum. Wählt man eine kovariante Ableitung als Ursprung, so entsprechen die Tensorfelder $C^b_{ca}$ den Verbindungsvektoren, mit deren Hilfe alle anderen kovarianten Ableitungen erreicht werden.
\end{remark}

\section{Derivationen und Kommutator}
\begin{definition}
Eine \textbf{Algebra} $(A,\cdot)$ ist ein Vektorraum $A$ zusammen mit einer Abbildung,
\begin{equation*}
\cdot : A \times A \to A,
\qquad
(a,b) \mapsto a \cdot b
\end{equation*}
die bilinear ist, d.h. für alle $v,v_1,v_2,w,w_1,w_2 \in A$ und alle $\lambda \in \mathbb{R}$ gilt:
\begin{equation*}
(v_1+v_2)\cdot w = v_1\cdot w + v_2\cdot w
\end{equation*}
\begin{equation*}
v\cdot (w_1+w_2) = v\cdot w_1 + v\cdot w_2
\end{equation*}
\begin{equation*}
(\lambda v)\cdot w = \lambda (v\cdot w) = v\cdot (\lambda w)
\end{equation*}
\end{definition}

\begin{example}
Beispiele sind die Tensoralgebra mit dem Tensorprodukt, Polynomalgebren mit dem Cauchyprodukt oder der $\mathbb{R}^3$ mit dem Kreuzprodukt.
\end{example}

\begin{definition}
Eine \textbf{Derivation} auf einer Algebra $A$ ist eine lineare Abbildung
\begin{equation*}
D : A \to A,
\end{equation*}
die die Leibniz-Regel erfüllt:
\begin{equation*}
\forall a,b \in A: \ D(a \cdot b) = Da\cdot b + a \cdot Db
\end{equation*}
Die Menge aller Derivationen wird mit $\mathrm{Der}(A)$ bezeichnet.
\end{definition}

\begin{example}
Die koordinatenkovariante Ableitung und kovariante Ableitung sind Derivationen.
\end{example}

\begin{proposition}
Die Menge aller Derivationen $\mathrm{Der}(A)$ bildet einen Vektorraum.
\end{proposition}

Die Komposition von Derivationen $D_1 \circ D_2$ ist im Allgemeinen keine Derivation, da zusätzliche Terme auftreten. Beim anschließend definierten Kommutator fallen diese zusätzlichen Terme weg. 

\begin{definition}
Die bilineare Abbildung
\begin{equation*}
* : \mathrm{Der}(A) \times \mathrm{Der}(A) \to \mathrm{Der}(A),
\qquad
D_1 * D_2 := [D_1,D_2] := D_1 \circ D_2 - D_2 \circ D_1
\end{equation*}
heißt der \textbf{Kommutator}. Damit wird $(\mathrm{Der}(A),*)$ zu einer Algebra.
\end{definition}

Wir kürzen ab jetzt ab mit $D_1 \circ D_2 = D_1D_2$.

\begin{proof}
Wir müssen zeigen, dass diese Definition wohldefiniert ist. Die Bilinearität folgt aus der Bilinearität der Derivation.

Also bleibt zz: Der Kommutator zweier Derivationen ist wieder eine Derivation. 

Seien $D_1,D_2 \in \mathrm{Der}(A)$ und $a,b\in A$.

zz: der Kommutator erfüllt die Leibniz-Regel: 
\begin{equation*}
[D_1,D_2](a\cdot b) = [D_1,D_2]a\cdot b + a\cdot [D_1,D_2]b
\end{equation*}

\begin{itemize}
    \item Betrachte die linke Seite
    \begin{equation*}
     [D_1,D_2](a\cdot b) = (D_1 D_2 - D_2 D_1) (a \cdot b) = (D_1 D_2)(a \cdot b) - (D_2 D_1) (a \cdot b) = 
     D_1(D_2(a \cdot b)) - D_2(D_1(a \cdot b))
    \end{equation*}

Dann ist
\begin{equation*}
D_1(D_2(a \cdot b)) = D_1(D_2a\cdot b + a\cdot D_2b) = D_1D_2a\cdot b + D_2a\cdot D_1b + D_1a\cdot D_2b + a\cdot D_1D_2b
\end{equation*}
Analog erhält man
\begin{equation*}
D_2(D_1(a\cdot b)) = D_2D_1a\cdot b + D_1a\cdot D_2b + D_2a\cdot D_1b + a\cdot D_2D_1b
\end{equation*}
Die Komposition $D_1 D_2$ ist daher im Allgemeinen \emph{keine} Derivation, da im Vergleich zur Leibniz-Regel zwei zusätzliche Terme auftreten.
\begin{equation*}
[D_1,D_2](a\cdot b) = D_1(D_2(a \cdot b)) - D_2(D_1(a \cdot b)) = D_1D_2a\cdot b - D_2D_1a\cdot b + a\cdot D_1D_2b - a\cdot D_2D_1b
\end{equation*}

\item Betrachte die rechte Seite
    \begin{align*}
     [D_1,D_2]a\cdot b + a\cdot [D_1,D_2]b &= (D_1 D_2 - D_2 D_1)a \cdot b + a \cdot (D_1 D_2 - D_2 D_1)b \\
     &= D_1 D_2a \cdot b - D_2 D_1a \cdot b + a \cdot D_1 D_2 b - a \cdot D_2 D_1b
    \end{align*}
\end{itemize}
Also ist die linke Seite gleich der rechten Seite und die Behauptung folgt.
\end{proof}

\begin{definition}
Fixiere $D \in \mathrm{Der}(A)$. Wir definieren eine neue Abbildung
\begin{equation*}
\mathcal{D} : \mathrm{Der}(A) \to \mathrm{Der}(A),
\qquad
\mathcal{D}(D_1) := [D,D_1]
\end{equation*}
Diese Abbildung ist selbst eine Derivation auf der Algebra $\mathrm{Der}(A)$
(\emph{Derivation auf Derivationen}).
\end{definition}

Insbesondere gilt für den Kommutator die Leibniz-Regel
\begin{equation*}
\mathcal{D}(D_1 * D_2)
=
\mathcal{D}(D_1) * D_2 + D_1 * \mathcal{D}(D_2)
\end{equation*}
beziehungsweise ausgeschrieben
\begin{equation*}
[D,[D_1,D_2]] = [[D,D_1],D_2] + [D_1,[D,D_2]]
\end{equation*}

Aus dieser Leibniz-Regel folgt unmittelbar mit $[A,B]=-[B,A]$ die \textbf{Jacobi-Identität}:
\begin{equation*}
[D_1,[D_2,D_3]] + [D_2,[D_3,D_1]] + [D_3,[D_1,D_2]] = 0
\end{equation*}

Also ist die Jacobi-Identität ein Spezialfall der Leibniz-Regel.

\begin{remark}
Die geschlungene Derivation $\mathcal{D}$ wird hier nur zur Motivation eingeführt. Sie erklärt, warum die Jacobi-Identität aus der Leibniz-Regel folgt, und dient als Vorbereitung für die Lie-Algebra der Derivationen, die in der anschließenden Vorlesung \href{https://tiss.tuwien.ac.at/course/courseDetails.xhtml?dswid=9420&dsrid=973}{Geometrie und Gravitation I} eine wichtige Rolle spielt.
\end{remark}


\section{Riemanntensor}

Da die Allgemeine Relativitätstheorie gekrümmte Räume betrachtet, benötigen wir ein Maß für diese Krümmung. Dieses Maß ist der sogenannte \textbf{Riemanntensor} oder \textbf{Riemannsche Krümmungstensor}.

\begin{definition}
Der \textbf{Riemanntensor} ist definiert durch die Wirkung des Kommutators zweier kovarianter Ableitungen auf ein Vektorfeld:
\begin{equation*}
R^c_{dab}\, v^d := [\nabla_a,\nabla_b]\, v^c
\end{equation*}
Dabei bezeichnen die eckigen Klammern den \textbf{Kommutator} der kovarianten Ableitungen
\begin{equation*}
[\nabla_a,\nabla_b] := \nabla_a \nabla_b - \nabla_b \nabla_a 
\end{equation*}
\end{definition}

\begin{figure}[H]
\centering
\includegraphics[width=0.4\textwidth]{images/Riemanntensor1.jpg}
\caption{Der Riemanntensor beschreibt den Unterschied zwischen der Ableitung $\nabla_b$ von $v^c$ in Richtung $b$ und der anschließenden Ableitung $\nabla_a$ dieses Ergebnisses in Richtung $a$ vs. der Ableitung $\nabla_a$ von $v^c$ in Richtung $a$ und der anschließenden Ableitung $\nabla_b$ dieses Ergebnisses in Richtung $b$. \newline
Anders formuliert: er beschreibt den Unterschied von der infinitesimalen Bewegung von $v^c$ zuerst in Richtung $b$ und dann in Richtung $a$ vs zuerst in Richtung $a$ und dann in Richtung $b$. Wenn es keinen Unterschied gibt, ist der Raum flach. In diesem Sinn ist der Riemanntensor ein Maß für die Krümmung. \href{https://profoundphysics.com/general-relativity-for-dummies/}{Quelle}}
\end{figure}
Der Riemanntensor misst somit die Veränderung der kovarianten Ableitung in Bezug auf sich selbst und charaketerisiert sie so.


\begin{proposition}
    Kommutator der kovarianten Ableitungen ist \emph{$C^\infty(M)$-linear}, d.h. Skalarfelder können herausgezogen werden
    \begin{equation}\label{eq:skalarheraus1}
    [\nabla_a,\nabla_b](f v^c) = f \cdot [\nabla_a,\nabla_b]v^c 
    \end{equation}
\end{proposition}
\begin{proof}
Nach der vierten Eigenschaft der kovarianten Ableitung (Torsionsfreiheit / Satz von Schwarz) gilt für alle Skalarfelder $f \in C^\infty(M)$:
\begin{align*}
& \nabla_a \nabla_b f = \nabla_b \nabla_a f \\
\Leftrightarrow  \ & (\nabla_a \nabla_b - \nabla_b \nabla_a) f = 0 \\ 
\Leftrightarrow  \ & [\nabla_a,\nabla_b]\, f = 0 
\end{align*}

Damit erhalten wir für ein Vektorfeld $v^c$ und ein Skalarfeld $f$:
\begin{equation*}
[\nabla_a,\nabla_b](f v^c) = \underbrace{[\nabla_a,\nabla_b]f}_{=0} \cdot v^c + f \cdot [\nabla_a,\nabla_b]v^c = f \cdot [\nabla_a,\nabla_b]v^c 
\end{equation*}
\end{proof}

\medskip
Wir leiten nun die \textbf{Koordinatendarstellung des Riemanntensors} her.
\begin{align*}
R^c_{dab} v^d = [\nabla_a,\nabla_b] v^c & = [\nabla_a,\nabla_b] (v^i\partial^c_i) \\
&\overset{\eqref{eq:skalarheraus1}}{=} \underbrace{v^i}_{\substack{=v^i \delta^i_j=  v^i\partial^d_jdx^i_d \\ =v^ddx^i_d}}[\nabla_a,\nabla_b] \partial^c_i \\
& = dx_d^i[\nabla_a,\nabla_b] \partial^c_i \ v^d
\end{align*}
Damit folgt die koordinatenabhängige Definition des Differenzentensors in der Karte $x^i$
\begin{equation}
R^c_{dab} = \underbrace{dx_d^i}_{\substack{\text{lokal} \\ \text{definiert}}} [\nabla_a,\nabla_b] \underbrace{\partial^c_i}_{\substack{\text{lokal} \\ \text{definiert}}}
\end{equation}

\begin{proposition}
$R^c_{dab}$ ist unabhängig von der Wahl der Karte und somit global definiert. Insbesondere ist $R^c_{dab}$ tatsächlich ein (1,3)-Tensorfeld.
\end{proposition}

\begin{proof}
Wähle eine Karte $\hat{x}^i = \hat{x}^i(x^k)$. 
\begin{equation*}
\text{zz}: \ \hat{R}^c_{dab} = R^c_{dab}
\end{equation*}

Es gilt
\begin{align*}
\hat{R}^c_{dab} &= d\hat{x}^i_d \, [\nabla_a,\nabla_b]\underbrace{\hat{\partial}_i^{\,c}}_{\overset{\substack{\text{Basis} \\ \text{trafo}}}{=} \hat{\partial}_i x^k \, \partial_k^c} \\
&= d\hat{x}^i_d \, [\nabla_a,\nabla_b] (\underbrace{\hat{\partial}_i x^k}_{\in C^\infty(M)} \, \partial_k^c) \\
&\overset{\eqref{eq:skalarheraus1}}{=} 
\underbrace{\hat{\partial}_i x^k \, d\hat{x}^i_d}_{\overset{\substack{\text{Basis} \\ \text{trafo}}}{=} dx_d^k} \, [\nabla_a,\nabla_b]\partial_k^c \\
&= dx^k_d \, [\nabla_a,\nabla_b]\partial_k^c \\
&= R^c_{dab}
\end{align*}
\end{proof}

Da die lokalen Darstellungen auf Überlappungsgebieten übereinstimmen, definiert $R^c_{dab}$ ein globales Tensorfeld. 

\begin{definition}
    Der \textbf{Ricci-Tensor} ist die Spur des Riemanntensors
    \begin{equation*}
    R_{ab} := R^c_{acb}
    \end{equation*}
\end{definition}

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{images/Riemanntensor.jpg}
\caption{Der Riemanntensor scheint also unser Dilemma aus Kapitel 1 mit der Parallelverschiebung im gekrümmten Raum zu lösen. Mehr dazu im nächsten Kapitel.}
\end{figure}

\section{Parallelverschiebung}
Die folgenden Kapitel wurden im WS25/26 in der letzten Vorlesung im Schnelldurchlauf durchgemacht und sind daher hier nicht detailliert ausgeführt. Genauer sind sie in den Mitschriften von Robert Berndl-Forstner (WS20/21) und Florian Lindenbauer (WS18/19), zu finden: \newline
\url{https://forum.fstph.at/t/einf-i-d-allgemeine-relativitaetstheorie-mitschrift-ws2018} \newline
\url{https://forum.fstph.at/t/latex-skriptum-allg-relativitaetstheorie}

Da wir nun die kovariante Ableitung eingeführt haben, können wir sie verwenden, um zu definieren, wie Parallelität im gekrümmten Raum zu verstehen ist.

\subsection{Situation im \texorpdfstring{$\mathbb{R}^n$}{Rn}}
Wir führen das zu Beginn des Kapitels motivierte Konzept der Parallelverschiebung nun genauer ein.

Sei eine parametrisierte Kurve
\begin{equation*}
\gamma : [0,1] \to \mathbb{R}^n
\end{equation*}
mit Koordinaten $\gamma(t)=x^m(t)$ und $v^b = v^i \partial_i^b$ ein Vektorfeld, welches wir mit $v^b(x^m(t))=v^i(x^m(t))\, \partial_i^{\,b}\big|_{x^m(t)}$ entlang der Kurve betrachten. 

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{images/Vfparallel.jpg}
\end{figure}

Das Vektorfeld heißt \textbf{parallel entlang der Kurve}, wenn es ausgewertet entlang der Kurve überall in dieselbe Richtung zeigt und denselben Betrag hat. Das heißt, wenn es sich entlang der Kurve nicht ändert. Also
\begin{equation*}
\frac{d}{dt}\bigl(v^b(x^m(t))\bigr) = 0
\end{equation*}

Da die Basisvektoren $\partial_i^b$ im $\mathbb{R}^n$ ortsunabhängig sind, reduziert sich diese Bedingung auf die Konstanz der Komponenten:
\begin{equation*}
\frac{d}{dt} v^i(x^m(t)) = 0
\end{equation*}
Die gewöhnliche Ableitung misst in diesem Fall genau die Abweichung von der Parallelität.

Mit der Kettenregel erhalten wir andererseits
\begin{equation}\label{eq:rn_chain_rule}
0 = \frac{d}{dt}\bigl(v^i(x^m(t))\bigr)
= \dot{x}^j(t) \, \partial_j v^i(x^m(t))
\end{equation}
wobei $\dot{x}^j(t)$ die Komponenten des Tangentialvektors $\dot\gamma(t)$ in der gewählten Karte sind.

\textbf{Parallelverschiebung} entlang einer Kurve im $\mathbb{R}^n$ bedeutet also, dass ein Vektor, der verschoben wurde, weiterhin in dieselbe Richtung zeigt und denselben Betrag hat.


\subsection{Verallgemeinerung auf Mannigfaltigkeiten}
Sei $\gamma:[0,1]\to M$ eine Kurve auf einer Mannigfaltigkeit.

Wir erinnern uns: ein Vektorfeld auf einer Mannigfaltigkeit ist definiert als eine Abbildung, die jedem Punkt $p\in M$ einen Tangentialvektor $v^a(p)\in T_pM$ zuordnet. Wenn wir dieses nun entlang einer Kurve $\gamma$ und abgebildet mit einer Karte betrachten, hat es die folgende Darstellung
\begin{equation*}
v^a(x^m(t)) = v^i(x^m(t))\,\partial_i^a\big|_{x^m(t)}
\end{equation*}

Bei einer Ableitung nach dem Kurvenparameter $t$ ändern sich nun nicht nur die Komponenten $v^i(x^m(t))$, sondern auch die Basisvektoren $\partial_i^a\big|_{x^m(t)}$ selbst, da diese vom Punkt $p=\gamma(t)\in M$ via $\phi^i(\gamma(t))=x^i(t)$ abhängen.

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{images/Vfparallelman.jpg}
\end{figure}

Die gewöhnliche Ableitung misst daher nicht ausschließlich eine tatsächliche geometrische Änderung des Vektors, sondern enthält zusätzliche Beiträge, die durch den Wechsel der lokalen Basis entstehen. Eine verschwindende gewöhnliche Ableitung ist somit im Allgemeinen nicht gleichbedeutend mit Parallelität.

Plan: die partielle Ableitung durch die kovariante Ableitung ersetzen.

Im $\mathbb{R}^n$ ist der Tangentialvektor der Kurve $\gamma$
\begin{equation}\label{eq:tangent_vector_def}
u^a(t) \coloneqq \dot\gamma^i(t)\,\partial_i^a = \dot x^i(t)\,\partial_i^a
\end{equation}

Damit kann \eqref{eq:rn_chain_rule} angeschrieben werden
\begin{equation}
\begin{aligned}\label{eq:paralleltang}
0 &= \frac{d}{dt}v^i(x^m(t)) \, \partial_i^b\\
&= \dot{x}^j \, \partial_j v^i \, \partial_i^b\\
&= \dot{x}^i \, \underbrace{\delta_i^j}_{=\partial_i^adx_a^j} \, \partial_j v^i \, \partial_i^b\\
&= \dot{x}^i \, \partial_i^a dx_a^j \, \partial_j v^i \, \partial_i^b\\
&= \underbrace{\dot{x}^i \partial_i^a}_{=u^a} \, \underbrace{\partial_j v^i dx_a^j \partial_i^b}_{=\partial_av^b}\\
&= u^a \partial_av^b
\end{aligned}
\end{equation}

Damit erhalten wir für Parallelität im flachen Raum eine Äquivalenz bzgl. der koordinatenkovarianten Ableitung des Vektorfelds
\begin{equation}\label{eq:parallelflach}
\frac{d}{dt}\bigl(v^i(x^m(t))\bigr)=0
\quad\Leftrightarrow\quad
u^a\,\partial_a v^b = 0
\end{equation}

Die koordinatenkovariante Ableitung ist lokal definiert. Nun globalisieren wir das ganze mit der kovarianten Ableitung.

\begin{definition}
Sei $\gamma:[0,1]\to M$ eine Kurve auf einer Mannigfaltigkeit mit Tangentialvektor \\
$u^a(t) := \dot\gamma^a(t) \in T_{\gamma(t)}M$ bzw. in Koordinaten $u^a(t) = \dot x^i(t)\,\partial_i^a$. Ein Vektorfeld $v^b$ heißt \textbf{$\nabla$-parallel} entlang $\gamma$, falls
\begin{equation}
(u^a\,\nabla_a) v^b = 0
\end{equation}
\end{definition}

Zur Vereinfachung verwenden wir die Kurzschreibweise
\begin{equation*}
(u \nabla) := (u^a \nabla_a)
\end{equation*}

Existieren $\nabla$-parallele Vektorfelder überhaupt?\newline
Ja! Wir konstruieren nun eines.

\subsection{Konstruktion \texorpdfstring{$\nabla$}{nabla}-paralleles Vektorfeld}
Wir fixieren nun eine Karte $(\mathcal O,\phi)$, in der das betrachtete Kurvenstück $\gamma$ liegt. 

\begin{definition}
Sei $(\mathcal O,\phi)$ eine Karte. Die Differenz der global definierten kovarianten Ableitung $\nabla_a$, ausgewertet bzgl. dieser Karte und der lokal definierten koordinatenkovariante Ableitung $\partial_a$ bzgl. dieser Karte ist der lokal definierte \textbf{Differenzentensor} $\Gamma^b_{ca}$
\begin{equation}\label{eq:Gamma_def}
\Gamma^b_{ca}\,v^c := (\nabla_a-\partial_a)\,v^b
\qquad
\forall v^b\in\mathcal T^1_0(\mathcal O)
\end{equation}
Ist $\nabla$ die Levi-Civita-Ableitung, so ist $\Gamma^b_{ca}$ das Christoffel-Symbol.
In Koordinaten
\begin{equation*}
\Gamma^b_{ca} = \Gamma^i_{jk}\,\partial^b_i dx^j_c dx_a^k
\end{equation*}
\end{definition}

Damit kann die globale kovariante Ableitung ausgewertet bzgl. der fixierten Karte durch die zugehörige lokale koordinatenkovariante Ableitung plus Zusatzterm geschrieben werden
\begin{align*}
0= (u \nabla) v^b = (u^a \nabla_a) v^b &= \underbrace{(u^a \partial_a) v^b}_{\overset{\eqref{eq:paralleltang}}{=} \frac{d}{dt}v^k(t) \, \partial_k^b} + \overset{\text{Zusatzterm}}{\underbrace{u^a \Gamma^b_{ca}\,v^c}_{= \dot x^l \Gamma^k_{jl} v^j \, \partial_k^b}} \\ 
&= \left(\frac{d}{dt}v^k(t) + \Gamma^k_{jl} \, \dot x^l v^j \right) \partial^b_k
\end{align*}
Dieser Ausdruck ist aufgrund der linearen Unabhängigkeit der Basisvektoren nur dann null, wenn alle Komponenten gleich null sind. D.h.
\begin{equation}\label{eq:paralleltrans}
\frac{d}{dt}v^k(t) + \underbrace{\Gamma^k_{jl}(x^m(t)) \, \dot x^l(t)}_{=H^k_j(t)} \, v^j(t) = 0
\end{equation}
\begin{equation}\label{eq:parallel_transport_H}
\dot v^k(t) + H^k_j(t) \, v^j(t) = 0
\end{equation}
Diese Gleichung erinnert an die zeitabhängige Schrödingergleichung mit dem zeitabhängigen Hamiltonoperator $H^k_j(t)$. Wenn wir diese lösen, erhalten wir ein Vektorfeld $v^a$, das entlang der Kurve $\nabla$-parallel ist (ein parallel verschobenes Vektorfeld). Man sagt auch, das Vektorfeld ist zu sich selbst parallel.

\begin{minipage}[t]{0.75\textwidth}
Die Gleichung kann mit einem \textbf{Zeitentwicklungsoperator/Parallelverschieber} $U$ gelöst werden. Wir schreiben die Lösung von \eqref{eq:parallel_transport_H} in der Form
\begin{equation}\label{eq:U_def}
v^i(t) := U^i_j(t)\,v^j(0)
\end{equation}
wobei $v^j(0)$ der Anfangsvektor am Startpunkt der Kurve ist und $U^i_j(t)$ der Parallelverschieber entlang $\gamma$.
\end{minipage}\hfill
\begin{minipage}[t]{0.25\textwidth}
\centering
\vspace{-1em}
\includegraphics[width=\textwidth]{images/Parallelverschieber.jpg}
\end{minipage}


Setzen wir \eqref{eq:U_def} in \eqref{eq:parallel_transport_H} ein, so erhalten wir
\begin{align*}
0 &= \frac{d}{dt}\bigl(U^k_i(t)\,v^i(0)\bigr) + H^k_j(t)\,U^j_i(t)\,v^i(0)\\
&= \dot U^k_j(t)\,v^j(0) + U^k_j(t)\, \underbrace{\frac{d}{dt} v^j(0)}_{=0} + H^k_j(t)\,U^j_i(t)\,v^i(0) \\
&= \left(\dot U^k_i(t) + H^k_j(t)\,U^j_i(t)\right)v^i(0)
\end{align*}
Da der Anfangsvektor $v^i(0)$ beliebig ist, folgt die Differentialgleichung für den Parallelverschieber:
\begin{equation}\label{eq:U_ode}
\dot U^k_i(t) + H^k_j(t)\,U^j_i(t) = 0
\end{equation}
Als Anfangsbedingung gilt
\begin{equation}\label{eq:U_initial}
v^i(0)=U^i_j(0)v^j(0) \Rightarrow U^i_{\,j}(0) = \delta^i_j
\end{equation}
da bei $t=0$ keine Verschiebung stattfindet, muss der Vektor derselbe bleiben.

Wenn wir Gleichung \eqref{eq:U_ode} lösen, so erhalten wir den Parallelverschieber $U(t)$. Anschließend können wir mit \eqref{eq:U_def} ein Vektorfeld konstruieren, das entlang $\gamma$ $\nabla$-parallel ist ($\nabla$-konstant ist).

\begin{remark}
Der Parallelverschieber $U(t)$ definiert eine lineare Abbildung zwischen Tangentialräumen:
\begin{equation}
U(t) : T_{\gamma(0)}M \to T_{\gamma(t)}M
\end{equation}
Er erlaubt es, Vektoren in verschiedenen Tangentialräumen entlang einer gegebenen Kurve miteinander zu identifizieren
und als „parallel“ zu vergleichen.
\end{remark}

\subsection{Autoparallele Kurven und Geodätengleichung}
\begin{definition}
Eine Kurve $\gamma:I \to M$ heißt \textbf{autoparallel} (bezüglich $\nabla$), wenn das Vektorfeld, bestehend aus ihren Tangentenvektoren $u^a = \dot{\gamma}^a(t)$ (in Koordinaten $u^a = \dot{x}^i(t)\, \partial_i^a$), $\nabla$-parallel entlang der Kurve ist, d.h.
\begin{equation}\label{eq:autoparallel_condition}
(u^a\nabla_a)u^b = 0
\end{equation}
gilt.
\end{definition}

In Komponentenform mit $u^i=\dot{x}^i(t)$ ergibt sich aus \eqref{eq:autoparallel_condition} bzw. \eqref{eq:paralleltrans}
\begin{equation}\label{eq:geodesic_equation}
\boxed{
\ddot{x}^i(t) + \Gamma^i_{jk}(x(t))\,\dot{x}^j(t)\,\dot{x}^k(t) = 0
}
\end{equation}
Diese Gleichung heißt \textbf{Geodätengleichung}. Sie ist die Bewegungsgleichung eines Teilchens im Gravitationsfeld.

Im flachen Raum gilt $\Gamma^i_{jk} = 0$, sodass sich die Gleichung \eqref{eq:geodesic_equation} vereinfacht zu
\begin{equation*}
\frac{d}{dt}\bigl(u^i(t)\bigr) = \ddot{x}^i(t) = 0
\end{equation*}
also zu einer Geraden $x(t)=x_0+ \dot x_0t$. 

Autoparallele Kurven sind daher die Verallgemeinerung von Geraden auf gekrümmte Räume. 

\section{Metrik und Levi-Civita-Ableitung}
Um Geodäten als kürzeste Verbindung zwischen zwei Punkten im gekrümmten Raum zu beschreiben, brauchen wir einen Abstandsbegriff.

\begin{definition}
Eine \textbf{Metrik} (genauer: \textbf{metrischer Tensor/pseudo-riemannsche Metrik}) auf einer Mannigfaltigkeit $M$ ist ein (0,2)-Tensorfeld
\begin{equation*}
g_{ab} \in \mathcal T^0_2(M)
\end{equation*}
d.h.
\begin{equation*}
\forall \, p \in M:\quad g_{ab}: 
\begin{cases}
\ M \to \bigotimes^0_2 T_pM \;\cong\;\mathfrak{Bil}(T_pM,\mathbb R) \\
\ p \mapsto g_{ab}(p)
\end{cases}
\end{equation*}
wobei $g_{ab}(p)$ eine nicht-entartete, symmetrische, indefinite Bilinearform ist, mit
\begin{equation*}
g_{ab}(p) : T_pM \times T_pM \to \mathbb R
\end{equation*}
\end{definition}

Mit symmetrisch meinen wir
\begin{equation*}
g_{ab}(p) = g_{ba}(p)
\end{equation*}

Damit ist $g_{ab}(p)$ ein \textbf{Pseudo-Skalarprodukt} auf dem Tangentialraum $T_pM$.


In jedem Punkt $p \in M$ gibt es eine geeignete Basis $E_\alpha^a$ von $T_pM$, sodass $g_{ab}(p)$ in dieser Basis Diagonalgestalt annimmt:
\begin{equation*}
g_{ab}(p)(E_\alpha,E_\beta) = g_{ab} E_\alpha^a E_\beta^b =
\begin{cases}
\pm 1 & \alpha=\beta,\\
0 & \alpha\neq\beta,
\end{cases}
\quad = g_{\alpha\beta} = \mathrm{diag}(\pm 1,\ldots,\pm 1)
\end{equation*}

Mit einer geeigneten Basis kann also in jedem $T_pM$ der metrische Tensor $g_{ab}$ zur Minkowksi-Metrik transformiert werden 
\begin{equation*}
g_{ab} E_\alpha^a E_\beta^b = \eta_{\alpha\beta}=\mathrm{diag}(-1,1,1,1)
\end{equation*}

Damit haben wir auf der Mannigfaltigkeit $M$ ein Tensorfeld $g_{ab}$ definiert, das in jedem Tangentialraum $T_pM$ eine Minkowski-Metrik induziert. Jeder Tangentialraum wird somit zu einem Minkowski-Raum $\mathbb{M}$. Die Raumzeit ist lokal flach (Minkowski-Raum), aber global gekrümmt.

\begin{figure}[H]
\centering
\includegraphics[width=0.3\textwidth]{images/Minkoman.jpg}
\end{figure}

Die einzelnen Minkowski-Räume sind jedoch zunächst unabhängig voneinander. Wir möchten sie mit dem Paralleltransport in Verbindung bringen.

Wir betrachten eine Kurve $\gamma$ mit Tangentenvektor in Koordinaten bzgl. einer Karte $(\mathcal{O}, \phi)$
\begin{equation}\label{eq:parallelvw}
u^a = \dot{x}^i(t)\partial^a_i
\end{equation}
sowie zwei Vektorfelder $v^a$ und $w^a$, die entlang der Kurve $\nabla$-parallel sind:
\begin{equation*}
(u\nabla) v^a = 0,
\qquad
(u\nabla) w^a = 0
\end{equation*}

wobei
\begin{equation*}
(u\nabla) := u^c \nabla_c
\end{equation*}

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{images/Pseudoprod.jpg}
\end{figure}

Wir fordern nun außerdem, dass sich auch das innere Produkt (genauer: dessen Pseudo-Skalarprodukt) $g_{ab}(p)(v^a, w^a)= g_{ab}v^aw^a \in \mathbb{R}$ entlang der Kurve nicht ändert. Dies ist skalar und repräsentiert den Winkel zwischen den beiden Vektoren. D.h. wir fordern, dass sich der Winkel zweier Vektoren bei Parallelverschiebung entlang einer Kurve nicht ändert.

Das heißt wir fordern
\begin{equation*}
(u\nabla) (g_{ab} v^a w^b) = 0
\end{equation*}

Wendet man die Leibniz-Regel an und benutzt die Parallelität von $v^a$ und $w^a$,
\begin{align*}
0 &= (u\nabla) (g_{ab} v^a w^b) = u^c\nabla_c(g_{ab}v^a w^b) \\
&= u^c\, \nabla_c g_{ab} \, v^a w^b
+ g_{ab}\,u^c\nabla_c v^a\,w^b
+ g_{ab}\,v^a\,u^c\nabla_c w^b
\end{align*}
Die letzten beiden Terme verschwinden wegen \eqref{eq:parallelvw}. Übrig bleibt
\begin{equation}
u^c \nabla_c g_{ab} v^a w^b = 0
\end{equation}

Für $u^a(0)$, $v^a(0)$ und $w^a(0)$ folgt $\nabla_ag_{bc}(0)=0$. Da Anfangswerte beliebig gewählt werden können und diese Beziehung für \textit{alle} gelten soll, muss
\begin{equation}\label{eq:metrischkomp}
\nabla_c g_{ab} = 0
\end{equation}

Das heißt, der metrische Tensor ist bezüglich dieser kovarianten Ableitung konstant. Eine kovariante Ableitung $\nabla_a$ die \eqref{eq:metrischkomp} erfüllt wird \textbf{metrisch kompatibel} genannt.

\begin{definition}
Eine kovariante Ableitung $\nabla_a$ heißt \textbf{Levi-Civita Ableitung} relativ zu $g_{ab}$ $:\Leftrightarrow \ \nabla_c g_{ab} = 0$
\end{definition}

Diese Bedingung wählt also eine ganz bestimmte kovariante Ableitung aus. Das ist die Ableitung, die in der Allgemeinen Relativitätstheorie verwendet wird.

\subsection{Eindeutigkeit der Levi-Civita Ableitung}

Seien $\nabla$ und $\tilde{\nabla}$ zwei Levi-Civita Ableitungen.
Ihre Differenz ist ein Tensor
\begin{equation*}
C^c_{ab} := (\tilde \nabla_a - \nabla_a)
\end{equation*}

Für Kovektoren gilt dann
\begin{equation}\label{eq:nabla_vs_tildenabla_covector}
\nabla_a \omega_b = \tilde\nabla_a \omega_b - C^c_{ab}\,\omega_c
\end{equation}

Somit gilt bem metrischen Tensor:
\begin{equation*}
0 = \nabla_a g_{bc}
= \tilde{\nabla}_a g_{bc}
- C^m_{ba} g_{mc}
- C^m_{ca} g_{bm}
\end{equation*}

Mit Index-Transport erhalten wir
\begin{equation*}
\tilde{\nabla}_a g_{bc} = C_{cba} + C_{bca}
\end{equation*}

Durch zyklische Permutation der Indizes und Ausnutzung der Symmetrie (folgt aus Torsionsfeiheit der kovarianten Ableitung)
\begin{equation*}
C^c_{ab} = C^c_{ba}
\end{equation*}
folgt schließlich
\begin{equation*}
C^c_{ab}
=
\frac{1}{2} g^{cd}
\bigl(
\tilde{\nabla}_a g_{bd}
+ \tilde{\nabla}_b g_{ad}
- \tilde{\nabla}_d g_{ab}
\bigr)
\end{equation*}

\begin{definition}
Sei $(M,g_{ab})$ eine pseudo-riemannsche Mannigfaltigkeit und $\nabla_a$ die zu $g_{ab}$ gehörige Levi-Civita-Ableitung.
Eine Kurve $\gamma$ heißt \textbf{Geodäte}, wenn sie autoparallel bezüglich $\nabla_a$ ist, d.h.
\begin{equation*}
(u^b \nabla_b) u^a = 0
\end{equation*}
\end{definition}
Geodäten sind damit die Verallgemeinerung von Geraden auf gekrümmte Räume. Sie sind der kürzeste Weg zwischen zwei Punkten auf einer gekrümmten Mannigfaltigkeit.

\begin{remark}
Mit der Levi-Civita-Ableitung haben wir nun ein geometrisches Werkzeug, das die zuvor mit $T_pM$ unabhängig voneinander definierten lokalen Minkowski-Räume $\mathbb{M}$ miteinander in Beziehung setzt. Sie definiert einen Paralleltransport entlang von Kurven, der das Pseudo-Skalarprodukt und somit Winkel erhält. Auf diese Weise können Vektoren in verschiedenen Tangentialräumen sinnvoll miteinander verglichen werden. Die Levi-Civita-Ableitung wird daher auch als \textbf{Levi-Civita-Zusammenhang} bezeichnet.
\end{remark}

\section{Newtonsche Deviationsgleichung}
Wir betrachten eine Familie von frei fallenden Objekten im Gravitationsfeld, die (nebeneinander) in Richtung Erdoberfläche fallen. Sie fallen alle in Richtung Erdmittelpunkt, daher verlaufen die Bahnlinien nicht nur „parallel nach unten“, sondern die Körper bewegen sich während des Falls auch aufeinander zu.

Die Teilchenbahnen hängen von zwei Parametern ab:
\begin{itemize}
\item $t$: Zeitparameter entlang einer Bahn
\item $s$: Selektorparameter, der angibt, welche Freifallkurve betrachtet wird
\end{itemize}

Die Bahnen werden beschrieben durch
\begin{equation*}
x^i = x^i(t,s)
\end{equation*}

Die Kurve $s=0$ dient als Referenzkurve.

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{images/Freifallkurven.jpg}
\end{figure}

Nach dem zweiten Newtonschen Axiom gilt für ein Teilchen der Masse $m(s)$
\begin{equation*}
m(s)\,\ddot{x}^i(t,s) = F^i(x(t,s))
\end{equation*}

Somit gilt für ein Teilchen newtonschen Gravitationsfeld 
\begin{equation*}
m(s)\,\ddot{x}^i(t,s) = -m(s)\,\partial_i \Phi\bigl(x^m(t,s)\bigr)
\end{equation*}

Die Masse kürzt sich heraus, sodass die Bahnkurve unabhängig von der Masse des Teilchens ist:
\begin{equation}\label{eq:newtonbeweg}
\ddot{x}^i(t,s) = - \partial_i \Phi(x^m(t,s))
\end{equation}

Um zu beschreiben, wie sich benachbarte Freifallkurven relativ zueinander bewegen, definieren wir das \textbf{Zeigerfeld}
\begin{equation*}
\xi^i(t) := \left.\frac{\partial x^i(t,s)}{\partial s}\right|_{s=0}
\end{equation*}

Dieses Vektorfeld ist bzgl. der Referenzkurve $(s=0)$ definiert und zeigt zur nächsten Freifallkurve. Es beschreibt, wie stark und in welche Richtung das Teilchen $x^i(t,0)$ aufgrund der Gravitation abgelenkt wird.

Wir differenzieren die Bewegungsgleichung \eqref{eq:newtonbeweg} nach $s$ und werten sie bei $s=0$ aus:
\begin{align*}
& \left.\frac{\partial}{\partial s}\right|_{s=0}
\left(
\ddot{x}^i(t,s) + \partial_i \Phi(x^m(t,s))
\right) = 0 \\
&\Leftrightarrow \ \left.\frac{\partial^3 x^i(t,s)}{\partial s \,\partial t^2 }\right|_{s=0} + \left.\frac{\partial}{\partial s}\right|_{s=0} \bigl(\partial_i \Phi(x^m(t,s))\bigr) = 0 \\
\end{align*}

Da die Ableitungen nach $t$ und $s$ vertauschen, erhalten wir
\begin{equation*}
\left.\frac{\partial^3 x^i(t,s)}{\partial t^2 \,\partial s }\right|_{s=0} = \ddot{\xi}^i(t)
\end{equation*}

Mit der Kettenregel folgt
\begin{align*}
\left.\frac{\partial}{\partial s}\right|_{s=0}
\bigl(\partial_i \Phi(x^m(t,s))\bigr)
&=
\left. \frac{\partial \partial_i \Phi(x^m(t))}{\partial x^j}
\frac{\partial x^j(t,s)}{\partial s}
\right|_{s=0} \\
&=
\partial_j \partial_i \Phi(x^m(t))\,\xi^j(t)
\end{align*}
Damit ergibt sich die \textbf{Newtonsche Deviationsgleichung}:
\begin{equation}\label{eq:newtondevgl}
\boxed{
\ddot{\xi}^i(t)
+ \partial_j \partial_i \Phi(x^m(t))\,\xi^j(t) = 0
}
\end{equation}

Die Gleichung beschreibt die relative Beschleunigung aufgrund der Gravitation zweier benachbarter, frei fallender Teilchen. Also die Ablenkung, welche ein Teilchen unter Einwirkung des Gravitationsfeldes erfährt.

\begin{itemize}
\item $\Phi$: Gravitationspotential (Skalarfeld)
\item $\partial_i \Phi$: Gradient des Gravitationspotentials: Gravitationsfeld/newtonsche Gravitationsfeldstärke (Vektorfeld)
\item $\partial_i \partial_j \Phi$: räumliche Änderung des Gravitationsfelds
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{earth-moon-gravitational-potential-3d.png}
\caption{Gravitationspotential und Gravitationsfeld von Erde und Mond dargestellt. Der skalare Wert gibt die Höhe an, die Pfeile geben die Richtung an.}
{\footnotesize Quelle: \url{https://physikbuch.schule/work-in-the-gravitational-field.html}}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{gravfeld.png}
\caption{Gravitationsfeld von Erde und Mond}
{\footnotesize Quelle: \url{https://physikbuch.schule/work-in-the-gravitational-field.html}}
\end{figure}

Ist das Gravitationsfeld $\partial_i \Phi$ lokal konstant/homogen, so verschwindet dort $\partial_i \partial_j \Phi$ und benachbarte Freifallkurven bleiben parallel (wenn wir beispielsweise zwei Gegenstände im Vakuum dicht nebeneinander fallen lassen). In inhomogenen Gravitationsfeldern hingegen beschreibt die Deviationsgleichung die \emph{\href{https://de.wikipedia.org/wiki/Gezeitenkraft}{Gezeitenkräfte}}.

Diese Gleichung stellt das Newtonsche Analogon zur \emph{Geodäten-Deviationsgleichung} der Allgemeinen Relativitätstheorie dar.

\section{Geodätische Deviationsgleichung}
Wir betrachten nun die Freifallkurven im Kontext der allgemeinen Relativitätstheorie. Hier bewegen sich frei fallende Körper auf kürzesten Wegen durch die gekrümmte Raumzeit, also auf \textit{Geodäten} bzw. \textit{autoparallelen Kurven}.

Seien die autoparallelen Kurven (Freifallkurven) wieder parametrisiert mit 
\begin{itemize}
\item $t$: Zeitparameter entlang einer Bahn (Eigenzeit der Teilchen)
\item $s$: Selektorparameter, der angibt, welche Freifallkurve betrachtet wird
\end{itemize}

Sei wieder das Zeigervektorfeld zwischen benachbarten Geodäten definiert als
\begin{equation*}
\xi^i(t) := \left.\frac{\partial x^i(t,s)}{\partial s}\right|_{s=0}
\end{equation*}

und das Tangentialvektorfeld der jeweiligen Freifallkurve
\begin{equation*}
u^i(t,s) = \frac{d x^i(t,s)}{d t}
\end{equation*}
wobei $u^a$ zeitartig ist mit Längenquadrat
\begin{equation*}
u^2 := g_{ab}u^a u^b < 0
\end{equation*}

Wir normieren das Tangentialvektorfeld sodass
\begin{equation*}
u^2 = -1
\end{equation*}

Da die Freifallkurven autoparallel sind, gilt (Tangentialvektorfeld ist $\nabla$-konstant)
\begin{equation}\label{eq:autoparallel}
(u\nabla)u^a = 0
\end{equation}
wobei $\nabla$ die Levi-Civita Ableitung ist. 

Wir erinnern uns, dass der metrische Tensor $g_{ab}$ invariant bzgl. der Levi-Civita Ableitung ist ($\nabla_c g_{ab}=0$). Daher und wegen der Autoparallelität der Kurve verändert sich das Längenquadrat von $u^a$ entlang der Kurve nicht:
\begin{equation*}
(u\nabla)(u^2)
= (u\nabla)(g_{ab}u^a u^b) = \underbrace{(u\nabla)g_{ab}}_{=0} \, u^a u^b + g_{ab} \underbrace{(u\nabla) u^a}_{=0} u^b +  g_{ab}u^a \underbrace{(u\nabla) u^b}_{=0} =0
\end{equation*}
Damit bleibt ein zu Beginn zeitartiger Tangentenvektor entlang der gesamten Freifallkurve zeitartig. Paralleltransport ändert also keine Längen- oder Kausalstruktur.

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{images/Freifallkurven1.jpg}
\end{figure}

$u^a$ ist ein Strömungsfeld, entlang dessen die Teilchen auf den Freifallkurven transportiert werden. Die Änderung des Strömungsfeldes durch das Zeigervektorfeld $\xi^a$ wird durch die sogenannte \textbf{konvektive Ableitung} bzw. \textbf{Lie-Ableitung} beschrieben 
\begin{equation*}
L_u \xi^a = \underbrace{(u\nabla)\xi^a}_{\substack{\text{Änderung des Vektorfelds }\xi^a \\ \text{entlang von } u^a}} - \underbrace{(\xi\nabla)u^a}_{\substack{\text{Änderung des Vektorfelds }u^a \\ \text{entlang von } \xi^a}}
\end{equation*}

\textit{In WS25/26 hatten wir keine Zeit die Lie-Ableitung genauer einzuführen.}

Wir fordern, dass das Zeigervektorfeld $\xi^a$ mit der Strömung $u^a$ mitgeführt wird, d.h. sich relativ zur Strömung nicht ändert:
\begin{equation*}
L_u \xi^a = 0
\end{equation*}
beziehungsweise 
\begin{equation}\label{eq:lie_condition}
(u\nabla)\xi^a = (\xi\nabla)u^a
\end{equation}

Da wir das relativistische Analogon zur Newton-Deviationsgleichung \eqref{eq:newtondevgl} herleiten wollen und in dieser die zweite Ableitung von $\xi$ auftritt, bestimmen wir nun die zweite Ableitung des Zeigervektorfelds entlang der Weltlinie, d.h. $(u\nabla)^2\xi^a$.

Mit \eqref{eq:lie_condition} schreiben wir
\begin{align*}
(u\nabla)^2 \xi^a
&= (u\nabla)\bigl((\xi\nabla)u^a\bigr)
\end{align*}
\begin{equation}\label{eq:zweitterm}
= (u\nabla)\xi^c\,\nabla_c u^a + \xi^c \underbrace{(u\nabla)}_{=u^d\nabla_d}\nabla_c u^a
\end{equation}

Nach der Definition des Riemann-Tensors gilt
\begin{equation*}
[\nabla_a, \nabla_b] v^c = (\nabla_a \nabla_b - \nabla_b \nabla_a) v^c = R^c_{dab}v^d
\end{equation*}
Dies können wir auf den zweiten Term von \eqref{eq:zweitterm} anwenden und die Ableitungen $\nabla_d \nabla_c$ vertauschen
\begin{align*}
&u^d \, \nabla_d\nabla_cu^a - \, u^d\nabla_c \nabla_du^a = u^d \, R^a_{bdc} u^b \\
&\Leftrightarrow u^d \, \nabla_d\nabla_cu^a = u^d\nabla_c \nabla_du^a + u^d \, R^a_{bdc}u^b
\end{align*}
Somit ist
\begin{align*}
\eqref{eq:zweitterm} &= (u\nabla)\xi^c\,\nabla_c u^a + \xi^c \underbrace{u^d \, \nabla_d\nabla_cu^a}_{\overset{\text{Leibniz}}{=} 
\nabla_c(\underbrace{u^d\nabla_du^a}_{\overset{\eqref{eq:autoparallel}}{=}0}) - \nabla_c u^d \nabla_d u^a} + \xi^c \,u^d R^a_{bdc} u^b \\
&\Leftrightarrow (u\nabla)\xi^c\,\nabla_c u^a - \underbrace{\xi^c \nabla_c}_{=(\xi \nabla)} u^d \nabla_d u^a + \xi^c \,u^d R^a_{bdc} u^b \\
&\Leftrightarrow \underbrace{\left((u\nabla)\xi^d - (\xi \nabla) u^d \right)}_{\overset{\eqref{eq:lie_condition}}{=}0} \nabla_d u^a +  R^a_{bdc} u^b u^d \xi^c
\end{align*}

Es folgt somit
\begin{equation*}
(u\nabla)^2 \xi^a = \underbrace{R^a_{bdc}}_{=-R^a_{bcd}} u^b u^d \xi^c
\end{equation*}

So erhalten wir die \textbf{geodätische Deviationsgleichung}
\begin{equation*}
\boxed{
(u\nabla)^2 \xi^a + R^a_{bcd}\,u^b u^d \xi^c = 0
}
\end{equation*}

Sie beschreibt, wie sich benachbarte frei fallende Teilchen in einer gekrümmten Raumzeit relativ zueinander bewegen. Die Rolle der zweiten Ableitung des Gravitationspotentials in der Newtonschen Deviationsgleichung wird nun vom Riemann-Krümmungstensor übernommen. Sie macht sichtbar, dass Gravitation in der Allgemeinen Relativitätstheorie durch Raumzeitkrümmung beschrieben wird.

\section{Einsteinsche Feldgleichungen}
Wir vergleichen die Newtonsche und die geodätische Deviationsgleichung.

Einsteins Idee: in schwachen Gravitationsfeldern, also bei wenig Raumzeitkrümmung, gilt (Newton-Limes)
\begin{equation}\label{eq:newtalbert}
R^a_{bcd}\,u^b u^d = \partial^a \partial_c \Phi
\end{equation}
wobei wir die Indizes $a,c$ anstelle von $i,j$ verwenden, da wir die Newtonsche Deviationsgleichung in eine raumzeitliche Form überführen.

Aus der Newton-Mechanik folgt im Gravitationspotential $\Phi$ (für Einheitsmasse) die Poisson-Gleichung
\begin{equation}
\partial^2 \Phi = \Delta \Phi = 4\pi\,\rho_m
\end{equation}
wobei $\rho_m$ die \textbf{Materie-/Energiedichte} bezeichnet und $\Delta$ der Laplace Operator.

Um in \eqref{eq:newtalbert} auf die zweite Ableitung zu kommen, müssen wir aus dem Krümmungstensor ein Objekt zweiter Stufe bauen. Dazu kontrahieren wir die Indizes $a$ und $c$ (Spurbildung):
\begin{equation}
R_{bd} := R^a_{bad}
\end{equation}
$R_{bd}$ wird Ricci-Tensor genannt.

Damit gilt
\begin{equation*}
R^a_{bab} u^b u^d =R_{bd}\,u^b u^d = \partial^2 \Phi = \Delta \Phi = 4\pi\,\rho_m
\end{equation*}

Die Materie-/Energiedichte $\rho_m$ kann mit dem \textbf{Energie-Impuls-Tensors} $T_{ab}$ geschrieben werden als
\begin{equation*}
\rho_m = T_{ab}\,u^a u^b
\end{equation*}

Der \href{https://en.wikipedia.org/wiki/Stress%E2%80%93energy_tensor}{Energie-Impuls-Tensor} beschreibt die Dichte und den Fluss von Energie und Masse in der Raumzeit. Er ist eine Verallgemeinerung des Stress-Tensors.

Damit ergibt sich als erster Ansatz (Vorstufe Einsteinsche Feldgleichung 1914)
\begin{equation*}
R_{ab} = 4\pi\,T_{ab}
\end{equation*}

Warum stimmt diese Gleichung nicht?

Physikalisch gilt lokale Energie-Impuls-Erhaltung (Energie-Impuls-Tensor ist divergenzfrei):
\begin{equation*}
\nabla^a T_{ab} = 0
\end{equation*}

Nähmen wir $R_{ab}=4\pi T_{ab}$, müsste also auch $\nabla^a R_{ab}=0$ gelten.
Aus der (kontrahierten) Bianchi-Identität folgt jedoch
\begin{equation}\label{eq:bianci}
\nabla^a R_{ab} = \frac{1}{2}\,\nabla_b R
\end{equation}
wobei $R := g^{ab}R_{ab}$ der Krümmungsskalar, der im Allgemeinen nicht konstant ist, und daher $\nabla_b R \neq 0$, also $\nabla^a R_{ab} \neq 0$.

Einstein betrachtete daher den Ausdruck
\begin{align*}
\nabla^a\!\left(R_{ab}-\frac{1}{2}  g_{ab}R\right) &= \nabla^a R_{ab}-\tfrac12 \underbrace{\nabla^a(g_{ab}R)}_{=\underbrace{\nabla^ag_{ab}}_{\overset{\substack{\text{Levi-Civita} \\ \text{Ableitung}}}{=}0}R+g_{ab}\nabla^aR} = \nabla^aR_{ab} - \frac{1}{2} g_{ab}\underbrace{\nabla^a}_{=g^{ab}\nabla_c}R \\
&= \nabla^aR_{ab} - \frac{1}{2}\underbrace{g_{ab}g^{ac}}_{=\delta_b^c} \nabla_cR = \nabla^aR_{ab} - \frac{1}{2}\nabla_bR \overset{\eqref{eq:bianci}}{=} 0
\end{align*}

Somit ist der sogenannte \textbf{Einstein-Tensor}
\begin{equation*}
G_{ab}:=R_{ab}-\frac{1}{2} g_{ab}R
\end{equation*}
divergenzfrei.

Einsteins finaler Ansatz lautet daher
\begin{equation*}
G_{ab} = 4\pi \alpha\,T_{ab}
\end{equation*}

Die Konstante $\alpha$ wird durch den Newton-Limes festgelegt; man erhält $\alpha=2$ und somit die endgültige Form der \textbf{Einsteinschen Feldgleichungen} (1915):
\begin{equation*}
\boxed{
G_{ab} = 8\pi\,T_{ab}
}
\end{equation*}

Die Einsteinsche Feldgleichung beschreibt die Wechselwirkung zwischen Materie (Masse) bzw. Energie (mit Energie-Impuls-Tensor $T_{ab}$) und der Geometrie der Raumzeit (mit Einstein-Tensor $G_{ab}$), wobei Materie die Raumzeit krümmt und diese Krümmung die Bewegung von Objekten (wie Planeten oder Licht) bestimmt. Es ist ein dynamisches Rückkopplungssystem. 

Die Lösung dieser Gleichung liefert die Raumzeitmetrik $g_{ab}$. Erst durch $g_{ab}$ erhalten Begriffe wie zeitartig, raumartig oder lichtartig überhaupt eine Bedeutung, da sie von der Metrik abhängen.

Der Physiker John Archibald Wheeler brachte es knapp auf den Punkt: 
\begin{quote}
“Materie sagt dem Raum, wie er sich krümmen soll. Der Raum sagt der Materie, wie sie sich bewegen soll.“
\end{quote}

\heading{Lösen der Gleichung}

{\footnotesize gute Links: \url{https://en.wikipedia.org/wiki/Einstein_field_equations#Mathematical_form}\newline
\url{https://profoundphysics.com/einstein-field-equations-fully-written-out-what-do-they-look-like-expanded/}}\newline
$G_{ab}$ und $T_{ab}$ sind Tensoren zweiter Stufe mit Indizes $a,b=0,1,2,3$. Das heißt, es sind $4\times4$ Matrizen. Für jedes Indexpaar $(a,b)$ gibt es eine Gleichung, also insgesamt $16$ Gleichungen. Aufgrund der Symmetrie $G_{ab}=G_{ba}$ bzw. $T_{ab}=T_{ba}$ bleiben nur $10$ Gleichungen übrig. Mit der (kontrahierten) Bianchi-Identität kann die Zahl der unabhängigen Gleichungen weiter auf $6$ reduziert werden.

Die Einsteinschen Feldgleichungen sind gekoppelte nichtlineare partielle Differentialgleichungen zweiter Ordnung. Die partiellen Ableitungen stecken in $G_{ab}$. 

\underline{Idee: Masse/Materieverteilung vorgeben}\newline
Um die Gleichung zu lösen, könnte man meinen, zunächst eine Materieverteilung $T_{ab}$ vorzugeben und anschließend die Feldgleichungen nach $g_{ab}$ lösen. Allerdings muss für die gewählte Materieverteilung $\nabla^a T_{ab} = 0$ gelten. Die Levi-Civita-Ableitung $\nabla^a$ hängt von der gesuchten Metrik $g_{ab}$ ab, daher kann erst nachdem die Metrik bestimmt wurde, geprüft werden, ob die zuvor gewählte Materieverteilung eine physikalisch zulässige war. Diese Methode ist sehr ineffizient.

\underline{Idee: Metrik vorgeben}\newline
Eine andere Idee wäre es, eine Metrik $g_{ab}$ vorgeben, daraus $G_{ab}$ zu berechnen und anschließend den Energie-Impuls-Tensor zu bestimmen mit
\begin{equation*}
T_{ab} = \frac{1}{8\pi}G_{ab}
\end{equation*}
Allerdings sind auch hier wieder nicht alle Ergebnisse für $T_{ab}$ physikalisch zulässig, da die Materie-/Energiedichte für alle Beobachter positiv sein muss (\href{https://de.wikipedia.org/wiki/Energiebedingung#Die_schwache_Energiebedingung}{schwache Energiebedingung}) d.h.
\begin{equation*}
\rho_m = T_{ab}u^a u^b \ge 0 
\qquad
\forall \text{ zeitartigen }u^a
\end{equation*}
Diese Methode ist ebenso sehr ineffizient.

\heading{Vakuumlösung}

Ein Spezialfall ist das \textbf{Vakuum}, in dem es keine Materie gibt, also ist der Energie-Impuls Tensor (Materieverteilung)
\begin{equation*}
T_{ab} = 0
\end{equation*}
Dann folgt $G_{ab}=0$ bzw. $R_{ab}=0$. Der Riemann-Krümmungstensor kann weiterhin von Null verschieden sein (Raumzeit nicht flach). Karl Schwarzschild fand 1916 eine \textbf{Vakuumlösung} der Feldgleichungen. Ihre spätere Interpretation zeigte, dass sie ein schwarzes Loch beschreibt, mit Ereignishorizont und einer Region unbeschränkt wachsender Raumzeitkrümmung.

\heading{Kosmologische Lösungen und kosmologische Konstante}

{\footnotesize guter Link: \url{https://en.wikipedia.org/wiki/Einstein_field_equations#Cosmological_constant}}\newline
Mit der Annahme des \textbf{kopernikanischen Prinzips} existieren Lösungen der Einsteinschen Feldgleichungen, die das Universum auf großen Skalen (Galaxiehaufen, Supercluster) beschreiben. Es lautet:
\begin{itemize}
\item \textbf{Homogenität:} Auf hinreichend großen Skalen sind physikalische Größen (z.B. Ladungsdichte, Materiedichte) auf Gleichzeitflächen ortsunabhängig.
\item \textbf{Isotropie:} Auf hinreichend großen Skalen gibt es keine ausgezeichneten Raumrichtungen. Das Universum schaut auf hinreichend großen Skalen in alle Richtungen gleich aus.
\end{itemize}
Unter diesen Annahmen vereinfachen sich die Einsteinschen Feldgleichungen stark und bilden ein dynamisches System von einem Freiheitsgrad. Dieser ist der \textbf{Skalenfaktor} $a(t)$. Das Universum kann sich daher insgesamt nur ausdehnen oder zusammenziehen.

Einstein stellte fest, dass die Einsteinschen Feldgleichungen unter diesen Annahmen \emph{kein statisches Universum} zulassen, sondern notwendigerweise ein dynamisches. Um dennoch ein statisches Universum zu ermöglichen, fügte er einen zusätzlichen Term hinzu:
\begin{equation*}
G_{ab} + \Lambda g_{ab} = 8\pi\,T_{ab}
\end{equation*}
wobei $\Lambda$ die \textbf{kosmologische Konstante} ist.

Dieser Zusatzterm ist mathematisch konsistent, da sowohl der Einstein-Tensor $G_{ab}$ als auch der metrische Tensor $g_{ab}$ kovariant divergenzfrei sind:
\begin{equation*}
\nabla^a G_{ab} = 0,
\qquad
\nabla^a g_{ab} = 0.
\end{equation*}
Die Forderung der lokalen Energie-Impuls-Erhaltung $\nabla^aT_{ab}=0$ bleibt somit erhalten.

Spätere Beobachtungen (insbesondere die Rotverschiebung ferner Galaxien durch Edwin Hubble) zeigten jedoch, dass sich das Universum tatsächlich ausdehnt. Einstein verwarf daraufhin die kosmologische Konstante und bezeichnete ihre Einführung später als seine \glqq größte Eselei\grqq.

Heute spielt die kosmologische Konstante erneut eine Rolle: Aktuelle kosmologische Beobachtungen deuten auf eine beschleunigte Expansion des Universums hin, die durch eine positive kosmologische Konstante erklärt werden kann. In diesem Kontext kann $\Lambda$ als Energie des Vakuums interpretiert werden (vgl. dunkle Energie). 

Auf der Skala von Galaxien oder kleineren Systemen ist der Einfluss der kosmologischen Konstante vernachlässigbar.
\vspace{1em}

\begin{tcolorbox}[noVO]
\begin{remark}[nicht Teil der VO]\breakafterhead

{\footnotesize Quelle: \url{https://en.wikipedia.org/wiki/Einstein_field_equations#Mathematical_form}\newline
How Mass WARPS SpaceTime: Einstein's Field Equations in Gen. Relativity | Physics for Beginners: \url{https://youtu.be/FJnTItLVIqQ}}

Die Einsteinschen Feldgleichungen lauten in SI-Einheiten
\begin{equation*}
G_{ab} + \Lambda g_{ab}
= \kappa\, T_{ab}
\end{equation*}
wobei $\kappa= \frac{8\pi G}{c^4}$ die \textbf{einsteinsche Gravitationskonstante} oder \textbf{Einsteinkonstante} mit der Newtonschen Gravitationskonstante $G$ und der Lichtgeschwindigkeit $c$ ist. In der Vorlesung wird $c=G=1$ verwendet.
\end{remark}
\end{tcolorbox}


\heading{Fazit}

Die \textbf{zentrale Aussage der Allgemeinen Relativitätstheorie} lautet: Gravitation ist keine Kraft, sondern eine Konsequenz der Raumzeitkrümmung. Objekte bewegen sich unter Einfluss der Gravitation auf geradest möglichen Bahnen (Geodäten). Die Raumzeitmetrik $g_{ab}$ entsteht aus dem Wechselspiel zwischen Geometrie und Materie.


% --- Back matter ---
\backmatter

\chapter*{Weiterführende Literatur}
\begin{itemize}
  \item R.~M.~Wald, \textit{General Relativity}.
  \item B.~Schutz, \textit{A First Course in General Relativity}.
  \item S.~Carroll, \textit{Spacetime and Geometry}.
\end{itemize}

\end{document}
